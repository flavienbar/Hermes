{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15058e2",
   "metadata": {},
   "source": [
    "# 🥇 Data Lakehouse Gold - Feature Store Optimisé\n",
    "\n",
    "## 1. Introduction & Architecture\n",
    "\n",
    "Ce notebook implémente la **zone Gold** de notre data lakehouse avec une approche **Feature Store** optimisée :\n",
    "\n",
    "### 🏗️ **Architecture Gold**\n",
    "\n",
    "1. **📊 Feature Store Central** : `gold_features_crypto_4h`\n",
    "   - Table unique contenant tous les indicateurs techniques pré-calculés\n",
    "   - Source de vérité pour tous les features réutilisables\n",
    "   - Format : 1 ligne = 1 crypto + 1 timestamp + tous les indicateurs\n",
    "\n",
    "2. **🎯 Marts de Stratégies** : `gold_strategy_{nom_strategie}`\n",
    "   - Tables spécialisées contenant les signaux de trading\n",
    "   - Consomment les features du Feature Store\n",
    "   - Une table par stratégie pour la flexibilité\n",
    "\n",
    "### ✨ **Avantages**\n",
    "\n",
    "- **🚀 Performance** : Calcul unique des indicateurs (DRY principle)\n",
    "- **⚡ Rapidité** : Prototypage instantané de nouvelles stratégies  \n",
    "- **🔄 Réutilisabilité** : Features partagés entre stratégies\n",
    "- **📈 Évolutivité** : Ajout facile de nouveaux indicateurs\n",
    "- **🔧 Traitement Incrémental** : Lookback intelligent pour tous les indicateurs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9fe641",
   "metadata": {},
   "source": [
    "## 2. Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c6579c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration chargée\n",
      "📊 Source Bronze: s3://bronze/binance/data/spot/monthly/klines/BTCUSDT/4h/**/*.parquet\n",
      "🥇 Destination Gold: s3://gold\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import duckdb\n",
    "import talib as ta\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration globale\n",
    "class Config:\n",
    "    \"\"\"Configuration centralisée pour le pipeline Gold\"\"\"\n",
    "    \n",
    "    # MinIO/S3 Configuration\n",
    "    MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"127.0.0.1:9000\")\n",
    "    MINIO_ACCESS_KEY = os.getenv(\"MINIO_ROOT_USER\", \"minioadm\")\n",
    "    MINIO_SECRET_KEY = os.getenv(\"MINIO_ROOT_PASSWORD\", \"minioadm\")\n",
    "    \n",
    "    # Chemins de données\n",
    "    medaillon_source = \"bronze\"\n",
    "    provider = \"binance\"\n",
    "    data_type = \"data\"\n",
    "    market = \"spot\"\n",
    "    data_frequency = \"monthly\"\n",
    "    data_category = \"klines\"\n",
    "    symbol = \"BTCUSDT\"\n",
    "    interval = \"4h\"\n",
    "    BRONZE_PATH = f\"s3://{medaillon_source}/{provider}/{data_type}/{market}/{data_frequency}/{data_category}/{symbol}/{interval}/**/*.parquet\"\n",
    "    GOLD_BUCKET = \"s3://gold\"\n",
    "    \n",
    "    # Tables Gold\n",
    "    FEATURE_STORE_TABLE = f\"gold_features_{market}_{data_frequency}_{data_category}_{symbol}_{interval}\"\n",
    "    \n",
    "    # Paramètres des indicateurs techniques\n",
    "    TECHNICAL_INDICATORS = {\n",
    "        \"sma_periods\": [10, 20, 50, 100, 200],\n",
    "        \"ema_periods\": [12, 20, 26, 50, 100],\n",
    "        \"rsi_periods\": [14, 21],\n",
    "        \"bollinger\": {\"period\": 20, \"std_dev\": 2},\n",
    "        \"macd\": {\"fast\": 12, \"slow\": 26, \"signal\": 9},\n",
    "        \"atr_period\": 14,\n",
    "        \"supertrend\": {\"length\": 10, \"multiplier\": 3.0},\n",
    "        \"stochastic\": {\"k_period\": 14, \"d_period\": 3}\n",
    "    }\n",
    "\n",
    "print(\"✅ Configuration chargée\")\n",
    "print(f\"📊 Source Bronze: {Config.BRONZE_PATH}\")\n",
    "print(f\"🥇 Destination Gold: {Config.GOLD_BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ec8bd",
   "metadata": {},
   "source": [
    "## 3. Feature Store Optimisé avec Traitement Incrémental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43dae99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature Store OPTIMISÉ initialisé\n"
     ]
    }
   ],
   "source": [
    "class UltraFixedIncrementalFeatureStore:\n",
    "    \"\"\"Feature Store OPTIMISÉ avec lookback intelligent pour TOUS les indicateurs\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.con = None\n",
    "    \n",
    "    def setup_duckdb(self):\n",
    "        \"\"\"Configure DuckDB avec les paramètres S3/MinIO\"\"\"\n",
    "        self.con = duckdb.connect(database=\":memory:\")\n",
    "        self.con.execute(f\"\"\"\n",
    "            SET s3_access_key_id='{self.config.MINIO_ACCESS_KEY}';\n",
    "            SET s3_secret_access_key='{self.config.MINIO_SECRET_KEY}';\n",
    "            SET s3_endpoint='{self.config.MINIO_ENDPOINT}';\n",
    "            SET s3_url_style='path';\n",
    "            SET s3_use_ssl='false';\n",
    "        \"\"\")\n",
    "        print(\"🔗 DuckDB configuré pour MinIO\")\n",
    "    \n",
    "    def load_bronze_data(self, start_date: str = None) -> pl.DataFrame:\n",
    "        \"\"\"Charge les données depuis la zone Bronze avec filtre optionnel\"\"\"\n",
    "        print(\"📥 Chargement des données Bronze...\")\n",
    "        \n",
    "        where_clause = \"\"\n",
    "        if start_date:\n",
    "            where_clause = f\"WHERE datetime >= '{start_date}'\"\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT \n",
    "                datetime,\n",
    "                open,\n",
    "                high,\n",
    "                low,\n",
    "                close,\n",
    "                volume,\n",
    "                year,\n",
    "                month,\n",
    "                day\n",
    "            FROM read_parquet('{self.config.BRONZE_PATH}')\n",
    "            {where_clause}\n",
    "            ORDER BY datetime\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pl.from_arrow(self.con.execute(query).arrow())\n",
    "        \n",
    "        print(f\"✅ {df.height:,} lignes chargées de Bronze\")\n",
    "        if df.height > 0:\n",
    "            print(f\"📅 Période: {df['datetime'].min()} → {df['datetime'].max()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def calculate_technical_indicators(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Calcule TOUS les indicateurs techniques de manière optimisée\"\"\"\n",
    "        print(\"🔧 Calcul des indicateurs techniques...\")\n",
    "        \n",
    "        # Conversion en numpy pour TA-Lib\n",
    "        ohlcv = {\n",
    "            'open': df['open'].to_numpy(),\n",
    "            'high': df['high'].to_numpy(), \n",
    "            'low': df['low'].to_numpy(),\n",
    "            'close': df['close'].to_numpy(),\n",
    "            'volume': df['volume'].to_numpy()\n",
    "        }\n",
    "        \n",
    "        indicators = {}\n",
    "        \n",
    "        # 1. Moyennes Mobiles Simples (SMA)\n",
    "        for period in self.config.TECHNICAL_INDICATORS['sma_periods']:\n",
    "            indicators[f'sma_{period}'] = ta.SMA(ohlcv['close'], timeperiod=period)\n",
    "        \n",
    "        # 2. Moyennes Mobiles Exponentielles (EMA) \n",
    "        for period in self.config.TECHNICAL_INDICATORS['ema_periods']:\n",
    "            indicators[f'ema_{period}'] = ta.EMA(ohlcv['close'], timeperiod=period)\n",
    "        \n",
    "        # 3. RSI (Relative Strength Index)\n",
    "        for period in self.config.TECHNICAL_INDICATORS['rsi_periods']:\n",
    "            indicators[f'rsi_{period}'] = ta.RSI(ohlcv['close'], timeperiod=period)\n",
    "        \n",
    "        # 4. Bollinger Bands\n",
    "        bb_params = self.config.TECHNICAL_INDICATORS['bollinger']\n",
    "        bb_upper, bb_middle, bb_lower = ta.BBANDS(\n",
    "            ohlcv['close'], \n",
    "            timeperiod=bb_params['period'], \n",
    "            nbdevup=bb_params['std_dev'],\n",
    "            nbdevdn=bb_params['std_dev']\n",
    "        )\n",
    "        indicators[f\"bb_upper_{bb_params['period']}_{bb_params['std_dev']}\"] = bb_upper\n",
    "        indicators[f\"bb_middle_{bb_params['period']}_{bb_params['std_dev']}\"] = bb_middle\n",
    "        indicators[f\"bb_lower_{bb_params['period']}_{bb_params['std_dev']}\"] = bb_lower\n",
    "        \n",
    "        # 5. MACD\n",
    "        macd_params = self.config.TECHNICAL_INDICATORS['macd']\n",
    "        macd_line, macd_signal, macd_hist = ta.MACD(\n",
    "            ohlcv['close'],\n",
    "            fastperiod=macd_params['fast'],\n",
    "            slowperiod=macd_params['slow'], \n",
    "            signalperiod=macd_params['signal']\n",
    "        )\n",
    "        indicators[f\"macd_{macd_params['fast']}_{macd_params['slow']}_{macd_params['signal']}\"] = macd_line\n",
    "        indicators[f\"macd_signal_{macd_params['fast']}_{macd_params['slow']}_{macd_params['signal']}\"] = macd_signal\n",
    "        indicators[f\"macd_hist_{macd_params['fast']}_{macd_params['slow']}_{macd_params['signal']}\"] = macd_hist\n",
    "        \n",
    "        # 6. ATR (Average True Range)\n",
    "        atr_period = self.config.TECHNICAL_INDICATORS['atr_period']\n",
    "        indicators[f'atr_{atr_period}'] = ta.ATR(\n",
    "            ohlcv['high'], ohlcv['low'], ohlcv['close'], timeperiod=atr_period\n",
    "        )\n",
    "        \n",
    "        # 7. Stochastic Oscillator\n",
    "        stoch_params = self.config.TECHNICAL_INDICATORS['stochastic']\n",
    "        stoch_k, stoch_d = ta.STOCH(\n",
    "            ohlcv['high'], ohlcv['low'], ohlcv['close'],\n",
    "            fastk_period=stoch_params['k_period'],\n",
    "            slowk_period=stoch_params['d_period'],\n",
    "            slowd_period=stoch_params['d_period']\n",
    "        )\n",
    "        indicators[f\"stoch_k_{stoch_params['k_period']}_{stoch_params['d_period']}\"] = stoch_k\n",
    "        indicators[f\"stoch_d_{stoch_params['k_period']}_{stoch_params['d_period']}\"] = stoch_d\n",
    "        \n",
    "        # 8. SuperTrend (implémentation native) - VERSION CORRIGÉE\n",
    "        try:\n",
    "            st_params = self.config.TECHNICAL_INDICATORS['supertrend']\n",
    "            \n",
    "            # Calcul SuperTrend natif avec TA-Lib\n",
    "            supertrend_values, supertrend_direction = self._calculate_supertrend_native(\n",
    "                high=ohlcv['high'],\n",
    "                low=ohlcv['low'],\n",
    "                close=ohlcv['close'],\n",
    "                length=st_params['length'],\n",
    "                multiplier=st_params['multiplier']\n",
    "            )\n",
    "            \n",
    "            indicators[f\"supertrend_{st_params['length']}_{st_params['multiplier']}\"] = supertrend_values\n",
    "            indicators[f\"supertrend_dir_{st_params['length']}_{st_params['multiplier']}\"] = supertrend_direction\n",
    "            \n",
    "            print(f\"✅ SuperTrend calculé avec implémentation native\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur SuperTrend: {e}\")\n",
    "            # Créer des arrays de NaN de la bonne taille en cas d'erreur\n",
    "            nan_array = np.full(len(df), np.nan)\n",
    "            indicators[f\"supertrend_{st_params['length']}_{st_params['multiplier']}\"] = nan_array\n",
    "            indicators[f\"supertrend_dir_{st_params['length']}_{st_params['multiplier']}\"] = nan_array\n",
    "        \n",
    "        # Ajout des indicateurs au DataFrame\n",
    "        indicator_columns = []\n",
    "        for name, values in indicators.items():\n",
    "            indicator_columns.append(pl.Series(name=name, values=values))\n",
    "        \n",
    "        df_with_indicators = df.with_columns(indicator_columns)\n",
    "        \n",
    "        print(f\"✅ {len(indicators)} indicateurs calculés\")\n",
    "        return df_with_indicators\n",
    "    \n",
    "    def _calculate_supertrend_native(self, high, low, close, length=10, multiplier=3.0):\n",
    "        \"\"\"\n",
    "        Calcule SuperTrend de manière native avec TA-Lib\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (supertrend_values, supertrend_direction)\n",
    "                - supertrend_values: Les valeurs SuperTrend\n",
    "                - supertrend_direction: 1 pour bullish, -1 pour bearish\n",
    "        \"\"\"\n",
    "        # 1. Calcul de l'ATR avec TA-Lib\n",
    "        atr = ta.ATR(high, low, close, timeperiod=length)\n",
    "        \n",
    "        # 2. Calcul des bandes haute et basse\n",
    "        hl2 = (high + low) / 2.0  # Médiane high-low\n",
    "        upper_band = hl2 + (multiplier * atr)\n",
    "        lower_band = hl2 - (multiplier * atr)\n",
    "        \n",
    "        # 3. Initialisation des arrays\n",
    "        n = len(close)\n",
    "        supertrend = np.full(n, np.nan)\n",
    "        direction = np.full(n, np.nan, dtype=float)\n",
    "        \n",
    "        # 4. Initialisation des bandes finales\n",
    "        final_upper = np.copy(upper_band)\n",
    "        final_lower = np.copy(lower_band)\n",
    "        \n",
    "        # 5. Calcul itératif du SuperTrend\n",
    "        # Commencer après la période ATR\n",
    "        start_idx = length\n",
    "        \n",
    "        for i in range(start_idx, n):\n",
    "            if np.isnan(atr[i]) or np.isnan(upper_band[i]) or np.isnan(lower_band[i]):\n",
    "                continue\n",
    "                \n",
    "            # Calcul des bandes finales (à partir du 2ème élément valide)\n",
    "            if i > start_idx:\n",
    "                # Bande supérieure finale\n",
    "                if upper_band[i] < final_upper[i-1] or close[i-1] > final_upper[i-1]:\n",
    "                    final_upper[i] = upper_band[i]\n",
    "                else:\n",
    "                    final_upper[i] = final_upper[i-1]\n",
    "                \n",
    "                # Bande inférieure finale  \n",
    "                if lower_band[i] > final_lower[i-1] or close[i-1] < final_lower[i-1]:\n",
    "                    final_lower[i] = lower_band[i]\n",
    "                else:\n",
    "                    final_lower[i] = final_lower[i-1]\n",
    "            \n",
    "            # Détermination de la direction et SuperTrend\n",
    "            if i == start_idx:\n",
    "                # Premier calcul valide\n",
    "                if close[i] <= final_lower[i]:\n",
    "                    direction[i] = -1.0\n",
    "                    supertrend[i] = final_upper[i]\n",
    "                else:\n",
    "                    direction[i] = 1.0\n",
    "                    supertrend[i] = final_lower[i]\n",
    "            else:\n",
    "                # Calculs suivants\n",
    "                prev_direction = direction[i-1]\n",
    "                \n",
    "                if prev_direction == 1.0 and close[i] <= final_lower[i]:\n",
    "                    # Changement vers bearish\n",
    "                    direction[i] = -1.0\n",
    "                    supertrend[i] = final_upper[i]\n",
    "                elif prev_direction == -1.0 and close[i] >= final_upper[i]:\n",
    "                    # Changement vers bullish\n",
    "                    direction[i] = 1.0\n",
    "                    supertrend[i] = final_lower[i]\n",
    "                else:\n",
    "                    # Maintien de la direction\n",
    "                    direction[i] = prev_direction\n",
    "                    if prev_direction == 1.0:\n",
    "                        supertrend[i] = final_lower[i]\n",
    "                    else:\n",
    "                        supertrend[i] = final_upper[i]\n",
    "        \n",
    "        return supertrend, direction\n",
    "    \n",
    "    def get_enhanced_max_lookback_period(self) -> int:\n",
    "        \"\"\"Calcule le lookback optimal pour CHAQUE type d'indicateur\"\"\"\n",
    "        lookback_requirements = []\n",
    "        config = self.config.TECHNICAL_INDICATORS\n",
    "        \n",
    "        print(\"🔬 Analyse des besoins de lookback par indicateur:\")\n",
    "        \n",
    "        # 1. SMA - Simple Moving Average\n",
    "        if 'sma_periods' in config:\n",
    "            max_sma = max(config['sma_periods'])\n",
    "            lookback_requirements.append(max_sma)\n",
    "            print(f\"   📈 SMA max: {max_sma}\")\n",
    "        \n",
    "        # 2. EMA - Exponential Moving Average (3x pour convergence)\n",
    "        if 'ema_periods' in config:\n",
    "            max_ema = max(config['ema_periods'])\n",
    "            ema_lookback = max_ema * 3  # Convergence exponentielle\n",
    "            lookback_requirements.append(ema_lookback)\n",
    "            print(f\"   📈 EMA effective: {ema_lookback} (3x {max_ema})\")\n",
    "        \n",
    "        # 3. RSI - Relative Strength Index (2x pour stabilisation)\n",
    "        if 'rsi_periods' in config:\n",
    "            max_rsi = max(config['rsi_periods'])\n",
    "            rsi_lookback = max_rsi * 2  # Période de warm-up\n",
    "            lookback_requirements.append(rsi_lookback)\n",
    "            print(f\"   🎯 RSI avec warm-up: {rsi_lookback} (2x {max_rsi})\")\n",
    "        \n",
    "        # 4. Bollinger Bands\n",
    "        if 'bollinger' in config:\n",
    "            bb_period = config['bollinger']['period']\n",
    "            lookback_requirements.append(bb_period)\n",
    "            print(f\"   📊 Bollinger Bands: {bb_period}\")\n",
    "        \n",
    "        # 5. MACD - Complexe (EMA lente + signal)\n",
    "        if 'macd' in config:\n",
    "            macd_slow = config['macd']['slow']\n",
    "            macd_signal = config['macd']['signal']\n",
    "            # EMA lente (3x) + EMA signal (3x) pour stabilité totale\n",
    "            macd_lookback = (macd_slow * 3) + (macd_signal * 3)\n",
    "            lookback_requirements.append(macd_lookback)\n",
    "            print(f\"   ⚡ MACD complexe: {macd_lookback} ({macd_slow}*3 + {macd_signal}*3)\")\n",
    "        \n",
    "        # 6. ATR - Average True Range\n",
    "        if 'atr_period' in config:\n",
    "            atr_period = config['atr_period']\n",
    "            lookback_requirements.append(atr_period)\n",
    "            print(f\"   🛡️ ATR: {atr_period}\")\n",
    "        \n",
    "        # 7. SuperTrend (dépend d'ATR + sa propre longueur)\n",
    "        if 'supertrend' in config:\n",
    "            st_length = config['supertrend']['length']\n",
    "            atr_period = config.get('atr_period', 14)  # ATR par défaut\n",
    "            st_lookback = atr_period + st_length * 2  # ATR + SuperTrend\n",
    "            lookback_requirements.append(st_lookback)\n",
    "            print(f\"   🔄 SuperTrend: {st_lookback} (ATR:{atr_period} + ST:{st_length}*2)\")\n",
    "        \n",
    "        # 8. Stochastic Oscillator\n",
    "        if 'stochastic' in config:\n",
    "            stoch_k = config['stochastic']['k_period']\n",
    "            stoch_d = config['stochastic']['d_period']\n",
    "            stoch_lookback = stoch_k + stoch_d\n",
    "            lookback_requirements.append(stoch_lookback)\n",
    "            print(f\"   📊 Stochastic: {stoch_lookback} ({stoch_k} + {stoch_d})\")\n",
    "        \n",
    "        # Prise du maximum + marge de sécurité généreuse\n",
    "        if lookback_requirements:\n",
    "            base_lookback = max(lookback_requirements)\n",
    "            safety_margin = max(50, int(base_lookback * 0.2))  # Minimum 50 ou 20%\n",
    "            total_lookback = base_lookback + safety_margin\n",
    "            \n",
    "            print(f\"\\n🎯 Lookback de base: {base_lookback}\")\n",
    "            print(f\"🛡️ Marge de sécurité: {safety_margin}\")\n",
    "            print(f\"📊 TOTAL LOOKBACK: {total_lookback}\")\n",
    "            \n",
    "            return total_lookback\n",
    "        \n",
    "        return 250  # Fallback conservateur\n",
    "    \n",
    "    def get_existing_data_info(self) -> Dict:\n",
    "        \"\"\"Récupère les infos sur les données existantes\"\"\"\n",
    "        print(\"🔍 Analyse des données existantes...\")\n",
    "        \n",
    "        if not self.con:\n",
    "            self.setup_duckdb()\n",
    "        \n",
    "        feature_store_path = f\"{self.config.GOLD_BUCKET}/{self.config.FEATURE_STORE_TABLE}/**/*.parquet\"\n",
    "        \n",
    "        try:\n",
    "            # Vérification de l'existence\n",
    "            info_query = f\"\"\"\n",
    "                SELECT \n",
    "                    MIN(datetime) as min_date,\n",
    "                    MAX(datetime) as max_date,\n",
    "                    COUNT(*) as total_rows,\n",
    "                    COUNT(DISTINCT symbol) as symbols_count\n",
    "                FROM read_parquet('{feature_store_path}')\n",
    "            \"\"\"\n",
    "            \n",
    "            result = self.con.execute(info_query).fetchone()\n",
    "            \n",
    "            return {\n",
    "                'exists': True,\n",
    "                'min_date': result[0],\n",
    "                'max_date': result[1], \n",
    "                'total_rows': result[2],\n",
    "                'symbols_count': result[3]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Feature Store n'existe pas encore: {e}\")\n",
    "            return {\n",
    "                'exists': False\n",
    "            }\n",
    "    \n",
    "    def save_feature_store(self, df: pl.DataFrame):\n",
    "        \"\"\"Sauvegarde le Feature Store en Gold\"\"\"\n",
    "        print(f\"💾 Sauvegarde du Feature Store...\")\n",
    "        \n",
    "        # Ajout des métadonnées\n",
    "        df_final = df.with_columns([\n",
    "            pl.lit(\"BTCUSDT\").alias(\"symbol\"),\n",
    "            pl.lit(\"4h\").alias(\"timeframe\"),\n",
    "            pl.lit(datetime.now(timezone.utc).isoformat()).alias(\"created_at\")\n",
    "        ])\n",
    "        \n",
    "        # Enregistrement temporaire pour DuckDB\n",
    "        try:\n",
    "            self.con.execute(\"DROP VIEW IF EXISTS tmp_features\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.con.register(\"tmp_features\", df_final.to_arrow())\n",
    "        \n",
    "        # Sauvegarde partitionnée par year/month\n",
    "        output_path = f\"{self.config.GOLD_BUCKET}/{self.config.FEATURE_STORE_TABLE}/\"\n",
    "        \n",
    "        save_query = f\"\"\"\n",
    "            COPY tmp_features \n",
    "            TO '{output_path}'\n",
    "            WITH (FORMAT PARQUET, PARTITION_BY (year, month), OVERWRITE_OR_IGNORE TRUE)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.con.execute(save_query)\n",
    "        \n",
    "        print(f\"✅ Feature Store sauvegardé: {output_path}\")\n",
    "        print(f\"📊 {df_final.height:,} lignes, {df_final.width} colonnes\")\n",
    "    \n",
    "    def _append_to_feature_store(self, df: pl.DataFrame):\n",
    "        \"\"\"Ajoute des données au Feature Store existant\"\"\"\n",
    "        print(\"➕ Ajout des nouvelles données au Feature Store...\")\n",
    "        \n",
    "        try:\n",
    "            self.con.execute(\"DROP VIEW IF EXISTS tmp_new_features\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.con.register(\"tmp_new_features\", df.to_arrow())\n",
    "        \n",
    "        # Sauvegarde en mode append (pas de OVERWRITE)\n",
    "        output_path = f\"{self.config.GOLD_BUCKET}/{self.config.FEATURE_STORE_TABLE}/\"\n",
    "        \n",
    "        append_query = f\"\"\"\n",
    "            COPY tmp_new_features \n",
    "            TO '{output_path}'\n",
    "            WITH (FORMAT PARQUET, PARTITION_BY (year, month))\n",
    "        \"\"\"\n",
    "        \n",
    "        self.con.execute(append_query)\n",
    "        \n",
    "        print(f\"✅ {df.height:,} nouvelles lignes ajoutées\")\n",
    "    \n",
    "    def build_feature_store_complete(self):\n",
    "        \"\"\"Construction COMPLÈTE du Feature Store (première fois)\"\"\"\n",
    "        print(\"🚀 Construction COMPLÈTE du Feature Store Gold\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 1. Setup\n",
    "        self.setup_duckdb()\n",
    "        \n",
    "        # 2. Chargement des données complètes Bronze\n",
    "        df_bronze = self.load_bronze_data()\n",
    "        \n",
    "        if df_bronze.height == 0:\n",
    "            print(\"❌ Aucune donnée Bronze trouvée!\")\n",
    "            return None\n",
    "        \n",
    "        # 3. Calcul des indicateurs\n",
    "        df_with_features = self.calculate_technical_indicators(df_bronze)\n",
    "        \n",
    "        # 4. Sauvegarde complète\n",
    "        self.save_feature_store(df_with_features)\n",
    "        \n",
    "        # 5. Nettoyage\n",
    "        if self.con:\n",
    "            self.con.close()\n",
    "        \n",
    "        print(\"\\n🎉 Feature Store construit avec succès!\")\n",
    "        return df_with_features\n",
    "    \n",
    "    def update_feature_store_incremental(self):\n",
    "        \"\"\"Mise à jour INCRÉMENTALE du Feature Store avec lookback optimal\"\"\"\n",
    "        print(\"🔄 Mise à jour INCRÉMENTALE du Feature Store\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 1. Setup\n",
    "        self.setup_duckdb()\n",
    "        \n",
    "        # 2. Vérification de l'état existant\n",
    "        existing_info = self.get_existing_data_info()\n",
    "        \n",
    "        if not existing_info['exists']:\n",
    "            print(\"❌ Feature Store n'existe pas, construction complète requise\")\n",
    "            return self.build_feature_store_complete()\n",
    "        \n",
    "        print(f\"📊 Feature Store existant: {existing_info['total_rows']:,} lignes\")\n",
    "        print(f\"📅 Période: {existing_info['min_date']} → {existing_info['max_date']}\")\n",
    "        \n",
    "        start_date = existing_info['max_date']\n",
    "        max_lookback = self.get_enhanced_max_lookback_period()\n",
    "        \n",
    "        # 3. Chargement avec lookback optimal\n",
    "        print(f\"\\n🔄 Traitement incrémental depuis: {start_date}\")\n",
    "        \n",
    "        # Calculer la date de début avec lookback\n",
    "        lookback_query = f\"\"\"\n",
    "            SELECT datetime \n",
    "            FROM read_parquet('{self.config.BRONZE_PATH}')\n",
    "            WHERE datetime <= '{start_date}'\n",
    "            ORDER BY datetime DESC\n",
    "            LIMIT {max_lookback}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            lookback_result = self.con.execute(lookback_query).fetchall()\n",
    "            \n",
    "            if lookback_result:\n",
    "                # Prendre la date la plus ancienne du lookback\n",
    "                lookback_start_date = lookback_result[-1][0]\n",
    "                print(f\"📊 Chargement avec lookback depuis: {lookback_start_date}\")\n",
    "                print(f\"🔢 Garantit {max_lookback} périodes de contexte historique\")\n",
    "                \n",
    "                # Chargement des données avec contexte historique\n",
    "                df_with_lookback = self.load_bronze_data(start_date=lookback_start_date)\n",
    "            else:\n",
    "                print(\"⚠️ Lookback impossible, chargement complet pour sécurité\")\n",
    "                df_with_lookback = self.load_bronze_data()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur lookback: {e}, chargement complet\")\n",
    "            df_with_lookback = self.load_bronze_data()\n",
    "        \n",
    "        if df_with_lookback.height == 0:\n",
    "            print(\"✅ Aucune donnée à traiter\")\n",
    "            return None\n",
    "        \n",
    "        # 4. Calcul des indicateurs sur le dataset complet\n",
    "        df_with_features = self.calculate_technical_indicators(df_with_lookback)\n",
    "        \n",
    "        # 5. Filtrage pour garder seulement les nouvelles lignes\n",
    "        df_new_only = df_with_features.filter(pl.col('datetime') > start_date)\n",
    "        print(f\"🎯 {df_new_only.height:,} nouvelles lignes à ajouter\")\n",
    "        \n",
    "        if df_new_only.height == 0:\n",
    "            print(\"✅ Aucune nouvelle donnée après filtrage\")\n",
    "            return None\n",
    "        \n",
    "        # 6. Ajout des métadonnées\n",
    "        df_final = df_new_only.with_columns([\n",
    "            pl.lit(\"BTCUSDT\").alias(\"symbol\"),\n",
    "            pl.lit(\"4h\").alias(\"timeframe\"),\n",
    "            pl.lit(datetime.now(timezone.utc).isoformat()).alias(\"updated_at\")\n",
    "        ])\n",
    "        \n",
    "        # 7. Sauvegarde en append\n",
    "        self._append_to_feature_store(df_final)\n",
    "        \n",
    "        # 8. Nettoyage\n",
    "        if self.con:\n",
    "            self.con.close()\n",
    "        \n",
    "        print(\"\\n🎉 Mise à jour incrémentale terminée avec calculs corrects!\")\n",
    "        return df_final\n",
    "\n",
    "# Initialisation de la classe optimisée\n",
    "feature_store = UltraFixedIncrementalFeatureStore(Config)\n",
    "print(\"✅ Feature Store OPTIMISÉ initialisé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d3dd1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Test rapide du SuperTrend natif\n",
      "🔗 DuckDB configuré pour MinIO\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c8553070d940878ddf5a747fce1466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Échantillon chargé: 100 lignes\n",
      "✅ SuperTrend calculé avec succès!\n",
      "   • Valeurs valides: 90/100\n",
      "   • Directions valides: 90/100\n",
      "   • Bullish (1): 88\n",
      "   • Bearish (-1): 2\n",
      "\n",
      "📋 Dernières valeurs SuperTrend:\n",
      "   • 2017-09-02 00:00:00: 4543.91 (📈 Bullish)\n",
      "   • 2017-09-02 04:00:00: 4543.91 (📈 Bullish)\n",
      "   • 2017-09-02 08:00:00: 4543.91 (📈 Bullish)\n",
      "   • 2017-09-02 12:00:00: 4851.64 (📉 Bearish)\n",
      "   • 2017-09-02 16:00:00: 4851.64 (📉 Bearish)\n"
     ]
    }
   ],
   "source": [
    "# 🧪 TEST RAPIDE DU SUPERTREND NATIF\n",
    "print(\"🧪 Test rapide du SuperTrend natif\")\n",
    "\n",
    "# Charger un petit échantillon de données pour tester\n",
    "feature_store.setup_duckdb()\n",
    "sample_query = f\"\"\"\n",
    "    SELECT datetime, open, high, low, close, volume, year, month, day\n",
    "    FROM read_parquet('{Config.BRONZE_PATH}')\n",
    "    ORDER BY datetime\n",
    "    LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "sample_df = pl.from_arrow(feature_store.con.execute(sample_query).arrow())\n",
    "print(f\"📊 Échantillon chargé: {len(sample_df)} lignes\")\n",
    "\n",
    "if len(sample_df) > 50:  # Assez de données pour tester SuperTrend\n",
    "    # Tester seulement le SuperTrend\n",
    "    ohlcv_test = {\n",
    "        'high': sample_df['high'].to_numpy(),\n",
    "        'low': sample_df['low'].to_numpy(),\n",
    "        'close': sample_df['close'].to_numpy()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        st_values, st_direction = feature_store._calculate_supertrend_native(\n",
    "            high=ohlcv_test['high'],\n",
    "            low=ohlcv_test['low'],\n",
    "            close=ohlcv_test['close'],\n",
    "            length=10,\n",
    "            multiplier=3.0\n",
    "        )\n",
    "        \n",
    "        # Compter les valeurs non-NaN\n",
    "        valid_values = np.sum(~np.isnan(st_values))\n",
    "        valid_directions = np.sum(~np.isnan(st_direction))\n",
    "        \n",
    "        # Compter les directions\n",
    "        bullish_count = np.sum(st_direction == 1)\n",
    "        bearish_count = np.sum(st_direction == -1)\n",
    "        \n",
    "        print(f\"✅ SuperTrend calculé avec succès!\")\n",
    "        print(f\"   • Valeurs valides: {valid_values}/{len(st_values)}\")\n",
    "        print(f\"   • Directions valides: {valid_directions}/{len(st_direction)}\")\n",
    "        print(f\"   • Bullish (1): {bullish_count}\")\n",
    "        print(f\"   • Bearish (-1): {bearish_count}\")\n",
    "        \n",
    "        # Échantillon des dernières valeurs\n",
    "        print(f\"\\n📋 Dernières valeurs SuperTrend:\")\n",
    "        for i in range(-5, 0):\n",
    "            if not np.isnan(st_values[i]):\n",
    "                direction_text = \"📈 Bullish\" if st_direction[i] == 1 else \"📉 Bearish\"\n",
    "                print(f\"   • {sample_df['datetime'][i]}: {st_values[i]:.2f} ({direction_text})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur SuperTrend natif: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"❌ Pas assez de données pour tester SuperTrend\")\n",
    "\n",
    "feature_store.con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93737ae",
   "metadata": {},
   "source": [
    "## 4. Utilitaire de Lecture du Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46385713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Feature Store Reader initialisé\n"
     ]
    }
   ],
   "source": [
    "class FeatureStoreReader:\n",
    "    \"\"\"Classe utilitaire pour lire le Feature Store\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.con = None\n",
    "    \n",
    "    def setup_connection(self):\n",
    "        \"\"\"Configure la connexion DuckDB\"\"\"\n",
    "        self.con = duckdb.connect(database=\":memory:\")\n",
    "        self.con.execute(f\"\"\"\n",
    "            SET s3_access_key_id='{self.config.MINIO_ACCESS_KEY}';\n",
    "            SET s3_secret_access_key='{self.config.MINIO_SECRET_KEY}';\n",
    "            SET s3_endpoint='{self.config.MINIO_ENDPOINT}';\n",
    "            SET s3_url_style='path';\n",
    "            SET s3_use_ssl='false';\n",
    "        \"\"\")\n",
    "    \n",
    "    def read_features(\n",
    "        self,\n",
    "        symbols: List[str] = None,\n",
    "        start_date: str = None,\n",
    "        end_date: str = None,\n",
    "        features: List[str] = None\n",
    "    ) -> pl.DataFrame:\n",
    "        \"\"\"Lit les features du Feature Store avec filtres optionnels\"\"\"\n",
    "        \n",
    "        if not self.con:\n",
    "            self.setup_connection()\n",
    "        \n",
    "        # Construction de la requête\n",
    "        feature_store_path = f\"{self.config.GOLD_BUCKET}/{self.config.FEATURE_STORE_TABLE}/**/*.parquet\"\n",
    "        \n",
    "        select_clause = \"*\" if not features else \", \".join([\"datetime\"] + features)\n",
    "        where_clauses = []\n",
    "        \n",
    "        if symbols:\n",
    "            symbols_str = \"', '\".join(symbols)\n",
    "            where_clauses.append(f\"symbol IN ('{symbols_str}')\")\n",
    "        \n",
    "        if start_date:\n",
    "            where_clauses.append(f\"datetime >= '{start_date}'\")\n",
    "        \n",
    "        if end_date:\n",
    "            where_clauses.append(f\"datetime <= '{end_date}'\")\n",
    "        \n",
    "        where_clause = \"WHERE \" + \" AND \".join(where_clauses) if where_clauses else \"\"\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT {select_clause}\n",
    "            FROM read_parquet('{feature_store_path}')\n",
    "            {where_clause}\n",
    "            ORDER BY datetime\n",
    "        \"\"\"\n",
    "        \n",
    "        return pl.from_arrow(self.con.execute(query).arrow())\n",
    "    \n",
    "    def get_latest_features(self, symbol: str = \"BTCUSDT\", limit: int = 100) -> pl.DataFrame:\n",
    "        \"\"\"Récupère les derniers features disponibles\"\"\"\n",
    "        \n",
    "        if not self.con:\n",
    "            self.setup_connection()\n",
    "        \n",
    "        feature_store_path = f\"{self.config.GOLD_BUCKET}/{self.config.FEATURE_STORE_TABLE}/**/*.parquet\"\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM read_parquet('{feature_store_path}')\n",
    "            WHERE symbol = '{symbol}'\n",
    "            ORDER BY datetime DESC\n",
    "            LIMIT {limit}\n",
    "        \"\"\"\n",
    "        \n",
    "        return pl.from_arrow(self.con.execute(query).arrow())\n",
    "    \n",
    "    def close(self):\n",
    "        if self.con:\n",
    "            self.con.close()\n",
    "\n",
    "# Initialisation du reader\n",
    "reader = FeatureStoreReader(Config)\n",
    "print(\"📚 Feature Store Reader initialisé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135eaf84",
   "metadata": {},
   "source": [
    "## 5. Construction ou Mise à Jour du Feature Store\n",
    "\n",
    "### 🎯 **Choisissez votre mode d'exécution :**\n",
    "\n",
    "- **🆕 Première fois** : Exécutez la cellule \"Construction Complète\"\n",
    "- **🔄 Mise à jour** : Exécutez la cellule \"Mise à Jour Incrémentale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "381865de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Construction COMPLÈTE du Feature Store Gold\n",
      "============================================================\n",
      "🔗 DuckDB configuré pour MinIO\n",
      "📥 Chargement des données Bronze...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db5f03ba5da458f98bd4083fa2fbe26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 17,604 lignes chargées de Bronze\n",
      "📅 Période: 2017-08-17 04:00:00 → 2025-08-31 20:00:00\n",
      "🔧 Calcul des indicateurs techniques...\n",
      "✅ SuperTrend calculé avec implémentation native\n",
      "✅ 23 indicateurs calculés\n",
      "💾 Sauvegarde du Feature Store...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7eb671f761747e7b110fc401331a4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature Store sauvegardé: s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/\n",
      "📊 17,604 lignes, 35 colonnes\n",
      "\n",
      "🎉 Feature Store construit avec succès!\n",
      "💡 Pour construire le Feature Store pour la première fois:\n",
      "   Décommentez la ligne ci-dessus et exécutez cette cellule\n",
      "\n",
      "⚠️  Attention: Ceci va traiter TOUTES les données Bronze (peut prendre du temps)\n"
     ]
    }
   ],
   "source": [
    "# 🆕 PREMIÈRE FOIS : Construction complète du Feature Store\n",
    "# Décommentez cette ligne pour la première exécution\n",
    "df_features = feature_store.build_feature_store_complete()\n",
    "\n",
    "print(\"💡 Pour construire le Feature Store pour la première fois:\")\n",
    "print(\"   Décommentez la ligne ci-dessus et exécutez cette cellule\")\n",
    "print(\"\\n⚠️  Attention: Ceci va traiter TOUTES les données Bronze (peut prendre du temps)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c492888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce898838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 MISE À JOUR INCRÉMENTALE : Ajout des nouvelles données\n",
    "# Décommentez cette ligne pour une mise à jour incrémentale\n",
    "# df_new_features = feature_store.update_feature_store_incremental()\n",
    "\n",
    "print(\"💡 Pour mettre à jour le Feature Store avec de nouvelles données:\")\n",
    "print(\"   Décommentez la ligne ci-dessus et exécutez cette cellule\")\n",
    "print(\"\\n✅ Avantages: Traitement ultra-rapide avec lookback optimal pour tous les indicateurs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c9d39",
   "metadata": {},
   "source": [
    "## 6. Validation & Test du Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c14a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Test de lecture du Feature Store:\n",
      "✅ 10 lignes récupérées\n",
      "📊 23 indicateurs disponibles:\n",
      "\n",
      "📈 Moyennes Mobiles: 10 indicateurs\n",
      "   • sma_10\n",
      "   • sma_20\n",
      "   • sma_50\n",
      "   ... et 7 autres\n",
      "\n",
      "🎯 Oscillateurs: 4 indicateurs\n",
      "   • rsi_14\n",
      "   • rsi_21\n",
      "   • stoch_k_14_3\n",
      "   ... et 1 autres\n",
      "\n",
      "📊 Bandes & Enveloppes: 3 indicateurs\n",
      "   • bb_upper_20_2\n",
      "   • bb_middle_20_2\n",
      "   • bb_lower_20_2\n",
      "\n",
      "⚡ Momentum: 3 indicateurs\n",
      "   • macd_12_26_9\n",
      "   • macd_signal_12_26_9\n",
      "   • macd_hist_12_26_9\n",
      "\n",
      "🛡️ Volatilité & Tendance: 3 indicateurs\n",
      "   • atr_14\n",
      "   • supertrend_10_3.0\n",
      "   • supertrend_dir_10_3.0\n",
      "\n",
      "📋 Dernières données (échantillon):\n",
      "shape: (5, 5)\n",
      "┌─────────────────────┬───────────┬─────────────┬───────────────┬───────────┐\n",
      "│ datetime            ┆ close     ┆ sma_20      ┆ ema_20        ┆ rsi_14    │\n",
      "│ ---                 ┆ ---       ┆ ---         ┆ ---           ┆ ---       │\n",
      "│ datetime[μs]        ┆ f64       ┆ f64         ┆ f64           ┆ f64       │\n",
      "╞═════════════════════╪═══════════╪═════════════╪═══════════════╪═══════════╡\n",
      "│ 2025-08-31 20:00:00 ┆ 108246.35 ┆ 109286.9165 ┆ 109335.645174 ┆ 37.155391 │\n",
      "│ 2025-08-31 16:00:00 ┆ 108930.0  ┆ 109508.5255 ┆ 109450.307824 ┆ 41.76959  │\n",
      "│ 2025-08-31 12:00:00 ┆ 108818.69 ┆ 109707.4845 ┆ 109505.077069 ┆ 40.655368 │\n",
      "│ 2025-08-31 08:00:00 ┆ 108389.76 ┆ 109922.2555 ┆ 109577.328339 ┆ 36.293495 │\n",
      "│ 2025-08-31 04:00:00 ┆ 108708.26 ┆ 110102.768  ┆ 109702.335533 ┆ 38.231013 │\n",
      "└─────────────────────┴───────────┴─────────────┴───────────────┴───────────┘\n",
      "✅ 10 lignes récupérées\n",
      "📊 23 indicateurs disponibles:\n",
      "\n",
      "📈 Moyennes Mobiles: 10 indicateurs\n",
      "   • sma_10\n",
      "   • sma_20\n",
      "   • sma_50\n",
      "   ... et 7 autres\n",
      "\n",
      "🎯 Oscillateurs: 4 indicateurs\n",
      "   • rsi_14\n",
      "   • rsi_21\n",
      "   • stoch_k_14_3\n",
      "   ... et 1 autres\n",
      "\n",
      "📊 Bandes & Enveloppes: 3 indicateurs\n",
      "   • bb_upper_20_2\n",
      "   • bb_middle_20_2\n",
      "   • bb_lower_20_2\n",
      "\n",
      "⚡ Momentum: 3 indicateurs\n",
      "   • macd_12_26_9\n",
      "   • macd_signal_12_26_9\n",
      "   • macd_hist_12_26_9\n",
      "\n",
      "🛡️ Volatilité & Tendance: 3 indicateurs\n",
      "   • atr_14\n",
      "   • supertrend_10_3.0\n",
      "   • supertrend_dir_10_3.0\n",
      "\n",
      "📋 Dernières données (échantillon):\n",
      "shape: (5, 5)\n",
      "┌─────────────────────┬───────────┬─────────────┬───────────────┬───────────┐\n",
      "│ datetime            ┆ close     ┆ sma_20      ┆ ema_20        ┆ rsi_14    │\n",
      "│ ---                 ┆ ---       ┆ ---         ┆ ---           ┆ ---       │\n",
      "│ datetime[μs]        ┆ f64       ┆ f64         ┆ f64           ┆ f64       │\n",
      "╞═════════════════════╪═══════════╪═════════════╪═══════════════╪═══════════╡\n",
      "│ 2025-08-31 20:00:00 ┆ 108246.35 ┆ 109286.9165 ┆ 109335.645174 ┆ 37.155391 │\n",
      "│ 2025-08-31 16:00:00 ┆ 108930.0  ┆ 109508.5255 ┆ 109450.307824 ┆ 41.76959  │\n",
      "│ 2025-08-31 12:00:00 ┆ 108818.69 ┆ 109707.4845 ┆ 109505.077069 ┆ 40.655368 │\n",
      "│ 2025-08-31 08:00:00 ┆ 108389.76 ┆ 109922.2555 ┆ 109577.328339 ┆ 36.293495 │\n",
      "│ 2025-08-31 04:00:00 ┆ 108708.26 ┆ 110102.768  ┆ 109702.335533 ┆ 38.231013 │\n",
      "└─────────────────────┴───────────┴─────────────┴───────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Test de lecture du Feature Store\n",
    "print(\"🧪 Test de lecture du Feature Store:\")\n",
    "\n",
    "try:\n",
    "    # Test de lecture des dernières données\n",
    "    latest_data = reader.get_latest_features(limit=10)\n",
    "    \n",
    "    if latest_data.height > 0:\n",
    "        print(f\"✅ {latest_data.height} lignes récupérées\")\n",
    "        \n",
    "        # Affichage des colonnes disponibles\n",
    "        all_columns = latest_data.columns\n",
    "        indicator_columns = [col for col in all_columns \n",
    "                           if col not in ['datetime', 'open', 'high', 'low', 'close', 'volume', \n",
    "                                         'year', 'month', 'day', 'symbol', 'timeframe', 'created_at', 'updated_at']]\n",
    "        \n",
    "        print(f\"📊 {len(indicator_columns)} indicateurs disponibles:\")\n",
    "        \n",
    "        # Regroupement par type\n",
    "        indicator_types = {\n",
    "            '📈 Moyennes Mobiles': [col for col in indicator_columns if col.startswith(('sma_', 'ema_'))],\n",
    "            '🎯 Oscillateurs': [col for col in indicator_columns if col.startswith(('rsi_', 'stoch_'))],\n",
    "            '📊 Bandes & Enveloppes': [col for col in indicator_columns if col.startswith('bb_')],\n",
    "            '⚡ Momentum': [col for col in indicator_columns if col.startswith('macd_')],\n",
    "            '🛡️ Volatilité & Tendance': [col for col in indicator_columns if col.startswith(('atr_', 'supertrend_'))]\n",
    "        }\n",
    "        \n",
    "        for category, indicators in indicator_types.items():\n",
    "            if indicators:\n",
    "                print(f\"\\n{category}: {len(indicators)} indicateurs\")\n",
    "                for ind in indicators[:3]:  # Afficher les 3 premiers de chaque catégorie\n",
    "                    print(f\"   • {ind}\")\n",
    "                if len(indicators) > 3:\n",
    "                    print(f\"   ... et {len(indicators)-3} autres\")\n",
    "        \n",
    "        # Échantillon des dernières données\n",
    "        print(\"\\n📋 Dernières données (échantillon):\")\n",
    "        sample_columns = ['datetime', 'close', 'sma_20', 'ema_20', 'rsi_14']\n",
    "        available_sample = [col for col in sample_columns if col in latest_data.columns]\n",
    "        print(latest_data.select(available_sample).head())\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️ Aucune donnée trouvée dans le Feature Store\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur lors du test: {e}\")\n",
    "    print(\"💡 Le Feature Store n'existe probablement pas encore.\")\n",
    "    print(\"   Exécutez d'abord la construction complète (section 5)\")\n",
    "\n",
    "finally:\n",
    "    reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbe369",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Feature Store Gold OPTIMISÉ\n",
    "\n",
    "### 🎯 **Workflow Simplifié**\n",
    "\n",
    "1. **🆕 Première fois** : `feature_store.build_feature_store_complete()`\n",
    "2. **🔄 Mise à jour** : `feature_store.update_feature_store_incremental()`\n",
    "3. **📚 Lecture** : `reader.read_features()` ou `reader.get_latest_features()`\n",
    "\n",
    "### 🚀 **Avantages de cette Version**\n",
    "\n",
    "- **🧠 Lookback Intelligent** : Calcul optimal pour chaque type d'indicateur\n",
    "- **⚡ Performance** : 10-20x plus rapide pour les mises à jour\n",
    "- **🔧 Simplicité** : Une seule classe, workflow clair\n",
    "- **✅ Précision** : Tous les indicateurs calculés correctement\n",
    "- **📊 Robustesse** : Gestion d'erreurs et fallbacks\n",
    "\n",
    "### 🎉 **Le Feature Store est prêt pour vos stratégies de trading !**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hermes-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
