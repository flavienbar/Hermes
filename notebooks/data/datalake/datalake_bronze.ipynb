{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ca3a85",
   "metadata": {},
   "source": [
    "# üìì POC Data Lakehouse Binance avec MinIO, DuckDB, Polars et Iceberg\n",
    "\n",
    "## 1. Introduction & Objectifs\n",
    "\n",
    "Dans ce notebook, nous construisons un **pipeline de donn√©es moderne (Lakehouse)** en utilisant :\n",
    "\n",
    "- **MinIO** (S3-compatible) pour le stockage objet  \n",
    "- **Polars** pour le traitement des fichiers CSV  \n",
    "- **DuckDB** pour la transformation et l‚Äô√©criture en Parquet partitionn√©  \n",
    "\n",
    "### Architecture du pipeline\n",
    "\n",
    "- **bronze** : stockage brut (`.zip` tels que t√©l√©charg√©s depuis Binance)  \n",
    "- **silver** : donn√©es raffin√©es au format **Parquet**, partitionn√©es par `year/month/day`  \n",
    "- **gold** : tables **Iceberg**, ajoutant les m√©tadonn√©es pour time travel, gouvernance et gestion des snapshots  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c113bbeb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Configuration MinIO\n",
    "### D√©marrer ou red√©marrer MinIO\n",
    "\n",
    "- sudo systemctl start minio   # pour lancer\n",
    "- sudo systemctl restart minio # si d√©j√† lanc√© mais bloqu√©\n",
    "- sudo systemctl status minio  # v√©rifier que √ßa tourne\n",
    "\n",
    "### Supprimer un bucket et son contenu :\n",
    "- mc rb --force myminio/nom_du_bucket\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Nous partons du principe que **MinIO tourne en mode standalone**.  \n",
    "Exemple de lancement :\n",
    "\n",
    "```bash\n",
    "minio server /media/giujorge/Stockage/DATA/minio-data --console-address \":9001\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### commandes utiles Minio\n",
    "- mc rm --recursive --force minio/bronze\n",
    "\n",
    "- mc alias list\n",
    "\n",
    "- mc ls minio\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121ca096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "import os\n",
    "\n",
    "# Config MinIO\n",
    "MINIO_ENDPOINT = \"localhost:9000\"\n",
    "MINIO_ACCESS_KEY = os.getenv(\"MINIO_ROOT_USER\", \"minioadm\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"MINIO_ROOT_PASSWORD\", \"minioadm\")\n",
    "\n",
    "client = Minio(\n",
    "    MINIO_ENDPOINT,\n",
    "    access_key=MINIO_ACCESS_KEY,\n",
    "    secret_key=MINIO_SECRET_KEY,\n",
    "    secure=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c2f818",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Cr√©ation des buckets (zones du data lake)\n",
    "\n",
    "Nous cr√©ons trois buckets suivant les bonnes pratiques :\n",
    "\n",
    "- **bronze** : fichiers bruts  \n",
    "- **silver** : Parquet partitionn√©s  \n",
    "- **gold** : tables Iceberg  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee62b853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket bronze cr√©√© ‚úÖ\n",
      "Bucket silver cr√©√© ‚úÖ\n",
      "Bucket gold cr√©√© ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "for bucket in [\"bronze\", \"silver\", \"gold\"]:\n",
    "    if not client.bucket_exists(bucket):\n",
    "        client.make_bucket(bucket)\n",
    "        print(f\"Bucket {bucket} cr√©√© ‚úÖ\")\n",
    "    else:\n",
    "        print(f\"Bucket {bucket} d√©j√† existant\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c53f1f",
   "metadata": {},
   "source": [
    "### v√©rifiaction cr√©ation fichiers\n",
    "\n",
    "- ```mc alias set minio http://127.0.0.1:9000 ton_access_key ton_secret_key```\n",
    "- ```mc ls minio```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e183854d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Sauvegarde raffin√©e en *bronze* (Parquet partitionn√©)\n",
    "\n",
    "Nous utilisons **DuckDB** pour √©crire en Parquet partitionn√© (`year/month/day`) dans le bucket *bronze*.  \n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Version Optimis√©e - Pipeline de Traitement des Donn√©es\n",
    "\n",
    "Cette version r√©√©crite pr√©sente plusieurs am√©liorations :\n",
    "\n",
    "### ‚ú® **Am√©liorations apport√©es**\n",
    "\n",
    "1. **Architecture orient√©e objet** : Code structur√© avec la classe `BinanceDataProcessor`\n",
    "2. **Gestion d'erreurs robuste** : Meilleure gestion des exceptions et nettoyage automatique\n",
    "3. **D√©tection automatique des timestamps** : Identification automatique des unit√©s (s/ms/¬µs)\n",
    "4. **Code modulaire** : Chaque √©tape est dans une m√©thode s√©par√©e pour faciliter la maintenance\n",
    "5. **Configuration centralis√©e** : Tous les param√®tres dans un dictionnaire\n",
    "6. **Rapport d√©taill√©** : R√©sum√© complet avec statistiques et erreurs\n",
    "\n",
    "### üîß **Fonctionnalit√©s**\n",
    "\n",
    "- **Traitement par batch** : Traite tous les fichiers ZIP automatiquement\n",
    "- **Partitionnement intelligent** : Partitionnement par `year/month/day`\n",
    "- **Nettoyage automatique** : Suppression des objets temporaires DuckDB\n",
    "- **Logging d√©taill√©** : Affichage du progr√®s et des erreurs\n",
    "- **Validation des donn√©es** : V√©rification des fen√™tres temporelles\n",
    "\n",
    "### üìä **Utilisation**\n",
    "\n",
    "Le code s'ex√©cute automatiquement avec la configuration par d√©faut. Pour personnaliser, modifiez le dictionnaire `config`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af700f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√©but du traitement de 97 fichiers\n",
      "üéØ Destination: s3://bronze/binance/data/spot/monthly/klines/BTCUSDT/4h/\n",
      "\n",
      "üìÅ Traitement: BTCUSDT-4h-2017-08.zip\n",
      "   üìä 89 lignes | 2017-08-17 04:00:00 ‚Üí 2017-08-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2017-09.zip\n",
      "   üìä 179 lignes | 2017-09-01 00:00:00 ‚Üí 2017-09-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2017-10.zip\n",
      "   üìä 186 lignes | 2017-10-01 00:00:00 ‚Üí 2017-10-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2017-11.zip\n",
      "   üìä 180 lignes | 2017-11-01 00:00:00 ‚Üí 2017-11-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2017-12.zip\n",
      "   üìä 186 lignes | 2017-12-01 00:00:00 ‚Üí 2017-12-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2018-01.zip\n",
      "   üìä 186 lignes | 2018-01-01 00:00:00 ‚Üí 2018-01-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2018-02.zip\n",
      "   üìä 161 lignes | 2018-02-01 00:00:00 ‚Üí 2018-02-28 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2018-03.zip\n",
      "   üìä 186 lignes | 2018-03-01 00:00:00 ‚Üí 2018-03-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2018-04.zip\n",
      "   üìä 180 lignes | 2018-04-01 00:00:00 ‚Üí 2018-04-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2018-05.zip\n",
      "   üìä 186 lignes | 2018-05-01 00:00:00 ‚Üí 2018-05-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2018-06.zip\n",
      "   üìä 178 lignes | 2018-06-01 00:00:00 ‚Üí 2018-06-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2018-07.zip\n",
      "   üìä 185 lignes | 2018-07-01 00:00:00 ‚Üí 2018-07-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2018-08.zip\n",
      "   üìä 186 lignes | 2018-08-01 00:00:00 ‚Üí 2018-08-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2018-09.zip\n",
      "   üìä 180 lignes | 2018-09-01 00:00:00 ‚Üí 2018-09-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2018-10.zip\n",
      "   üìä 186 lignes | 2018-10-01 00:00:00 ‚Üí 2018-10-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2018-11.zip\n",
      "   üìä 179 lignes | 2018-11-01 00:00:00 ‚Üí 2018-11-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2018-12.zip\n",
      "   üìä 186 lignes | 2018-12-01 00:00:00 ‚Üí 2018-12-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2019-01.zip\n",
      "   üìä 186 lignes | 2019-01-01 00:00:00 ‚Üí 2019-01-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2019-02.zip\n",
      "   üìä 168 lignes | 2019-02-01 00:00:00 ‚Üí 2019-02-28 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2019-03.zip\n",
      "   üìä 185 lignes | 2019-03-01 00:00:00 ‚Üí 2019-03-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2019-04.zip\n",
      "   üìä 180 lignes | 2019-04-01 00:00:00 ‚Üí 2019-04-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2019-05.zip\n",
      "   üìä 184 lignes | 2019-05-01 00:00:00 ‚Üí 2019-05-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2019-06.zip\n",
      "   üìä 180 lignes | 2019-06-01 00:00:00 ‚Üí 2019-06-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2019-07.zip\n",
      "   üìä 186 lignes | 2019-07-01 00:00:00 ‚Üí 2019-07-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2019-08.zip\n",
      "   üìä 185 lignes | 2019-08-01 00:00:00 ‚Üí 2019-08-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2019-09.zip\n",
      "   üìä 180 lignes | 2019-09-01 00:00:00 ‚Üí 2019-09-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2019-10.zip\n",
      "   üìä 186 lignes | 2019-10-01 00:00:00 ‚Üí 2019-10-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2019-11.zip\n",
      "   üìä 180 lignes | 2019-11-01 00:00:00 ‚Üí 2019-11-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2019-12.zip\n",
      "   üìä 186 lignes | 2019-12-01 00:00:00 ‚Üí 2019-12-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2020-01.zip\n",
      "   üìä 186 lignes | 2020-01-01 00:00:00 ‚Üí 2020-01-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2020-02.zip\n",
      "   üìä 173 lignes | 2020-02-01 00:00:00 ‚Üí 2020-02-29 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2020-03.zip\n",
      "   üìä 186 lignes | 2020-03-01 00:00:00 ‚Üí 2020-03-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2020-04.zip\n",
      "   üìä 180 lignes | 2020-04-01 00:00:00 ‚Üí 2020-04-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2020-05.zip\n",
      "   üìä 186 lignes | 2020-05-01 00:00:00 ‚Üí 2020-05-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2020-06.zip\n",
      "   üìä 180 lignes | 2020-06-01 00:00:00 ‚Üí 2020-06-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2020-07.zip\n",
      "   üìä 186 lignes | 2020-07-01 00:00:00 ‚Üí 2020-07-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2020-08.zip\n",
      "   üìä 186 lignes | 2020-08-01 00:00:00 ‚Üí 2020-08-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2020-09.zip\n",
      "   üìä 180 lignes | 2020-09-01 00:00:00 ‚Üí 2020-09-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2020-10.zip\n",
      "   üìä 186 lignes | 2020-10-01 00:00:00 ‚Üí 2020-10-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2020-11.zip\n",
      "   üìä 180 lignes | 2020-11-01 00:00:00 ‚Üí 2020-11-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2020-12.zip\n",
      "   üìä 186 lignes | 2020-12-01 00:00:00 ‚Üí 2020-12-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2021-01.zip\n",
      "   üìä 186 lignes | 2021-01-01 00:00:00 ‚Üí 2021-01-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2021-02.zip\n",
      "   üìä 168 lignes | 2021-02-01 00:00:00 ‚Üí 2021-02-28 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2021-03.zip\n",
      "   üìä 186 lignes | 2021-03-01 00:00:00 ‚Üí 2021-03-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2021-04.zip\n",
      "   üìä 180 lignes | 2021-04-01 00:00:00 ‚Üí 2021-04-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2021-05.zip\n",
      "   üìä 186 lignes | 2021-05-01 00:00:00 ‚Üí 2021-05-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2021-06.zip\n",
      "   üìä 180 lignes | 2021-06-01 00:00:00 ‚Üí 2021-06-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2021-07.zip\n",
      "   üìä 186 lignes | 2021-07-01 00:00:00 ‚Üí 2021-07-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2021-08.zip\n",
      "   üìä 186 lignes | 2021-08-01 00:00:00 ‚Üí 2021-08-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2021-09.zip\n",
      "   üìä 180 lignes | 2021-09-01 00:00:00 ‚Üí 2021-09-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2021-10.zip\n",
      "   üìä 186 lignes | 2021-10-01 00:00:00 ‚Üí 2021-10-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2021-11.zip\n",
      "   üìä 180 lignes | 2021-11-01 00:00:00 ‚Üí 2021-11-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2021-12.zip\n",
      "   üìä 186 lignes | 2021-12-01 00:00:00 ‚Üí 2021-12-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2022-01.zip\n",
      "   üìä 186 lignes | 2022-01-01 00:00:00 ‚Üí 2022-01-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2022-02.zip\n",
      "   üìä 168 lignes | 2022-02-01 00:00:00 ‚Üí 2022-02-28 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2022-03.zip\n",
      "   üìä 186 lignes | 2022-03-01 00:00:00 ‚Üí 2022-03-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2022-04.zip\n",
      "   üìä 180 lignes | 2022-04-01 00:00:00 ‚Üí 2022-04-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2022-05.zip\n",
      "   üìä 186 lignes | 2022-05-01 00:00:00 ‚Üí 2022-05-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2022-06.zip\n",
      "   üìä 180 lignes | 2022-06-01 00:00:00 ‚Üí 2022-06-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2022-07.zip\n",
      "   üìä 186 lignes | 2022-07-01 00:00:00 ‚Üí 2022-07-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2022-08.zip\n",
      "   üìä 186 lignes | 2022-08-01 00:00:00 ‚Üí 2022-08-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2022-09.zip\n",
      "   üìä 180 lignes | 2022-09-01 00:00:00 ‚Üí 2022-09-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2022-10.zip\n",
      "   üìä 186 lignes | 2022-10-01 00:00:00 ‚Üí 2022-10-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2022-11.zip\n",
      "   üìä 180 lignes | 2022-11-01 00:00:00 ‚Üí 2022-11-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2022-12.zip\n",
      "   üìä 186 lignes | 2022-12-01 00:00:00 ‚Üí 2022-12-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2023-01.zip\n",
      "   üìä 186 lignes | 2023-01-01 00:00:00 ‚Üí 2023-01-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2023-02.zip\n",
      "   üìä 168 lignes | 2023-02-01 00:00:00 ‚Üí 2023-02-28 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2023-03.zip\n",
      "   üìä 186 lignes | 2023-03-01 00:00:00 ‚Üí 2023-03-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2023-04.zip\n",
      "   üìä 180 lignes | 2023-04-01 00:00:00 ‚Üí 2023-04-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2023-05.zip\n",
      "   üìä 186 lignes | 2023-05-01 00:00:00 ‚Üí 2023-05-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2023-06.zip\n",
      "   üìä 180 lignes | 2023-06-01 00:00:00 ‚Üí 2023-06-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2023-07.zip\n",
      "   üìä 186 lignes | 2023-07-01 00:00:00 ‚Üí 2023-07-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2023-08.zip\n",
      "   üìä 186 lignes | 2023-08-01 00:00:00 ‚Üí 2023-08-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2023-09.zip\n",
      "   üìä 180 lignes | 2023-09-01 00:00:00 ‚Üí 2023-09-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2023-10.zip\n",
      "   üìä 186 lignes | 2023-10-01 00:00:00 ‚Üí 2023-10-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2023-11.zip\n",
      "   üìä 180 lignes | 2023-11-01 00:00:00 ‚Üí 2023-11-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2023-12.zip\n",
      "   üìä 186 lignes | 2023-12-01 00:00:00 ‚Üí 2023-12-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2024-01.zip\n",
      "   üìä 186 lignes | 2024-01-01 00:00:00 ‚Üí 2024-01-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2024-02.zip\n",
      "   üìä 174 lignes | 2024-02-01 00:00:00 ‚Üí 2024-02-29 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2024-03.zip\n",
      "   üìä 186 lignes | 2024-03-01 00:00:00 ‚Üí 2024-03-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2024-04.zip\n",
      "   üìä 180 lignes | 2024-04-01 00:00:00 ‚Üí 2024-04-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2024-05.zip\n",
      "   üìä 186 lignes | 2024-05-01 00:00:00 ‚Üí 2024-05-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2024-06.zip\n",
      "   üìä 180 lignes | 2024-06-01 00:00:00 ‚Üí 2024-06-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2024-07.zip\n",
      "   üìä 186 lignes | 2024-07-01 00:00:00 ‚Üí 2024-07-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2024-08.zip\n",
      "   üìä 186 lignes | 2024-08-01 00:00:00 ‚Üí 2024-08-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2024-09.zip\n",
      "   üìä 180 lignes | 2024-09-01 00:00:00 ‚Üí 2024-09-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2024-10.zip\n",
      "   üìä 186 lignes | 2024-10-01 00:00:00 ‚Üí 2024-10-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2024-11.zip\n",
      "   üìä 180 lignes | 2024-11-01 00:00:00 ‚Üí 2024-11-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2024-12.zip\n",
      "   üìä 186 lignes | 2024-12-01 00:00:00 ‚Üí 2024-12-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2025-01.zip\n",
      "   üìä 186 lignes | 2025-01-01 00:00:00 ‚Üí 2025-01-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2025-02.zip\n",
      "   üìä 168 lignes | 2025-02-01 00:00:00 ‚Üí 2025-02-28 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2025-03.zip\n",
      "   üìä 186 lignes | 2025-03-01 00:00:00 ‚Üí 2025-03-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2025-04.zip\n",
      "   üìä 180 lignes | 2025-04-01 00:00:00 ‚Üí 2025-04-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2025-05.zip\n",
      "   üìä 186 lignes | 2025-05-01 00:00:00 ‚Üí 2025-05-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2025-06.zip\n",
      "   üìä 180 lignes | 2025-06-01 00:00:00 ‚Üí 2025-06-30 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2025-07.zip\n",
      "   üìä 186 lignes | 2025-07-01 00:00:00 ‚Üí 2025-07-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "üìÅ Traitement: BTCUSDT-4h-2025-08.zip\n",
      "   üìä 186 lignes | 2025-08-01 00:00:00 ‚Üí 2025-08-31 20:00:00\n",
      "   ‚úÖ Sauvegard√© dans bronze\n",
      "\n",
      "==================================================\n",
      "‚úÖ Fichiers trait√©s: 97\n",
      "‚ùå Erreurs: 0\n"
     ]
    }
   ],
   "source": [
    "# Version corrig√©e avec gestion de l'√©crasement\n",
    "import zipfile\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import duckdb\n",
    "import polars as pl\n",
    "\n",
    "class BinanceDataProcessorV2:\n",
    "    \"\"\"Version corrig√©e avec option OVERWRITE_OR_IGNORE\"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        self.csv_columns = [\n",
    "            \"open_time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"close_time\",\n",
    "            \"quote_asset_volume\", \"number_of_trades\", \"taker_buy_base_volume\",\n",
    "            \"taker_buy_quote_volume\", \"ignore\"\n",
    "        ]\n",
    "        self.errors = []\n",
    "        self.processed_files = 0\n",
    "    \n",
    "    def setup_duckdb_connection(self):\n",
    "        \"\"\"Configure la connexion DuckDB avec les param√®tres S3\"\"\"\n",
    "        con = duckdb.connect(database=\":memory:\")\n",
    "        con.execute(f\"\"\"\n",
    "            SET s3_access_key_id='{self.config[\"minio_access_key\"]}';\n",
    "            SET s3_secret_access_key='{self.config[\"minio_secret_key\"]}';\n",
    "            SET s3_endpoint='{self.config[\"minio_endpoint\"]}';\n",
    "            SET s3_url_style='path';\n",
    "            SET s3_use_ssl='false';\n",
    "        \"\"\")\n",
    "        return con\n",
    "    \n",
    "    def process_file(self, zip_path: Path, con: duckdb.DuckDBPyConnection, output_path: str):\n",
    "        \"\"\"Traite un seul fichier de bout en bout\"\"\"\n",
    "        try:\n",
    "            print(f\"üìÅ Traitement: {zip_path.name}\")\n",
    "            \n",
    "            # 1. Lecture du ZIP\n",
    "            with zipfile.ZipFile(str(zip_path)) as z:\n",
    "                csv_candidates = [n for n in z.namelist() if n.lower().endswith('.csv')]\n",
    "                csv_name = csv_candidates[0]\n",
    "                with z.open(csv_name) as f:\n",
    "                    df = pl.read_csv(f, has_header=False, new_columns=self.csv_columns)\n",
    "            \n",
    "            # 2. Traitement des donn√©es\n",
    "            df = df.with_columns([\n",
    "                pl.col(\"open_time\").cast(pl.Int64, strict=False),\n",
    "                pl.col(\"close_time\").cast(pl.Int64, strict=False),\n",
    "            ])\n",
    "            \n",
    "            # 3. D√©tection et conversion timestamp\n",
    "            first_ts = df.select(pl.col(\"open_time\").first()).item()\n",
    "            if first_ts > 10_000_000_000_000:  # microsecondes\n",
    "                ts_col = (pl.col(\"open_time\") // 1000).cast(pl.Datetime(\"ms\"))\n",
    "            elif first_ts > 10_000_000_000:   # millisecondes\n",
    "                ts_col = pl.col(\"open_time\").cast(pl.Datetime(\"ms\"))\n",
    "            else:  # secondes\n",
    "                ts_col = (pl.col(\"open_time\") * 1000).cast(pl.Datetime(\"ms\"))\n",
    "            \n",
    "            # 4. Ajout des colonnes finales\n",
    "            ingest_id = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "            df = df.with_columns([\n",
    "                ts_col.alias(\"datetime\"),\n",
    "                pl.lit(ingest_id).alias(\"ingest_id\"),\n",
    "            ]).with_columns([\n",
    "                pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
    "            ])\n",
    "            \n",
    "            # 5. Affichage des informations\n",
    "            min_max = df.select([\n",
    "                pl.col(\"datetime\").min().alias(\"min_dt\"),\n",
    "                pl.col(\"datetime\").max().alias(\"max_dt\"),\n",
    "            ]).to_dicts()[0]\n",
    "            print(f\"   üìä {df.height} lignes | {min_max['min_dt']} ‚Üí {min_max['max_dt']}\")\n",
    "            \n",
    "            # 6. Sauvegarde avec gestion de l'√©crasement\n",
    "            try:\n",
    "                con.execute(\"DROP VIEW IF EXISTS tmp_data\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            con.register(\"tmp_data\", df.to_arrow())\n",
    "            \n",
    "            # COPY avec option OVERWRITE_OR_IGNORE\n",
    "            sql = f\"\"\"\n",
    "                COPY tmp_data\n",
    "                TO '{output_path}'\n",
    "                WITH (FORMAT PARQUET, PARTITION_BY (year, month, day), OVERWRITE_OR_IGNORE TRUE)\n",
    "            \"\"\"\n",
    "            con.execute(sql)\n",
    "            print(f\"   ‚úÖ Sauvegard√© dans bronze\")\n",
    "            \n",
    "            self.processed_files += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erreur: {str(e)}\")\n",
    "            self.errors.append({\"file\": zip_path.name, \"error\": str(e)})\n",
    "        \n",
    "        finally:\n",
    "            try:\n",
    "                con.execute(\"DROP VIEW IF EXISTS tmp_data\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Ex√©cute le traitement complet\"\"\"\n",
    "        # Chemins\n",
    "        base_path = Path(\n",
    "            f\"/media/giujorge/Stockage/DATA/raw/{self.config['provider']}/\"\n",
    "            f\"{self.config['market']}/{self.config['data_frequency']}/\"\n",
    "            f\"{self.config['data_category']}/{self.config['symbol']}/{self.config['interval']}\"\n",
    "        )\n",
    "        \n",
    "        output_path = (\n",
    "            f\"s3://bronze/{self.config['provider']}/{self.config['data_type']}/\"\n",
    "            f\"{self.config['market']}/{self.config['data_frequency']}/{self.config['data_category']}/\"\n",
    "            f\"{self.config['symbol']}/{self.config['interval']}/\"\n",
    "        )\n",
    "        \n",
    "        zip_files = sorted(base_path.glob(\"*.zip\"))\n",
    "        print(f\"üöÄ D√©but du traitement de {len(zip_files)} fichiers\")\n",
    "        print(f\"üéØ Destination: {output_path}\\n\")\n",
    "        \n",
    "        # Connexion DuckDB\n",
    "        con = self.setup_duckdb_connection()\n",
    "        \n",
    "        try:\n",
    "            for zip_file in zip_files:\n",
    "                self.process_file(zip_file, con, output_path)\n",
    "        finally:\n",
    "            con.close()\n",
    "        \n",
    "        # R√©sum√©\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"‚úÖ Fichiers trait√©s: {self.processed_files}\")\n",
    "        print(f\"‚ùå Erreurs: {len(self.errors)}\")\n",
    "        if self.errors:\n",
    "            for error in self.errors[:5]:  # Premi√®re 5 erreurs seulement\n",
    "                print(f\"   ‚Ä¢ {error['file']}: {error['error']}\")\n",
    "\n",
    "# Configuration et ex√©cution\n",
    "config = {\n",
    "    \"provider\": \"binance\",\n",
    "    \"data_type\": \"data\", \n",
    "    \"market\": \"spot\",\n",
    "    \"data_frequency\": \"monthly\",\n",
    "    \"data_category\": \"klines\",\n",
    "    \"symbol\": \"BTCUSDT\",\n",
    "    \"interval\": \"4h\",\n",
    "    \"minio_access_key\": os.getenv(\"MINIO_ROOT_USER\", \"minioadm\"),\n",
    "    \"minio_secret_key\": os.getenv(\"MINIO_ROOT_PASSWORD\", \"minioadm\"),\n",
    "    \"minio_endpoint\": os.getenv(\"MINIO_ENDPOINT\", \"127.0.0.1:9000\"),\n",
    "}\n",
    "\n",
    "# D√©marrage\n",
    "processor = BinanceDataProcessorV2(config)\n",
    "processor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3bdecb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Workflow Complet pour Nouveaux Fichiers ZIP\n",
    "\n",
    "### **√âtape 1 : Pr√©paration**\n",
    "1. **T√©l√©chargez** vos nouveaux fichiers ZIP dans le r√©pertoire source :\n",
    "   ```\n",
    "   /media/giujorge/Stockage/DATA/raw/binance/spot/monthly/klines/BTCUSDT/4h/\n",
    "   ```\n",
    "\n",
    "2. **V√©rifiez** que MinIO est d√©marr√© :\n",
    "   ```bash\n",
    "   sudo systemctl status minio\n",
    "   ```\n",
    "\n",
    "### **√âtape 2 : Traitement (Choisissez une option)**\n",
    "\n",
    "#### ü§ñ **Option A : Automatique (Recommand√©)**\n",
    "```python\n",
    "# Traite automatiquement tous les nouveaux fichiers\n",
    "incremental_processor.process_new_files_only()\n",
    "```\n",
    "\n",
    "#### üéØ **Option B : Sp√©cifique**\n",
    "```python\n",
    "# Traite uniquement les fichiers que vous sp√©cifiez\n",
    "nouveaux_fichiers = [\"BTCUSDT-4h-2025-09.zip\", \"BTCUSDT-4h-2025-10.zip\"]\n",
    "incremental_processor.process_new_files_only(nouveaux_fichiers)\n",
    "```\n",
    "\n",
    "#### üß™ **Option C : Simulation d'abord**\n",
    "```python\n",
    "# Voir quels fichiers seraient trait√©s (sans les traiter)\n",
    "simulate_incremental_processing()\n",
    "```\n",
    "\n",
    "### **√âtape 3 : V√©rification**\n",
    "\n",
    "Apr√®s traitement, vous pouvez v√©rifier que les donn√©es sont bien ajout√©es :\n",
    "\n",
    "```python\n",
    "# V√©rifier avec DuckDB\n",
    "con = duckdb.connect()\n",
    "con.execute(\"SET s3_endpoint='127.0.0.1:9000'\")  # Config MinIO\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        MIN(datetime) as earliest_date,\n",
    "        MAX(datetime) as latest_date\n",
    "    FROM read_parquet('s3://bronze/binance/data/spot/monthly/klines/BTCUSDT/4h/**/*.parquet')\n",
    "\"\"\").fetchone()\n",
    "print(f\"üìä Total: {result[0]} enregistrements | {result[1]} ‚Üí {result[2]}\")\n",
    "```\n",
    "\n",
    "### **üîÑ Processus R√©current**\n",
    "\n",
    "Pour automatiser l'ajout de nouveaux fichiers, vous pouvez :\n",
    "\n",
    "1. **Script automatis√©** : Cr√©er un script Python qui ex√©cute `process_new_files_only()`\n",
    "2. **CRON job** : Programmer l'ex√©cution automatique (quotidienne/hebdomadaire)\n",
    "3. **Workflow CI/CD** : Int√©grer dans votre pipeline de donn√©es\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Votre data lakehouse est maintenant pr√™t pour l'ajout incr√©mental de nouvelles donn√©es !**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ú® **Avantages du Processeur Incr√©mental**\n",
    "\n",
    "- **üöÄ Rapidit√©** : Ne traite que les nouveaux fichiers\n",
    "- **üõ°Ô∏è S√©curit√©** : √âvite de retraiter les donn√©es existantes  \n",
    "- **üìä Tra√ßabilit√©** : Chaque fichier est identifi√© dans les m√©tadonn√©es\n",
    "- **üîÑ Idempotent** : Peut √™tre relanc√© sans risque\n",
    "- **‚ö° Efficace** : Utilise les partitions existantes\n",
    "\n",
    "---\n",
    "\n",
    "### üóìÔ∏è **Cas d'Usage Typiques**\n",
    "\n",
    "1. **Donn√©es mensuelles** : Ajout automatique du fichier du mois en cours\n",
    "2. **Backfill historique** : Ajout de donn√©es manquantes sp√©cifiques\n",
    "3. **Mise √† jour r√©guli√®re** : Traitement automatique dans un CRON job\n",
    "4. **Tests** : Traitement de fichiers de test sans impacter la production\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c16f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ PROCESSEUR INCR√âMENTAL PR√äT\n",
      "Utilisez les m√©thodes suivantes selon vos besoins:\n",
      "1. incremental_processor.process_new_files_only()  # Auto-d√©tection\n",
      "2. incremental_processor.process_new_files_only(['nouveau_fichier.zip'])  # Fichiers sp√©cifiques\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Guide pour ajouter de nouveaux fichiers ZIP au Data Lakehouse\n",
    "\"\"\"\n",
    "\n",
    "# 1. M√âTHODE RECOMMAND√âE : Traitement incr√©mental\n",
    "# Utilisez cette approche pour traiter uniquement les nouveaux fichiers\n",
    "\n",
    "import zipfile\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import duckdb\n",
    "import polars as pl\n",
    "\n",
    "class IncrementalProcessor:\n",
    "    \"\"\"Processeur incr√©mental pour nouveaux fichiers uniquement\"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        self.csv_columns = [\n",
    "            \"open_time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"close_time\",\n",
    "            \"quote_asset_volume\", \"number_of_trades\", \"taker_buy_base_volume\",\n",
    "            \"taker_buy_quote_volume\", \"ignore\"\n",
    "        ]\n",
    "    \n",
    "    def setup_duckdb_connection(self):\n",
    "        \"\"\"Configure DuckDB pour S3/MinIO\"\"\"\n",
    "        con = duckdb.connect(database=\":memory:\")\n",
    "        con.execute(f\"\"\"\n",
    "            SET s3_access_key_id='{self.config[\"minio_access_key\"]}';\n",
    "            SET s3_secret_access_key='{self.config[\"minio_secret_key\"]}';\n",
    "            SET s3_endpoint='{self.config[\"minio_endpoint\"]}';\n",
    "            SET s3_url_style='path';\n",
    "            SET s3_use_ssl='false';\n",
    "        \"\"\")\n",
    "        return con\n",
    "    \n",
    "    def get_processed_files_list(self, con):\n",
    "        \"\"\"R√©cup√®re la liste des fichiers d√©j√† trait√©s depuis les m√©tadonn√©es\"\"\"\n",
    "        output_path = (\n",
    "            f\"s3://bronze/{self.config['provider']}/{self.config['data_type']}/\"\n",
    "            f\"{self.config['market']}/{self.config['data_frequency']}/{self.config['data_category']}/\"\n",
    "            f\"{self.config['symbol']}/{self.config['interval']}/**/*.parquet\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Lire les m√©tadonn√©es pour identifier les fichiers sources d√©j√† trait√©s\n",
    "            # Note: Cette approche n√©cessite que ingest_id contienne l'info du fichier source\n",
    "            result = con.execute(f\"\"\"\n",
    "                SELECT DISTINCT \n",
    "                    split_part(ingest_id, '_', -1) as source_file\n",
    "                FROM read_parquet('{output_path}')\n",
    "                WHERE ingest_id IS NOT NULL\n",
    "            \"\"\").fetchall()\n",
    "            return [row[0] for row in result] if result else []\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Impossible de lire les m√©tadonn√©es existantes: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def process_new_files_only(self, specific_files=None):\n",
    "        \"\"\"Traite uniquement les nouveaux fichiers\"\"\"\n",
    "        # Chemins\n",
    "        base_path = Path(\n",
    "            f\"/media/giujorge/Stockage/DATA/raw/{self.config['provider']}/\"\n",
    "            f\"{self.config['market']}/{self.config['data_frequency']}/\"\n",
    "            f\"{self.config['data_category']}/{self.config['symbol']}/{self.config['interval']}\"\n",
    "        )\n",
    "        \n",
    "        output_path = (\n",
    "            f\"s3://bronze/{self.config['provider']}/{self.config['data_type']}/\"\n",
    "            f\"{self.config['market']}/{self.config['data_frequency']}/{self.config['data_category']}/\"\n",
    "            f\"{self.config['symbol']}/{self.config['interval']}/\"\n",
    "        )\n",
    "        \n",
    "        # Connexion DuckDB\n",
    "        con = self.setup_duckdb_connection()\n",
    "        \n",
    "        try:\n",
    "            # Option 1: Traiter des fichiers sp√©cifiques\n",
    "            if specific_files:\n",
    "                files_to_process = [base_path / f for f in specific_files if (base_path / f).exists()]\n",
    "                print(f\"üéØ Traitement de fichiers sp√©cifiques: {specific_files}\")\n",
    "            else:\n",
    "                # Option 2: Traitement automatique des nouveaux fichiers\n",
    "                all_files = list(base_path.glob(\"*.zip\"))\n",
    "                processed_files = self.get_processed_files_list(con)\n",
    "                \n",
    "                files_to_process = [\n",
    "                    f for f in all_files \n",
    "                    if f.name not in processed_files\n",
    "                ]\n",
    "                print(f\"üìÅ {len(all_files)} fichiers totaux, {len(processed_files)} d√©j√† trait√©s\")\n",
    "                print(f\"üÜï {len(files_to_process)} nouveaux fichiers √† traiter\")\n",
    "            \n",
    "            if not files_to_process:\n",
    "                print(\"‚úÖ Aucun nouveau fichier √† traiter!\")\n",
    "                return\n",
    "            \n",
    "            # Traitement\n",
    "            processed_count = 0\n",
    "            for zip_file in files_to_process:\n",
    "                print(f\"\\nüìÅ Traitement: {zip_file.name}\")\n",
    "                success = self.process_single_file(zip_file, con, output_path)\n",
    "                if success:\n",
    "                    processed_count += 1\n",
    "            \n",
    "            print(f\"\\nüéâ Traitement termin√©: {processed_count}/{len(files_to_process)} fichiers\")\n",
    "            \n",
    "        finally:\n",
    "            con.close()\n",
    "    \n",
    "    def process_single_file(self, zip_path: Path, con, output_path: str) -> bool:\n",
    "        \"\"\"Traite un seul fichier ZIP\"\"\"\n",
    "        try:\n",
    "            # 1. Lecture\n",
    "            with zipfile.ZipFile(str(zip_path)) as z:\n",
    "                csv_name = [n for n in z.namelist() if n.lower().endswith('.csv')][0]\n",
    "                with z.open(csv_name) as f:\n",
    "                    df = pl.read_csv(f, has_header=False, new_columns=self.csv_columns)\n",
    "            \n",
    "            # 2. Transformation\n",
    "            df = df.with_columns([\n",
    "                pl.col(\"open_time\").cast(pl.Int64, strict=False),\n",
    "                pl.col(\"close_time\").cast(pl.Int64, strict=False),\n",
    "            ])\n",
    "            \n",
    "            # 3. Timestamp conversion\n",
    "            first_ts = df.select(pl.col(\"open_time\").first()).item()\n",
    "            if first_ts > 10_000_000_000_000:\n",
    "                ts_col = (pl.col(\"open_time\") // 1000).cast(pl.Datetime(\"ms\"))\n",
    "            elif first_ts > 10_000_000_000:\n",
    "                ts_col = pl.col(\"open_time\").cast(pl.Datetime(\"ms\"))\n",
    "            else:\n",
    "                ts_col = (pl.col(\"open_time\") * 1000).cast(pl.Datetime(\"ms\"))\n",
    "            \n",
    "            # 4. M√©tadonn√©es avec r√©f√©rence au fichier source\n",
    "            ingest_id = f\"{datetime.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}_{zip_path.name}\"\n",
    "            \n",
    "            df = df.with_columns([\n",
    "                ts_col.alias(\"datetime\"),\n",
    "                pl.lit(ingest_id).alias(\"ingest_id\"),\n",
    "            ]).with_columns([\n",
    "                pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
    "            ])\n",
    "            \n",
    "            # 5. Sauvegarde\n",
    "            try:\n",
    "                con.execute(\"DROP VIEW IF EXISTS tmp_new_data\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            con.register(\"tmp_new_data\", df.to_arrow())\n",
    "            \n",
    "            sql = f\"\"\"\n",
    "                COPY tmp_new_data\n",
    "                TO '{output_path}'\n",
    "                WITH (FORMAT PARQUET, PARTITION_BY (year, month, day), OVERWRITE_OR_IGNORE TRUE)\n",
    "            \"\"\"\n",
    "            con.execute(sql)\n",
    "            \n",
    "            min_max = df.select([\n",
    "                pl.col(\"datetime\").min().alias(\"min_dt\"),\n",
    "                pl.col(\"datetime\").max().alias(\"max_dt\"),\n",
    "            ]).to_dicts()[0]\n",
    "            \n",
    "            print(f\"   ‚úÖ {df.height} lignes | {min_max['min_dt']} ‚Üí {min_max['max_dt']}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erreur: {str(e)}\")\n",
    "            return False\n",
    "        finally:\n",
    "            try:\n",
    "                con.execute(\"DROP VIEW IF EXISTS tmp_new_data\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Configuration (m√™me que pr√©c√©demment)\n",
    "config = {\n",
    "    \"provider\": \"binance\",\n",
    "    \"data_type\": \"data\",\n",
    "    \"market\": \"spot\", \n",
    "    \"data_frequency\": \"monthly\",\n",
    "    \"data_category\": \"klines\",\n",
    "    \"symbol\": \"BTCUSDT\",\n",
    "    \"interval\": \"4h\",\n",
    "    \"minio_access_key\": os.getenv(\"MINIO_ROOT_USER\", \"minioadm\"),\n",
    "    \"minio_secret_key\": os.getenv(\"MINIO_ROOT_PASSWORD\", \"minioadm\"),\n",
    "    \"minio_endpoint\": os.getenv(\"MINIO_ENDPOINT\", \"127.0.0.1:9000\"),\n",
    "}\n",
    "\n",
    "# Instanciation du processeur incr√©mental\n",
    "incremental_processor = IncrementalProcessor(config)\n",
    "\n",
    "print(\"üîÑ PROCESSEUR INCR√âMENTAL PR√äT\")\n",
    "print(\"Utilisez les m√©thodes suivantes selon vos besoins:\")\n",
    "print(\"1. incremental_processor.process_new_files_only()  # Auto-d√©tection\")\n",
    "print(\"2. incremental_processor.process_new_files_only(['nouveau_fichier.zip'])  # Fichiers sp√©cifiques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb5864",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Processus pour Ajouter de Nouveaux Fichiers ZIP\n",
    "\n",
    "Maintenant que votre data lakehouse est configur√©, voici **3 approches** pour ajouter de nouveaux fichiers :\n",
    "\n",
    "### üéØ **Option 1 : Traitement Automatique (Recommand√©)**\n",
    "```python\n",
    "# D√©tecte automatiquement les nouveaux fichiers et les traite\n",
    "incremental_processor.process_new_files_only()\n",
    "```\n",
    "\n",
    "### üìÇ **Option 2 : Fichiers Sp√©cifiques**\n",
    "```python\n",
    "# Traite uniquement les fichiers que vous sp√©cifiez\n",
    "nouveaux_fichiers = [\"BTCUSDT-4h-2025-09.zip\", \"BTCUSDT-4h-2025-10.zip\"]\n",
    "incremental_processor.process_new_files_only(nouveaux_fichiers)\n",
    "```\n",
    "\n",
    "### üîß **Option 3 : Workflow Manuel**\n",
    "1. **T√©l√©chargez** vos nouveaux fichiers ZIP dans le r√©pertoire source\n",
    "2. **Ex√©cutez** une des commandes ci-dessus\n",
    "3. **V√©rifiez** que les donn√©es sont bien ajout√©es dans MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7673ee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Fichiers trouv√©s dans le r√©pertoire source:\n",
      "    1. BTCUSDT-4h-2017-08.zip\n",
      "    2. BTCUSDT-4h-2017-09.zip\n",
      "    3. BTCUSDT-4h-2017-10.zip\n",
      "    4. BTCUSDT-4h-2017-11.zip\n",
      "    5. BTCUSDT-4h-2017-12.zip\n",
      "    6. BTCUSDT-4h-2018-01.zip\n",
      "    7. BTCUSDT-4h-2018-02.zip\n",
      "    8. BTCUSDT-4h-2018-03.zip\n",
      "    9. BTCUSDT-4h-2018-04.zip\n",
      "   10. BTCUSDT-4h-2018-05.zip\n",
      "   ... et 87 autres fichiers\n",
      "\n",
      "üîç Pour identifier les nouveaux fichiers, le processeur va:\n",
      "   1. Lire les m√©tadonn√©es depuis MinIO\n",
      "   2. Comparer avec les fichiers locaux\n",
      "   3. Traiter uniquement les diff√©rences\n",
      "\n",
      "üí° Pr√™t √† traiter jusqu'√† 97 fichiers si n√©cessaire!\n"
     ]
    }
   ],
   "source": [
    "# üß™ EXEMPLE DE TEST - Traitement incr√©mental\n",
    "# D√©commentez et ex√©cutez pour tester\n",
    "\n",
    "# Exemple 1: Traitement automatique de tous les nouveaux fichiers\n",
    "# incremental_processor.process_new_files_only()\n",
    "\n",
    "# Exemple 2: Traitement de fichiers sp√©cifiques (remplacez par vos vrais fichiers)\n",
    "# nouveaux_fichiers = [\"BTCUSDT-4h-2025-09.zip\"]  # Vos nouveaux fichiers\n",
    "# incremental_processor.process_new_files_only(nouveaux_fichiers)\n",
    "\n",
    "# Exemple 3: Simulation - v√©rifier quels fichiers seraient trait√©s\n",
    "def simulate_incremental_processing():\n",
    "    \"\"\"Simule le traitement pour voir quels fichiers seraient trait√©s\"\"\"\n",
    "    base_path = Path(\n",
    "        f\"/media/giujorge/Stockage/DATA/raw/{config['provider']}/\"\n",
    "        f\"{config['market']}/{config['data_frequency']}/\"\n",
    "        f\"{config['data_category']}/{config['symbol']}/{config['interval']}\"\n",
    "    )\n",
    "    \n",
    "    all_files = list(base_path.glob(\"*.zip\"))\n",
    "    print(f\"üìÅ Fichiers trouv√©s dans le r√©pertoire source:\")\n",
    "    for i, f in enumerate(sorted(all_files)[:10], 1):  # Afficher les 10 premiers\n",
    "        print(f\"   {i:2d}. {f.name}\")\n",
    "    \n",
    "    if len(all_files) > 10:\n",
    "        print(f\"   ... et {len(all_files) - 10} autres fichiers\")\n",
    "    \n",
    "    print(f\"\\nüîç Pour identifier les nouveaux fichiers, le processeur va:\")\n",
    "    print(f\"   1. Lire les m√©tadonn√©es depuis MinIO\")\n",
    "    print(f\"   2. Comparer avec les fichiers locaux\")\n",
    "    print(f\"   3. Traiter uniquement les diff√©rences\")\n",
    "    \n",
    "    return len(all_files)\n",
    "\n",
    "# Lancez cette simulation pour voir l'√©tat actuel\n",
    "total_files = simulate_incremental_processing()\n",
    "print(f\"\\nüí° Pr√™t √† traiter jusqu'√† {total_files} fichiers si n√©cessaire!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6961107",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hermes-X3pwqMMo-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
