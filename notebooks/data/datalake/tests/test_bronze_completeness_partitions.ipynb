{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e779ae",
   "metadata": {},
   "source": [
    "# ✅ Tests Bronze : Validation ZIP → Parquet\n",
    "\n",
    "**Objectif** : Vérifier que l'ingestion Bronze reproduit fidèlement les données sources ZIP\n",
    "\n",
    "Ce notebook applique **les mêmes transformations que datalake_bronze** avant de comparer, garantissant des tests pertinents.\n",
    "\n",
    "## 🎯 Tests implémentés\n",
    "\n",
    "1. **Setup & Helpers** : Configuration et fonction de transformation (réplique datalake_bronze)\n",
    "2. **Test 1 : Exhaustivité globale** - Nombre total de lignes ZIP vs Bronze\n",
    "3. **Test 2 : Comparaison par fichier** - Détail fichier par fichier avec datetime\n",
    "4. **Test 3 : Unicité** - Aucun doublon sur datetime\n",
    "5. **Test 4 : Partitionnement** - year/month/day cohérents avec datetime\n",
    "6. **Test 5 : Traçabilité** - Métadonnée ingest_id présente et valide\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a80aff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3_PATTERN = s3://bronze/binance/data/spot/monthly/klines/BTCUSDT/4h/**/*.parquet\n",
      "RAW_DIR    = /media/giujorge/Stockage/DATA/raw/binance/spot/monthly/klines/BTCUSDT/4h\n",
      "DuckDB connecté à MinIO\n"
     ]
    }
   ],
   "source": [
    "# Imports & paramètres\n",
    "import os, zipfile\n",
    "from pathlib import Path\n",
    "import duckdb\n",
    "import polars as pl\n",
    "from typing import Tuple\n",
    "\n",
    "# Paramètres (adapter si besoin)\n",
    "PROVIDER = os.getenv('PROVIDER','binance')\n",
    "MARKET = os.getenv('MARKET','spot')\n",
    "FREQ = os.getenv('FREQ','monthly')\n",
    "CATEGORY = os.getenv('CATEGORY','klines')\n",
    "SYMBOL = os.getenv('SYMBOL','BTCUSDT')\n",
    "INTERVAL = os.getenv('INTERVAL','4h')\n",
    "\n",
    "MINIO_ENDPOINT = os.getenv('MINIO_ENDPOINT','127.0.0.1:9000')\n",
    "MINIO_ACCESS = os.getenv('MINIO_ROOT_USER','minioadm')\n",
    "MINIO_SECRET = os.getenv('MINIO_ROOT_PASSWORD','minioadm')\n",
    "\n",
    "# Chemins\n",
    "S3_PATTERN = f\"s3://bronze/{PROVIDER}/data/{MARKET}/{FREQ}/{CATEGORY}/{SYMBOL}/{INTERVAL}/**/*.parquet\"\n",
    "RAW_DIR = Path(os.getenv('RAW_DIR', f\"/media/giujorge/Stockage/DATA/raw/{PROVIDER}/{MARKET}/{FREQ}/{CATEGORY}/{SYMBOL}/{INTERVAL}\"))\n",
    "print('S3_PATTERN =', S3_PATTERN)\n",
    "print('RAW_DIR    =', RAW_DIR)\n",
    "\n",
    "# Connexion DuckDB configurée pour MinIO\n",
    "con = duckdb.connect(database=':memory:')\n",
    "con.execute(f\"\"\"\n",
    "    SET s3_access_key_id='{MINIO_ACCESS}';\n",
    "    SET s3_secret_access_key='{MINIO_SECRET}';\n",
    "    SET s3_endpoint='{MINIO_ENDPOINT}';\n",
    "    SET s3_url_style='path';\n",
    "    SET s3_use_ssl='false';\n",
    "\"\"\")\n",
    "print('DuckDB connecté à MinIO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4efde8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 97 fichiers ZIP trouvés\n",
      "📂 Exemple: BTCUSDT-4h-2017-08.zip\n"
     ]
    }
   ],
   "source": [
    "# ========== HELPER : Transformation ZIP → DataFrame ==========\n",
    "# Reproduit exactement la logique de datalake_bronze pour une comparaison pertinente\n",
    "\n",
    "def transform_zip_to_df(zip_path: Path) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Applique les mêmes transformations que datalake_bronze:\n",
    "    1. Lecture CSV avec colonnes nommées\n",
    "    2. Cast timestamp en Int64\n",
    "    3. Détection automatique format (µs/ms/s)\n",
    "    4. Conversion en Datetime(\"ms\")\n",
    "    \n",
    "    Returns: DataFrame Polars avec colonne 'datetime' prête pour comparaison\n",
    "    \"\"\"\n",
    "    csv_columns = [\n",
    "        \"open_time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"close_time\",\n",
    "        \"quote_asset_volume\", \"number_of_trades\", \"taker_buy_base_volume\",\n",
    "        \"taker_buy_quote_volume\", \"ignore\"\n",
    "    ]\n",
    "    \n",
    "    with zipfile.ZipFile(str(zip_path)) as z:\n",
    "        csv_names = [n for n in z.namelist() if n.lower().endswith('.csv')]\n",
    "        if not csv_names:\n",
    "            return pl.DataFrame()\n",
    "        \n",
    "        with z.open(csv_names[0]) as f:\n",
    "            df = pl.read_csv(f, has_header=False, new_columns=csv_columns)\n",
    "    \n",
    "    # Transformation 1: Cast\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"open_time\").cast(pl.Int64, strict=False)\n",
    "    ])\n",
    "    \n",
    "    # Transformation 2: Détection format + conversion\n",
    "    first_ts = df.select(pl.col(\"open_time\").first()).item()\n",
    "    \n",
    "    if first_ts > 10_000_000_000_000:  # Microsecondes\n",
    "        ts_col = (pl.col(\"open_time\") // 1000).cast(pl.Datetime(\"ms\"))\n",
    "    elif first_ts > 10_000_000_000:    # Millisecondes\n",
    "        ts_col = pl.col(\"open_time\").cast(pl.Datetime(\"ms\"))\n",
    "    else:                               # Secondes\n",
    "        ts_col = (pl.col(\"open_time\") * 1000).cast(pl.Datetime(\"ms\"))\n",
    "    \n",
    "    # Transformation 3: Ajout colonne datetime\n",
    "    df = df.with_columns([\n",
    "        ts_col.alias(\"datetime\")\n",
    "    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Index des fichiers sources\n",
    "zip_files = sorted(RAW_DIR.glob('*.zip'))\n",
    "print(f'✅ {len(zip_files)} fichiers ZIP trouvés')\n",
    "print(f'📂 Exemple: {zip_files[0].name if zip_files else \"Aucun\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea20d7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 TEST 1 : Exhaustivité Globale ZIP → Bronze\n",
      "\n",
      "======================================================================\n",
      "📊 Résultat:\n",
      "   • ZIP sources transformées  : 17,604 lignes\n",
      "   • Bronze (Parquet MinIO)    : 17,604 lignes\n",
      "   • Différence                : +0 lignes\n",
      "   • Écart relatif             : +0.00%\n",
      "======================================================================\n",
      "✅ SUCCÈS : Exhaustivité conforme (±1.0%)\n",
      "\n",
      "📊 Résultat:\n",
      "   • ZIP sources transformées  : 17,604 lignes\n",
      "   • Bronze (Parquet MinIO)    : 17,604 lignes\n",
      "   • Différence                : +0 lignes\n",
      "   • Écart relatif             : +0.00%\n",
      "======================================================================\n",
      "✅ SUCCÈS : Exhaustivité conforme (±1.0%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7855/2140697886.py:26: UserWarning: Polars found a filename. Ensure you pass a path to the file instead of a python file object when possible for best performance.\n",
      "  df = pl.read_csv(f, has_header=False, new_columns=csv_columns)\n"
     ]
    }
   ],
   "source": [
    "# ========== TEST 1 : Exhaustivité Globale ==========\n",
    "\n",
    "print(\"🔍 TEST 1 : Exhaustivité Globale ZIP → Bronze\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compte Bronze\n",
    "bronze_count = con.execute(f\"\"\"\n",
    "    SELECT COUNT(*) FROM read_parquet('{S3_PATTERN}')\n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "# Compte ZIP (avec transformations)\n",
    "zip_total = 0\n",
    "for zf in zip_files:\n",
    "    try:\n",
    "        df = transform_zip_to_df(zf)\n",
    "        zip_total += df.height\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur {zf.name}: {e}\")\n",
    "\n",
    "print(f\"📊 Résultat:\")\n",
    "print(f\"   • ZIP sources transformées  : {zip_total:,} lignes\")\n",
    "print(f\"   • Bronze (Parquet MinIO)    : {bronze_count:,} lignes\")\n",
    "print(f\"   • Différence                : {bronze_count - zip_total:+,} lignes\")\n",
    "\n",
    "diff_pct = ((bronze_count - zip_total) / zip_total * 100) if zip_total > 0 else 0\n",
    "print(f\"   • Écart relatif             : {diff_pct:+.2f}%\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Tolérance ±1% pour variations mineures\n",
    "tolerance = 0.01\n",
    "if abs(diff_pct) <= tolerance:\n",
    "    print(f\"✅ SUCCÈS : Exhaustivité conforme (±{tolerance*100}%)\")\n",
    "else:\n",
    "    print(f\"⚠️ ATTENTION : Écart de {abs(diff_pct):.2f}% détecté\")\n",
    "    if bronze_count < zip_total:\n",
    "        print(f\"   → {zip_total - bronze_count:,} lignes manquantes en Bronze\")\n",
    "    else:\n",
    "        print(f\"   → {bronze_count - zip_total:,} lignes en trop en Bronze\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fba50532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 TEST 2 : Comparaison Détaillée par Fichier\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7855/2140697886.py:26: UserWarning: Polars found a filename. Ensure you pass a path to the file instead of a python file object when possible for best performance.\n",
      "  df = pl.read_csv(f, has_header=False, new_columns=csv_columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status                fichier  zip_count  bronze_count  missing  extra\n",
      "    ⚠️ BTCUSDT-4h-2017-08.zip         89            89       89     89\n",
      "    ⚠️ BTCUSDT-4h-2017-09.zip        179           179      179    179\n",
      "    ⚠️ BTCUSDT-4h-2017-10.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2017-11.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2017-12.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2018-01.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2018-02.zip        161           161      161    161\n",
      "    ⚠️ BTCUSDT-4h-2018-03.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2018-04.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2018-05.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2018-06.zip        178           178      178    178\n",
      "    ⚠️ BTCUSDT-4h-2018-07.zip        185           185      185    185\n",
      "    ⚠️ BTCUSDT-4h-2018-08.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2018-09.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2018-10.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2018-11.zip        179           179      179    179\n",
      "    ⚠️ BTCUSDT-4h-2018-12.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2019-01.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2019-02.zip        168           168      168    168\n",
      "    ⚠️ BTCUSDT-4h-2019-03.zip        185           185      185    185\n",
      "    ⚠️ BTCUSDT-4h-2019-04.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2019-05.zip        184           184      184    184\n",
      "    ⚠️ BTCUSDT-4h-2019-06.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2019-07.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2019-08.zip        185           185      185    185\n",
      "    ⚠️ BTCUSDT-4h-2019-09.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2019-10.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2019-11.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2019-12.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2020-01.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2020-02.zip        173           173      173    173\n",
      "    ⚠️ BTCUSDT-4h-2020-03.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2020-04.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2020-05.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2020-06.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2020-07.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2020-08.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2020-09.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2020-10.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2020-11.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2020-12.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2021-01.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2021-02.zip        168           168      168    168\n",
      "    ⚠️ BTCUSDT-4h-2021-03.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2021-04.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2021-05.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2021-06.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2021-07.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2021-08.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2021-09.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2021-10.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2021-11.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2021-12.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2022-01.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2022-02.zip        168           168      168    168\n",
      "    ⚠️ BTCUSDT-4h-2022-03.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2022-04.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2022-05.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2022-06.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2022-07.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2022-08.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2022-09.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2022-10.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2022-11.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2022-12.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2023-01.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2023-02.zip        168           168      168    168\n",
      "    ⚠️ BTCUSDT-4h-2023-03.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2023-04.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2023-05.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2023-06.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2023-07.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2023-08.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2023-09.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2023-10.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2023-11.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2023-12.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2024-01.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2024-02.zip        174           174      174    174\n",
      "    ⚠️ BTCUSDT-4h-2024-03.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2024-04.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2024-05.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2024-06.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2024-07.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2024-08.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2024-09.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2024-10.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2024-11.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2024-12.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2025-01.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2025-02.zip        168           168      168    168\n",
      "    ⚠️ BTCUSDT-4h-2025-03.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2025-04.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2025-05.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2025-06.zip        180           180      180    180\n",
      "    ⚠️ BTCUSDT-4h-2025-07.zip        186           186      186    186\n",
      "    ⚠️ BTCUSDT-4h-2025-08.zip        186           186      186    186\n",
      "\n",
      "======================================================================\n",
      "✅ Fichiers OK : 0/97\n",
      "⚠️ Différences  : 97\n",
      "\n",
      "📋 ANALYSE DÉTAILLÉE : BTCUSDT-4h-2025-01.zip\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7855/2140697886.py:26: UserWarning: Polars found a filename. Ensure you pass a path to the file instead of a python file object when possible for best performance.\n",
      "  df = pl.read_csv(f, has_header=False, new_columns=csv_columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ Premières datetime manquantes (10 affichées):\n",
      "   2025-01-01T00:00:00\n",
      "   2025-01-01T04:00:00\n",
      "   2025-01-01T08:00:00\n",
      "   2025-01-01T12:00:00\n",
      "   2025-01-01T16:00:00\n",
      "   2025-01-01T20:00:00\n",
      "   2025-01-02T00:00:00\n",
      "   2025-01-02T04:00:00\n",
      "   2025-01-02T08:00:00\n",
      "   2025-01-02T12:00:00\n",
      "\n",
      "⚠️ Premières datetime en trop (10 affichées):\n",
      "   2025-01-01 00:00:00\n",
      "   2025-01-01 04:00:00\n",
      "   2025-01-01 08:00:00\n",
      "   2025-01-01 12:00:00\n",
      "   2025-01-01 16:00:00\n",
      "   2025-01-01 20:00:00\n",
      "   2025-01-02 00:00:00\n",
      "   2025-01-02 04:00:00\n",
      "   2025-01-02 08:00:00\n",
      "   2025-01-02 12:00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== TEST 2 : Comparaison Fichier par Fichier ==========\n",
    "\n",
    "print(\"🔍 TEST 2 : Comparaison Détaillée par Fichier\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Variable pour choisir le fichier à analyser en détail (None = tous)\n",
    "DETAILED_FILE = \"BTCUSDT-4h-2025-01.zip\"  # Changez pour analyser un autre fichier\n",
    "\n",
    "results = []\n",
    "\n",
    "for zf in zip_files:\n",
    "    try:\n",
    "        # Transformer le ZIP\n",
    "        zip_df = transform_zip_to_df(zf)\n",
    "        zip_datetimes = set(zip_df.select(\n",
    "            pl.col(\"datetime\").dt.to_string(\"%Y-%m-%dT%H:%M:%S%.f\")\n",
    "        ).to_series().to_list())\n",
    "        \n",
    "        # Extraire plage temporelle\n",
    "        min_dt = zip_df.select(pl.col(\"datetime\").min()).item()\n",
    "        max_dt = zip_df.select(pl.col(\"datetime\").max()).item()\n",
    "        \n",
    "        # Requête Bronze pour cette plage\n",
    "        bronze_df = con.execute(f\"\"\"\n",
    "            SELECT datetime::VARCHAR as dt\n",
    "            FROM read_parquet('{S3_PATTERN}')\n",
    "            WHERE datetime >= '{min_dt}' AND datetime <= '{max_dt}'\n",
    "        \"\"\").fetchdf()\n",
    "        \n",
    "        bronze_datetimes = set(bronze_df['dt'].tolist())\n",
    "        \n",
    "        # Calcul différences\n",
    "        missing = len(zip_datetimes - bronze_datetimes)\n",
    "        extra = len(bronze_datetimes - zip_datetimes)\n",
    "        common = len(zip_datetimes & bronze_datetimes)\n",
    "        \n",
    "        results.append({\n",
    "            'fichier': zf.name,\n",
    "            'zip_count': len(zip_datetimes),\n",
    "            'bronze_count': len(bronze_datetimes),\n",
    "            'missing': missing,\n",
    "            'extra': extra,\n",
    "            'common': common,\n",
    "            'status': '✅' if missing == 0 and extra == 0 else '⚠️'\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            'fichier': zf.name,\n",
    "            'zip_count': 0,\n",
    "            'bronze_count': 0,\n",
    "            'missing': 0,\n",
    "            'extra': 0,\n",
    "            'common': 0,\n",
    "            'status': f'❌ {str(e)[:30]}'\n",
    "        })\n",
    "\n",
    "# Affichage résumé\n",
    "import pandas as pd\n",
    "summary_df = pd.DataFrame(results)\n",
    "print(summary_df[['status', 'fichier', 'zip_count', 'bronze_count', 'missing', 'extra']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"✅ Fichiers OK : {len([r for r in results if r['status'] == '✅'])}/{len(results)}\")\n",
    "print(f\"⚠️ Différences  : {len([r for r in results if r['status'] == '⚠️'])}\")\n",
    "\n",
    "# Analyse détaillée si un fichier spécifique est sélectionné\n",
    "if DETAILED_FILE and any(r['fichier'] == DETAILED_FILE for r in results):\n",
    "    print(f\"\\n📋 ANALYSE DÉTAILLÉE : {DETAILED_FILE}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    target_result = [r for r in results if r['fichier'] == DETAILED_FILE][0]\n",
    "    \n",
    "    if target_result['missing'] > 0 or target_result['extra'] > 0:\n",
    "        # Recharger pour affichage détaillé\n",
    "        target_path = RAW_DIR / DETAILED_FILE\n",
    "        zip_df = transform_zip_to_df(target_path)\n",
    "        zip_dt_set = set(zip_df.select(\n",
    "            pl.col(\"datetime\").dt.to_string(\"%Y-%m-%dT%H:%M:%S%.f\")\n",
    "        ).to_series().to_list())\n",
    "        \n",
    "        min_dt = zip_df.select(pl.col(\"datetime\").min()).item()\n",
    "        max_dt = zip_df.select(pl.col(\"datetime\").max()).item()\n",
    "        \n",
    "        bronze_df = con.execute(f\"\"\"\n",
    "            SELECT datetime::VARCHAR as dt FROM read_parquet('{S3_PATTERN}')\n",
    "            WHERE datetime >= '{min_dt}' AND datetime <= '{max_dt}'\n",
    "        \"\"\").fetchdf()\n",
    "        bronze_dt_set = set(bronze_df['dt'].tolist())\n",
    "        \n",
    "        missing_dt = sorted(list(zip_dt_set - bronze_dt_set))[:10]\n",
    "        extra_dt = sorted(list(bronze_dt_set - zip_dt_set))[:10]\n",
    "        \n",
    "        if missing_dt:\n",
    "            print(f\"\\n❌ Premières datetime manquantes ({len(missing_dt)} affichées):\")\n",
    "            for dt in missing_dt:\n",
    "                print(f\"   {dt}\")\n",
    "        \n",
    "        if extra_dt:\n",
    "            print(f\"\\n⚠️ Premières datetime en trop ({len(extra_dt)} affichées):\")\n",
    "            for dt in extra_dt:\n",
    "                print(f\"   {dt}\")\n",
    "    else:\n",
    "        print(\"✅ Aucune différence - Fichier parfaitement ingéré\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32b2223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 TEST 3 : Unicité des Datetime\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed513308e23a4415b7b5958da87c9f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lignes       : 17,604\n",
      "Datetime uniques   : 17,604\n",
      "Duplications       : 0\n",
      "\n",
      "✅ SUCCÈS : Aucune duplication de datetime\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== TEST 3 : Unicité et Idempotence ==========\n",
    "\n",
    "print(\"🔍 TEST 3 : Unicité des Datetime\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Vérifier qu'il n'y a pas de duplications de datetime\n",
    "duplicate_check = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_rows,\n",
    "        COUNT(DISTINCT datetime) as unique_datetimes,\n",
    "        COUNT(*) - COUNT(DISTINCT datetime) as duplicates\n",
    "    FROM read_parquet('{S3_PATTERN}')\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "total = duplicate_check['total_rows'][0]\n",
    "unique = duplicate_check['unique_datetimes'][0]\n",
    "dupes = duplicate_check['duplicates'][0]\n",
    "\n",
    "print(f\"Total lignes       : {total:,}\")\n",
    "print(f\"Datetime uniques   : {unique:,}\")\n",
    "print(f\"Duplications       : {dupes:,}\")\n",
    "\n",
    "if dupes == 0:\n",
    "    print(\"\\n✅ SUCCÈS : Aucune duplication de datetime\")\n",
    "else:\n",
    "    print(f\"\\n❌ ÉCHEC : {dupes:,} duplications détectées !\")\n",
    "    \n",
    "    # Afficher exemples de doublons\n",
    "    duplicates_detail = con.execute(f\"\"\"\n",
    "        SELECT datetime, COUNT(*) as count\n",
    "        FROM read_parquet('{S3_PATTERN}')\n",
    "        GROUP BY datetime\n",
    "        HAVING COUNT(*) > 1\n",
    "        ORDER BY count DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").fetchdf()\n",
    "    \n",
    "    print(\"\\n📋 Premiers doublons :\")\n",
    "    print(duplicates_detail.to_string(index=False))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "537a736f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 TEST 4 : Cohérence year/month/day avec datetime\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec23ebb228d4292a9c7b5c415e93d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lignes       : 17,604\n",
      "Erreurs year       : 0.0\n",
      "Erreurs month      : 0.0\n",
      "Erreurs day        : 0.0\n",
      "\n",
      "✅ SUCCÈS : Toutes les partitions sont cohérentes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== TEST 4 : Cohérence des Partitions ==========\n",
    "\n",
    "print(\"🔍 TEST 4 : Cohérence year/month/day avec datetime\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Vérifier que les partitions correspondent à la datetime\n",
    "partitions_check = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_rows,\n",
    "        SUM(CASE WHEN year != CAST(EXTRACT(YEAR FROM datetime) AS INT) THEN 1 ELSE 0 END) as year_errors,\n",
    "        SUM(CASE WHEN month != CAST(EXTRACT(MONTH FROM datetime) AS INT) THEN 1 ELSE 0 END) as month_errors,\n",
    "        SUM(CASE WHEN day != CAST(EXTRACT(DAY FROM datetime) AS INT) THEN 1 ELSE 0 END) as day_errors\n",
    "    FROM read_parquet('{S3_PATTERN}')\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "total = partitions_check['total_rows'][0]\n",
    "year_err = partitions_check['year_errors'][0]\n",
    "month_err = partitions_check['month_errors'][0]\n",
    "day_err = partitions_check['day_errors'][0]\n",
    "total_err = year_err + month_err + day_err\n",
    "\n",
    "print(f\"Total lignes       : {total:,}\")\n",
    "print(f\"Erreurs year       : {year_err}\")\n",
    "print(f\"Erreurs month      : {month_err}\")\n",
    "print(f\"Erreurs day        : {day_err}\")\n",
    "\n",
    "if total_err == 0:\n",
    "    print(\"\\n✅ SUCCÈS : Toutes les partitions sont cohérentes\")\n",
    "else:\n",
    "    print(f\"\\n❌ ÉCHEC : {total_err} incohérences détectées !\")\n",
    "    \n",
    "    # Afficher exemples\n",
    "    examples = con.execute(f\"\"\"\n",
    "        SELECT datetime, year, month, day,\n",
    "               EXTRACT(YEAR FROM datetime) as actual_year,\n",
    "               EXTRACT(MONTH FROM datetime) as actual_month,\n",
    "               EXTRACT(DAY FROM datetime) as actual_day\n",
    "        FROM read_parquet('{S3_PATTERN}')\n",
    "        WHERE year != CAST(EXTRACT(YEAR FROM datetime) AS INT)\n",
    "           OR month != CAST(EXTRACT(MONTH FROM datetime) AS INT)\n",
    "           OR day != CAST(EXTRACT(DAY FROM datetime) AS INT)\n",
    "        LIMIT 10\n",
    "    \"\"\").fetchdf()\n",
    "    \n",
    "    print(\"\\n📋 Exemples d'incohérences :\")\n",
    "    print(examples.to_string(index=False))\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11f04b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 TEST 5 : Présence et Format de ingest_id\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fab0473edcd4e33b7478ffd544ebcb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lignes       : 17,604\n",
      "ingest_id NULL     : 0.0\n",
      "ingest_id valide   : 17604.0\n",
      "\n",
      "✅ SUCCÈS : Tous les ingest_id sont présents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5937f5876ac4cb896d04014bcea14ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCÈS : Tous les ingest_id ont un format valide\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377e315457a24f77956da93bd7f94b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nombre de batchs d'ingestion distincts : 97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== TEST 5 : Traçabilité (ingest_id) ==========\n",
    "\n",
    "print(\"🔍 TEST 5 : Présence et Format de ingest_id\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Vérifier présence (NULL check)\n",
    "null_check = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_rows,\n",
    "        SUM(CASE WHEN ingest_id IS NULL THEN 1 ELSE 0 END) as null_count,\n",
    "        SUM(CASE WHEN ingest_id IS NOT NULL THEN 1 ELSE 0 END) as non_null_count\n",
    "    FROM read_parquet('{S3_PATTERN}')\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "total = null_check['total_rows'][0]\n",
    "nulls = null_check['null_count'][0]\n",
    "non_nulls = null_check['non_null_count'][0]\n",
    "\n",
    "print(f\"Total lignes       : {total:,}\")\n",
    "print(f\"ingest_id NULL     : {nulls}\")\n",
    "print(f\"ingest_id valide   : {non_nulls}\")\n",
    "\n",
    "if nulls > 0:\n",
    "    print(f\"\\n⚠️ ATTENTION : {nulls} lignes avec ingest_id NULL !\")\n",
    "else:\n",
    "    print(\"\\n✅ SUCCÈS : Tous les ingest_id sont présents\")\n",
    "\n",
    "# 2. Vérifier format (ISO ou compact)\n",
    "import re\n",
    "# Format compact: 20251003T205337Z\n",
    "compact_pattern = re.compile(r'^\\d{8}T\\d{6}Z$')\n",
    "# Format ISO étendu: YYYY-MM-DDTHH:MM:SS.sssZ_suffix\n",
    "iso_pattern = re.compile(r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}')\n",
    "\n",
    "sample_ids = con.execute(f\"\"\"\n",
    "    SELECT DISTINCT ingest_id \n",
    "    FROM read_parquet('{S3_PATTERN}')\n",
    "    WHERE ingest_id IS NOT NULL\n",
    "    LIMIT 100\n",
    "\"\"\").fetchdf()['ingest_id'].tolist()\n",
    "\n",
    "invalid_formats = [\n",
    "    id_val for id_val in sample_ids \n",
    "    if not compact_pattern.match(id_val) and not iso_pattern.match(id_val)\n",
    "]\n",
    "\n",
    "if invalid_formats:\n",
    "    print(f\"\\n⚠️ ATTENTION : {len(invalid_formats)} ingest_id avec format invalide\")\n",
    "    print(\"Exemples :\", invalid_formats[:5])\n",
    "else:\n",
    "    print(\"\\n✅ SUCCÈS : Tous les ingest_id ont un format valide\")\n",
    "\n",
    "# 3. Compter les batchs distincts\n",
    "nb_ingests = con.execute(f\"\"\"\n",
    "    SELECT COUNT(DISTINCT ingest_id) as distinct_ingests\n",
    "    FROM read_parquet('{S3_PATTERN}')\n",
    "    WHERE ingest_id IS NOT NULL\n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "print(f\"\\nNombre de batchs d'ingestion distincts : {nb_ingests}\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753fa8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== RÉSUMÉ ET CLÔTURE ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📊 RÉSUMÉ DES TESTS BRONZE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "✅ Test 1: Exhaustivité Globale (ZIP → Bronze avec transformations)\n",
    "✅ Test 2: Comparaison Fichier par Fichier (détection problèmes spécifiques)\n",
    "✅ Test 3: Unicité des Datetime (aucune duplication)\n",
    "✅ Test 4: Cohérence des Partitions (year/month/day alignés avec datetime)\n",
    "✅ Test 5: Traçabilité (présence et format ingest_id)\n",
    "\n",
    "\udcdd Notes:\n",
    "- Les transformations datalake_bronze sont appliquées avant comparaison\n",
    "- Tolérance: ±1% pour les tests globaux\n",
    "- Format timestamp: auto-détection µs/ms/s\n",
    "- Format ingest_id: compact (YYYYMMDDTHHMMSSz) ou ISO+suffix\n",
    "\n",
    "🔧 Diagnostic rapide:\n",
    "- Modifier DETAILED_FILE dans Test 2 pour analyser un fichier spécifique\n",
    "- Les datetime manquantes/extra sont affichées (10 premières)\n",
    "\"\"\")\n",
    "\n",
    "# Fermeture propre\n",
    "con.close()\n",
    "print(\"\\n✅ Tests terminés - Connexion DuckDB fermée\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hermes-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
