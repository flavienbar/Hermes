{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca78b1da",
   "metadata": {},
   "source": [
    "# ğŸ§ª Feature Store Test - DÃ©veloppement de Nouvelles Features\n",
    "\n",
    "## Architecture\n",
    "Ce notebook facilite le dÃ©veloppement et test de nouvelles features pour le backtesting :\n",
    "- **Source** : Gold Features principales (production)\n",
    "- **Traitement** : Ajout de nouvelles features expÃ©rimentales\n",
    "- **Sortie** : Table Gold Test pour backtesting avancÃ©\n",
    "- **CohÃ©rence** : PrÃ©servation de la continuitÃ© des calculs sur tout l'historique\n",
    "\n",
    "## Workflow\n",
    "1. **Configuration** : ParamÃ¨tres et chemins\n",
    "2. **Chargement** : Features Gold existantes\n",
    "3. **Ajout Features** : Nouvelles features expÃ©rimentales\n",
    "4. **Sauvegarde** : Table Gold Test pour backtesting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47432cdb",
   "metadata": {},
   "source": [
    "## 1. ğŸ“¦ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc596b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import talib as ta\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class FeatureStoreTestConfig:\n",
    "    # === PARAMÃˆTRES DE DONNÃ‰ES ===\n",
    "    provider: str = \"binance\"\n",
    "    market: str = \"spot\"\n",
    "    data_frequency: str = \"monthly\"\n",
    "    data_category: str = \"klines\"\n",
    "    symbol: str = \"BTCUSDT\"\n",
    "    interval: str = \"4h\"\n",
    "    \n",
    "    # === ARCHITECTURE MEDALLION ===\n",
    "    gold_bucket: str = \"gold\"\n",
    "    test_bucket: str = \"test\"\n",
    "    \n",
    "    # === PÃ‰RIODE ===\n",
    "    start_date: Optional[str] = \"2017-09-01\"  # 7 ans d'historique OU None pour tout\n",
    "    \n",
    "    # === CHUNKING & CONTINUITÃ‰ ===\n",
    "    chunk_size: int = 100_000  # Lignes par chunk\n",
    "    context_buffer: int = 200   # Buffer plus large pour features complexes\n",
    "    \n",
    "    # === NOUVELLES FEATURES (Ã  dÃ©velopper) ===\n",
    "    # ParamÃ¨tres pour nouvelles features expÃ©rimentales\n",
    "    experimental_features: Dict = None\n",
    "    \n",
    "    # === MINIO ===\n",
    "    minio_endpoint: str = \"127.0.0.1:9000\"\n",
    "    minio_access: str = \"minioadm\"\n",
    "    minio_secret: str = \"minioadm\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.experimental_features is None:\n",
    "            self.experimental_features = {\n",
    "                # Exemple de paramÃ¨tres pour futures features\n",
    "                \"volume_profile\": {\"bins\": 20, \"lookback\": 100},\n",
    "                \"support_resistance\": {\"window\": 50, \"min_strength\": 3},\n",
    "                \"market_regime\": {\"volatility_window\": 30, \"trend_window\": 50}\n",
    "            }\n",
    "    \n",
    "    # === CHEMINS CALCULÃ‰S (Architecture Hermes) ===\n",
    "    @property\n",
    "    def feature_store_table(self) -> str:\n",
    "        \"\"\"Nom de la table Gold Features principale\"\"\"\n",
    "        return f\"gold_features_{self.market}_{self.data_frequency}_{self.data_category}_{self.symbol}_{self.interval}\"\n",
    "    \n",
    "    @property\n",
    "    def feature_store_test_table(self) -> str:\n",
    "        \"\"\"Nom de la table Gold Features Test (avec nouvelles features)\"\"\"\n",
    "        return f\"gold_features_test_{self.market}_{self.data_frequency}_{self.data_category}_{self.symbol}_{self.interval}\"\n",
    "    \n",
    "    @property\n",
    "    def source_path(self) -> str:\n",
    "        \"\"\"Chemin source Gold Features principales\"\"\"\n",
    "        return f\"s3://{self.gold_bucket}/{self.feature_store_table}/**/*.parquet\"\n",
    "    \n",
    "    @property\n",
    "    def output_path(self) -> str:\n",
    "        \"\"\"Chemin de sortie Gold Features Test\"\"\"\n",
    "        return f\"s3://{self.test_bucket}/{self.feature_store_test_table}/\"\n",
    "\n",
    "config = FeatureStoreTestConfig()\n",
    "print(f\"âœ… Configuration chargÃ©e - {config.provider} {config.symbol} {config.interval}\")\n",
    "print(f\"ğŸ“… PÃ©riode: {config.start_date or 'Tout historique'} â†’ maintenant\")\n",
    "print(f\"ğŸ”„ Chunks: {config.chunk_size:,} lignes avec buffer {config.context_buffer}\")\n",
    "print(f\"\\nğŸ“ CHEMINS ARCHITECTURE HERMES:\")\n",
    "print(f\"   â€¢ Source: {config.source_path}\")\n",
    "print(f\"   â€¢ Test Output: {config.output_path}\")\n",
    "print(f\"   â€¢ Table Source: {config.feature_store_table}\")\n",
    "print(f\"   â€¢ Table Test: {config.feature_store_test_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab61855",
   "metadata": {},
   "source": [
    "## 2. ğŸ”Œ Connexion & Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d905afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation DuckDB + MinIO\n",
    "con = duckdb.connect()\n",
    "con.execute(f\"SET s3_endpoint='{config.minio_endpoint}';\")\n",
    "con.execute(f\"SET s3_access_key_id='{config.minio_access}';\")\n",
    "con.execute(f\"SET s3_secret_access_key='{config.minio_secret}';\")\n",
    "con.execute(\"SET s3_url_style='path'; SET s3_use_ssl='false';\")\n",
    "con.execute(\"SET threads TO 6; SET memory_limit = '4GB';\")\n",
    "\n",
    "# Analyse rapide des donnÃ©es source\n",
    "date_filter = f\"AND datetime >= '{config.start_date}'\" if config.start_date else \"\"\n",
    "\n",
    "try:\n",
    "    summary = con.execute(f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_rows,\n",
    "            MIN(datetime) as start_date,\n",
    "            MAX(datetime) as end_date,\n",
    "            COUNT(DISTINCT DATE_TRUNC('day', datetime)) as unique_days\n",
    "        FROM read_parquet('{config.source_path}')\n",
    "        WHERE symbol = '{config.symbol}' {date_filter}\n",
    "    \"\"\").fetchone()\n",
    "    \n",
    "    total_rows, start_date, end_date, days = summary\n",
    "    estimated_chunks = (total_rows // config.chunk_size) + 1\n",
    "    \n",
    "    print(f\"ğŸ“Š DONNÃ‰ES SOURCE DISPONIBLES:\")\n",
    "    print(f\"   â€¢ Lignes totales: {total_rows:,}\")\n",
    "    print(f\"   â€¢ PÃ©riode: {start_date} â†’ {end_date}\")\n",
    "    print(f\"   â€¢ Jours uniques: {days:,}\")\n",
    "    print(f\"   â€¢ Chunks estimÃ©s: {estimated_chunks:,}\")\n",
    "    print(f\"   â€¢ Temps estimÃ©: ~{estimated_chunks * 3:.0f} secondes\")\n",
    "    \n",
    "    # VÃ©rifier les colonnes disponibles\n",
    "    schema_result = con.execute(f\"\"\"\n",
    "        DESCRIBE SELECT * FROM read_parquet('{config.source_path}') LIMIT 1\n",
    "    \"\"\").fetchall()\n",
    "    \n",
    "    available_columns = [row[0] for row in schema_result]\n",
    "    print(f\"\\nğŸ“Š COLONNES DISPONIBLES ({len(available_columns)}):\")\n",
    "    \n",
    "    # Grouper par type\n",
    "    base_cols = [col for col in available_columns if col in ['datetime', 'symbol', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    indicator_cols = [col for col in available_columns if col not in base_cols]\n",
    "    \n",
    "    print(f\"   â€¢ Colonnes de base: {base_cols}\")\n",
    "    print(f\"   â€¢ Indicateurs disponibles: {len(indicator_cols)}\")\n",
    "    if len(indicator_cols) > 0:\n",
    "        print(f\"     - Premiers indicateurs: {indicator_cols[:10]}\")\n",
    "        if len(indicator_cols) > 10:\n",
    "            print(f\"     - ... et {len(indicator_cols)-10} autres\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur lors de l'analyse: {e}\")\n",
    "    total_rows, estimated_chunks = 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15680d",
   "metadata": {},
   "source": [
    "## 3. ğŸ§ª GÃ©nÃ©rateur de Nouvelles Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b2bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentalFeatureGenerator:\n",
    "    \"\"\"\n",
    "    ğŸ§ª GÃ‰NÃ‰RATEUR DE FEATURES EXPÃ‰RIMENTALES\n",
    "    \n",
    "    Ce gÃ©nÃ©rateur permet d'ajouter facilement de nouvelles features\n",
    "    aux donnÃ©es existantes tout en prÃ©servant la cohÃ©rence des calculs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: FeatureStoreTestConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def add_experimental_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Ajoute les nouvelles features expÃ©rimentales aux donnÃ©es\n",
    "        \n",
    "        Pour l'instant, aucune nouvelle feature n'est implÃ©mentÃ©e.\n",
    "        Cette fonction servira de template pour de futures features.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"ğŸ§ª Ajout de features expÃ©rimentales sur {len(df)} lignes...\")\n",
    "        \n",
    "        # Commencer avec les donnÃ©es originales\n",
    "        enhanced_df = df.clone()\n",
    "        \n",
    "        # === TEMPLATE POUR FUTURES FEATURES ===\n",
    "        # Exemple de structure pour ajouter de nouvelles features:\n",
    "        \n",
    "        # 1. Feature basÃ©e sur les volumes\n",
    "        # enhanced_df = enhanced_df.with_columns([\n",
    "        #     # Volume profile ou volume relatif\n",
    "        #     (pl.col('volume') / pl.col('volume').rolling_mean(20)).alias('volume_relative'),\n",
    "        #     # Volume momentum\n",
    "        #     (pl.col('volume') / pl.col('volume').shift(1) - 1).alias('volume_momentum')\n",
    "        # ])\n",
    "        \n",
    "        # 2. Features de volatilitÃ© avancÃ©es\n",
    "        # enhanced_df = enhanced_df.with_columns([\n",
    "        #     # VolatilitÃ© rÃ©alisÃ©e\n",
    "        #     ((pl.col('high') - pl.col('low')) / pl.col('close')).rolling_std(14).alias('realized_volatility'),\n",
    "        #     # VolatilitÃ© asymÃ©trique\n",
    "        #     pl.when(pl.col('close') > pl.col('close').shift(1))\n",
    "        #       .then((pl.col('close') / pl.col('close').shift(1) - 1))\n",
    "        #       .otherwise(0).rolling_std(14).alias('upside_volatility')\n",
    "        # ])\n",
    "        \n",
    "        # 3. Features de microstructure\n",
    "        # enhanced_df = enhanced_df.with_columns([\n",
    "        #     # Spread bid-ask estimÃ©\n",
    "        #     ((pl.col('high') - pl.col('low')) / pl.col('close')).alias('estimated_spread'),\n",
    "        #     # Price impact estimÃ©\n",
    "        #     (abs(pl.col('close') - pl.col('open')) / pl.col('volume')).alias('price_impact')\n",
    "        # ])\n",
    "        \n",
    "        # === MÃ‰TADONNÃ‰ES POUR TRACKING ===\n",
    "        enhanced_df = enhanced_df.with_columns([\n",
    "            # Timestamp de gÃ©nÃ©ration\n",
    "            pl.lit(datetime.now().isoformat()).alias('features_generated_at'),\n",
    "            # Version des features\n",
    "            pl.lit('v1.0.0').alias('features_version'),\n",
    "            # Type de table\n",
    "            pl.lit('test').alias('table_type')\n",
    "        ])\n",
    "        \n",
    "        # Compter les nouvelles colonnes\n",
    "        original_cols = len(df.columns)\n",
    "        new_cols = len(enhanced_df.columns)\n",
    "        added_features = new_cols - original_cols\n",
    "        \n",
    "        print(f\"   âœ… {added_features} nouvelles features ajoutÃ©es\")\n",
    "        print(f\"   ğŸ“Š Total colonnes: {original_cols} â†’ {new_cols}\")\n",
    "        \n",
    "        return enhanced_df\n",
    "    \n",
    "    def validate_features(self, df: pl.DataFrame) -> Dict:\n",
    "        \"\"\"Valide la qualitÃ© des nouvelles features\"\"\"\n",
    "        \n",
    "        validation = {\n",
    "            'total_rows': len(df),\n",
    "            'total_columns': len(df.columns),\n",
    "            'null_counts': {},\n",
    "            'warnings': []\n",
    "        }\n",
    "        \n",
    "        # VÃ©rifier les valeurs nulles\n",
    "        for col in df.columns:\n",
    "            null_count = df.select(pl.col(col).is_null().sum()).item()\n",
    "            if null_count > 0:\n",
    "                validation['null_counts'][col] = null_count\n",
    "                if null_count > len(df) * 0.1:  # Plus de 10% de nulls\n",
    "                    validation['warnings'].append(f\"Colonne {col}: {null_count} valeurs nulles ({null_count/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        return validation\n",
    "\n",
    "# Initialisation du gÃ©nÃ©rateur\n",
    "feature_generator = ExperimentalFeatureGenerator(config)\n",
    "\n",
    "print(\"ğŸ§ª GÃ©nÃ©rateur de features expÃ©rimentales initialisÃ©\")\n",
    "print(\"ğŸ’¡ PrÃªt pour l'ajout de nouvelles features personnalisÃ©es\")\n",
    "print(\"ğŸ“ Modifiez la mÃ©thode add_experimental_features() pour ajouter vos features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea5a11",
   "metadata": {},
   "source": [
    "## 4. ğŸ”„ Traitement par Chunks avec ContinuitÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features_chunked() -> List[pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    ğŸ”„ TRAITEMENT PAR CHUNKS AVEC CONTINUITÃ‰ GARANTIE\n",
    "    \n",
    "    Principe :\n",
    "    1. Charger chunk des features Gold existantes\n",
    "    2. Ajouter buffer de contexte pour continuitÃ© des calculs\n",
    "    3. GÃ©nÃ©rer nouvelles features expÃ©rimentales\n",
    "    4. Extraire rÃ©sultats sans contexte\n",
    "    5. Sauvegarder contexte pour chunk suivant\n",
    "    \"\"\"\n",
    "    \n",
    "    if total_rows == 0:\n",
    "        print(\"âŒ Pas de donnÃ©es Ã  traiter\")\n",
    "        return []\n",
    "    \n",
    "    print(\"ğŸš€ TRAITEMENT FEATURES PAR CHUNKS\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    all_results = []\n",
    "    context_buffer = None  # Buffer pour continuitÃ©\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Filtre de date\n",
    "    date_filter = f\"AND datetime >= '{config.start_date}'\" if config.start_date else \"\"\n",
    "    \n",
    "    # Traitement chunk par chunk\n",
    "    for offset in range(0, total_rows, config.chunk_size):\n",
    "        chunk_num = (offset // config.chunk_size) + 1\n",
    "        current_size = min(config.chunk_size, total_rows - offset)\n",
    "        \n",
    "        print(f\"[{chunk_num:>3}/{estimated_chunks}] Chunk {offset:,}-{offset+current_size:,}\", end=\" | \")\n",
    "        \n",
    "        try:\n",
    "            # === CHARGEMENT CHUNK ===\n",
    "            chunk_start = time.time()\n",
    "            \n",
    "            # Calculer offset avec contexte\n",
    "            actual_offset = offset\n",
    "            actual_limit = current_size\n",
    "            \n",
    "            # Ajouter contexte si pas le premier chunk\n",
    "            if offset > 0 and context_buffer is None:\n",
    "                actual_offset = max(0, offset - config.context_buffer)\n",
    "                actual_limit = current_size + (offset - actual_offset)\n",
    "            \n",
    "            # RequÃªte DuckDB\n",
    "            query = f\"\"\"\n",
    "                SELECT *\n",
    "                FROM read_parquet('{config.source_path}')\n",
    "                WHERE symbol = '{config.symbol}' {date_filter}\n",
    "                ORDER BY datetime\n",
    "                LIMIT {actual_limit} OFFSET {actual_offset}\n",
    "            \"\"\"\n",
    "            \n",
    "            chunk_df = pl.from_arrow(con.execute(query).arrow())\n",
    "            \n",
    "            if len(chunk_df) == 0:\n",
    "                print(\"âš ï¸ Chunk vide\")\n",
    "                break\n",
    "            \n",
    "            # === AJOUT CONTEXTE ===\n",
    "            if context_buffer is not None and offset > 0:\n",
    "                # Ã‰viter doublons temporels\n",
    "                last_context_time = context_buffer['datetime'].max()\n",
    "                chunk_df = chunk_df.filter(pl.col('datetime') > last_context_time)\n",
    "                \n",
    "                if len(chunk_df) > 0:\n",
    "                    chunk_df = pl.concat([context_buffer, chunk_df])\n",
    "            \n",
    "            # === GÃ‰NÃ‰RATION NOUVELLES FEATURES ===\n",
    "            enhanced_df = feature_generator.add_experimental_features(chunk_df)\n",
    "            \n",
    "            # === EXTRACTION RÃ‰SULTATS ===\n",
    "            context_size = len(context_buffer) if context_buffer is not None and offset > 0 else 0\n",
    "            \n",
    "            if context_size > 0:\n",
    "                result_df = enhanced_df.slice(context_size)  # Skip contexte\n",
    "            else:\n",
    "                result_df = enhanced_df\n",
    "            \n",
    "            # === MISE Ã€ JOUR CONTEXTE ===\n",
    "            if len(enhanced_df) > config.context_buffer:\n",
    "                context_buffer = enhanced_df.tail(config.context_buffer)\n",
    "            \n",
    "            # === VALIDATION ===\n",
    "            validation = feature_generator.validate_features(result_df)\n",
    "            \n",
    "            # === MÃ‰TRIQUES ===\n",
    "            chunk_time = time.time() - chunk_start\n",
    "            rows_per_sec = len(result_df) / max(chunk_time, 0.001)\n",
    "            \n",
    "            print(f\"{len(result_df):>5} lignes | {validation['total_columns']:>3} cols | âš¡ {chunk_time:.1f}s | {rows_per_sec:>6.0f} l/s\")\n",
    "            \n",
    "            # Afficher warnings de validation\n",
    "            for warning in validation.get('warnings', []):\n",
    "                print(f\"    âš ï¸ {warning}\")\n",
    "            \n",
    "            # Ajouter aux rÃ©sultats\n",
    "            if len(result_df) > 0:\n",
    "                all_results.append(result_df)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur: {e}\")\n",
    "            break\n",
    "    \n",
    "    # === RÃ‰SUMÃ‰ ===\n",
    "    total_time = time.time() - start_time\n",
    "    total_processed = sum(len(df) for df in all_results)\n",
    "    total_columns = len(all_results[0].columns) if all_results else 0\n",
    "    \n",
    "    print(\"=\" * 45)\n",
    "    print(f\"âœ… TRAITEMENT TERMINÃ‰\")\n",
    "    print(f\"ğŸ“Š Lignes traitÃ©es: {total_processed:,}\")\n",
    "    print(f\"ğŸ“Š Colonnes finales: {total_columns}\")\n",
    "    print(f\"ğŸ§ª Features ajoutÃ©es: {total_columns - len(available_columns) if 'available_columns' in locals() else 'N/A'}\")\n",
    "    print(f\"â±ï¸ Temps total: {total_time:.1f}s\")\n",
    "    print(f\"âš¡ Performance: {total_processed/max(total_time, 0.1):,.0f} lignes/sec\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Traitement des features\n",
    "if total_rows > 0:\n",
    "    enhanced_chunks = process_features_chunked()\n",
    "else:\n",
    "    enhanced_chunks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e31cc9",
   "metadata": {},
   "source": [
    "## 5. ğŸ’¾ Sauvegarde Gold Features Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db19f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_gold_test(chunks: List[pl.DataFrame]) -> bool:\n",
    "    \"\"\"Sauvegarde les features enrichies dans Gold Test Layer\"\"\"\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"âŒ Pas de features Ã  sauvegarder\")\n",
    "        return False\n",
    "    \n",
    "    print(\"ğŸ’¾ SAUVEGARDE GOLD FEATURES TEST\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    try:\n",
    "        # Consolidation\n",
    "        final_features = pl.concat(chunks)\n",
    "        print(f\"ğŸ“Š Consolidation: {len(final_features):,} lignes\")\n",
    "        print(f\"ğŸ“Š Colonnes totales: {len(final_features.columns)}\")\n",
    "        \n",
    "        # Ajout mÃ©tadonnÃ©es Gold Test\n",
    "        final_features = final_features.with_columns([\n",
    "            pl.col('datetime').dt.year().alias('year'),\n",
    "            pl.col('datetime').dt.month().alias('month'),\n",
    "            pl.lit(\"test\").alias(\"layer\"),\n",
    "            pl.lit(\"feature_store\").alias(\"data_type\"),\n",
    "            pl.lit(config.provider).alias(\"provider\"),\n",
    "            pl.lit(config.market).alias(\"market\"),\n",
    "            pl.lit(config.data_frequency).alias(\"data_frequency\"),\n",
    "            pl.lit(config.data_category).alias(\"data_category\"),\n",
    "            pl.lit(config.interval).alias(\"interval\")\n",
    "        ])\n",
    "        \n",
    "        # Chemin Gold Test partitionnÃ©\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        gold_test_path = f\"{config.output_path}version_{timestamp}/features.parquet\"\n",
    "        \n",
    "        # Export via DuckDB\n",
    "        con.register(\"temp_features\", final_features.to_arrow())\n",
    "        con.execute(f\"\"\"\n",
    "            COPY (SELECT * FROM temp_features ORDER BY datetime)\n",
    "            TO '{gold_test_path}'\n",
    "            (FORMAT PARQUET, COMPRESSION 'snappy')\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"âœ… Sauvegarde rÃ©ussie: {gold_test_path}\")\n",
    "        print(f\"ğŸ“Š PÃ©riode: {final_features['datetime'].min()} â†’ {final_features['datetime'].max()}\")\n",
    "        print(f\"ğŸ›ï¸ Architecture: Medallion Gold Test Layer\")\n",
    "        print(f\"ğŸ§ª PrÃªt pour backtesting avancÃ©\")\n",
    "        \n",
    "        # Sauvegarde mÃ©tadonnÃ©es\n",
    "        metadata = {\n",
    "            'table_info': {\n",
    "                'name': config.feature_store_test_table,\n",
    "                'type': 'feature_store_test',\n",
    "                'version': timestamp,\n",
    "                'source_table': config.feature_store_table\n",
    "            },\n",
    "            'data_info': {\n",
    "                'total_rows': len(final_features),\n",
    "                'total_columns': len(final_features.columns),\n",
    "                'period_start': str(final_features['datetime'].min()),\n",
    "                'period_end': str(final_features['datetime'].max()),\n",
    "                'symbol': config.symbol,\n",
    "                'interval': config.interval\n",
    "            },\n",
    "            'features_info': {\n",
    "                'experimental_features': config.experimental_features,\n",
    "                'generation_timestamp': datetime.now().isoformat()\n",
    "            },\n",
    "            'paths': {\n",
    "                'gold_test_path': gold_test_path,\n",
    "                'source_path': config.source_path\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        metadata_path = f\"{config.output_path}version_{timestamp}/metadata.parquet\"\n",
    "        metadata_df = pl.DataFrame([metadata])\n",
    "        con.register(\"temp_metadata\", metadata_df.to_arrow())\n",
    "        con.execute(f\"\"\"\n",
    "            COPY (SELECT * FROM temp_metadata)\n",
    "            TO '{metadata_path}'\n",
    "            (FORMAT PARQUET)\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"ğŸ“‹ MÃ©tadonnÃ©es sauvegardÃ©es: {metadata_path}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur sauvegarde: {e}\")\n",
    "        \n",
    "        # Sauvegarde locale de secours\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            local_path = f\"/tmp/features_test_{config.symbol}_{timestamp}.parquet\"\n",
    "            pl.concat(chunks).write_parquet(local_path)\n",
    "            print(f\"ğŸ’¾ Sauvegarde locale: {local_path}\")\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "# Sauvegarde\n",
    "if enhanced_chunks:\n",
    "    save_success = save_to_gold_test(enhanced_chunks)\n",
    "else:\n",
    "    save_success = False\n",
    "    print(\"âŒ Aucune feature Ã  sauvegarder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d84cfc1",
   "metadata": {},
   "source": [
    "## 6. ğŸ“Š RÃ©sumÃ© et Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b447d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RÃ©sumÃ© final\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ§ª FEATURE STORE TEST - RÃ‰SUMÃ‰ FINAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if enhanced_chunks:\n",
    "    total_rows = sum(len(chunk) for chunk in enhanced_chunks)\n",
    "    total_columns = len(enhanced_chunks[0].columns)\n",
    "    original_columns = len(available_columns) if 'available_columns' in locals() else 0\n",
    "    new_features = total_columns - original_columns\n",
    "    \n",
    "    print(f\"ğŸ“Š TRAITEMENT:\")\n",
    "    print(f\"   â€¢ PÃ©riode: {start_date} â†’ {end_date}\")\n",
    "    print(f\"   â€¢ Lignes traitÃ©es: {total_rows:,}\")\n",
    "    print(f\"   â€¢ Chunks: {len(enhanced_chunks)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ§ª FEATURES:\")\n",
    "    print(f\"   â€¢ Colonnes originales: {original_columns}\")\n",
    "    print(f\"   â€¢ Colonnes finales: {total_columns}\")\n",
    "    print(f\"   â€¢ Nouvelles features: {new_features}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ SAUVEGARDE: {'âœ… RÃ©ussie' if save_success else 'âŒ Ã‰chouÃ©e'}\")\n",
    "    print(f\"ğŸ›ï¸ Architecture: Medallion Gold Test Layer\")\n",
    "    print(f\"ğŸ“ Table: {config.feature_store_test_table}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Aucune feature gÃ©nÃ©rÃ©e\")\n",
    "\n",
    "print(f\"\\nâš™ï¸ CONFIGURATION:\")\n",
    "print(f\"   â€¢ Provider: {config.provider}\")\n",
    "print(f\"   â€¢ Symbole: {config.symbol} {config.interval}\")\n",
    "print(f\"   â€¢ Market: {config.market}\")\n",
    "print(f\"   â€¢ Frequency: {config.data_frequency}\")\n",
    "print(f\"   â€¢ Chunks: {config.chunk_size:,} + buffer {config.context_buffer}\")\n",
    "\n",
    "print(f\"\\nğŸš€ PROCHAINES Ã‰TAPES:\")\n",
    "print(\"   1. Modifier add_experimental_features() pour ajouter vos features\")\n",
    "print(\"   2. Tester sur petit Ã©chantillon avant historique complet\")\n",
    "print(\"   3. Utiliser la table test dans strategy_chunked_backtesting\")\n",
    "print(\"   4. Valider les nouvelles features avec backtesting\")\n",
    "\n",
    "print(\"\\nğŸ’¡ EXEMPLES DE FEATURES Ã€ AJOUTER:\")\n",
    "print(\"   â€¢ Volume relatif et momentum de volume\")\n",
    "print(\"   â€¢ VolatilitÃ© rÃ©alisÃ©e et asymÃ©trique\")\n",
    "print(\"   â€¢ Support/rÃ©sistance dynamiques\")\n",
    "print(\"   â€¢ RÃ©gime de marchÃ© (trend/sideways)\")\n",
    "print(\"   â€¢ Features de microstructure\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… FEATURE STORE TEST OPÃ‰RATIONNEL\")\n",
    "print(\"ğŸ§ª PrÃªt pour dÃ©veloppement de nouvelles features\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
