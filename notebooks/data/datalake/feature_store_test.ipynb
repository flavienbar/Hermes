{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca78b1da",
   "metadata": {},
   "source": [
    "# üß™ Feature Store Test - D√©veloppement de Nouvelles Features\n",
    "\n",
    "## Architecture\n",
    "Ce notebook facilite le d√©veloppement et test de nouvelles features pour le backtesting :\n",
    "- **Source** : Gold Features principales (production)\n",
    "- **Traitement** : Ajout de nouvelles features exp√©rimentales\n",
    "- **Sortie** : Table Gold Test pour backtesting avanc√©\n",
    "- **Coh√©rence** : Pr√©servation de la continuit√© des calculs sur tout l'historique\n",
    "\n",
    "## Workflow\n",
    "1. **Configuration** : Param√®tres et chemins\n",
    "2. **Chargement** : Features Gold existantes\n",
    "3. **Ajout Features** : Nouvelles features exp√©rimentales\n",
    "4. **Sauvegarde** : Table Gold Test pour backtesting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47432cdb",
   "metadata": {},
   "source": [
    "## 1. üì¶ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc596b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import talib as ta\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class FeatureStoreTestConfig:\n",
    "    # === PARAM√àTRES DE DONN√âES ===\n",
    "    provider: str = \"binance\"\n",
    "    market: str = \"spot\"\n",
    "    data_frequency: str = \"monthly\"\n",
    "    data_category: str = \"klines\"\n",
    "    symbol: str = \"BTCUSDT\"\n",
    "    interval: str = \"4h\"\n",
    "    \n",
    "    # === ARCHITECTURE MEDALLION ===\n",
    "    gold_bucket: str = \"gold\"\n",
    "    test_bucket: str = \"test\"\n",
    "    \n",
    "    # === P√âRIODE ===\n",
    "    start_date: Optional[str] = \"2017-09-01\"  # 7 ans d'historique OU None pour tout\n",
    "    \n",
    "    # === CHUNKING & CONTINUIT√â ===\n",
    "    chunk_size: int = 100_000  # Lignes par chunk\n",
    "    context_buffer: int = 200   # Buffer plus large pour features complexes\n",
    "    \n",
    "    # === NOUVELLES FEATURES (√† d√©velopper) ===\n",
    "    # Param√®tres pour nouvelles features exp√©rimentales\n",
    "    experimental_features: Dict = None\n",
    "    \n",
    "    # === MINIO ===\n",
    "    minio_endpoint: str = \"127.0.0.1:9000\"\n",
    "    minio_access: str = \"minioadm\"\n",
    "    minio_secret: str = \"minioadm\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.experimental_features is None:\n",
    "            self.experimental_features = {\n",
    "                # Exemple de param√®tres pour futures features\n",
    "                \"volume_profile\": {\"bins\": 20, \"lookback\": 100},\n",
    "                \"support_resistance\": {\"window\": 50, \"min_strength\": 3},\n",
    "                \"market_regime\": {\"volatility_window\": 30, \"trend_window\": 50}\n",
    "            }\n",
    "    \n",
    "    # === CHEMINS CALCUL√âS (Architecture Hermes) ===\n",
    "    @property\n",
    "    def feature_store_table(self) -> str:\n",
    "        \"\"\"Nom de la table Gold Features principale\"\"\"\n",
    "        return f\"gold_features_{self.market}_{self.data_frequency}_{self.data_category}_{self.symbol}_{self.interval}\"\n",
    "    \n",
    "    @property\n",
    "    def feature_store_test_table(self) -> str:\n",
    "        \"\"\"Nom de la table Gold Features Test (avec nouvelles features)\"\"\"\n",
    "        return f\"gold_features_test_{self.market}_{self.data_frequency}_{self.data_category}_{self.symbol}_{self.interval}\"\n",
    "    \n",
    "    @property\n",
    "    def source_path(self) -> str:\n",
    "        \"\"\"Chemin source Gold Features principales\"\"\"\n",
    "        return f\"s3://{self.gold_bucket}/{self.feature_store_table}/**/*.parquet\"\n",
    "    \n",
    "    @property\n",
    "    def output_path(self) -> str:\n",
    "        \"\"\"Chemin de sortie Gold Features Test\"\"\"\n",
    "        return f\"s3://{self.test_bucket}/{self.feature_store_test_table}/\"\n",
    "\n",
    "config = FeatureStoreTestConfig()\n",
    "print(f\"‚úÖ Configuration charg√©e - {config.provider} {config.symbol} {config.interval}\")\n",
    "print(f\"üìÖ P√©riode: {config.start_date or 'Tout historique'} ‚Üí maintenant\")\n",
    "print(f\"üîÑ Chunks: {config.chunk_size:,} lignes avec buffer {config.context_buffer}\")\n",
    "print(f\"\\nüìÅ CHEMINS ARCHITECTURE HERMES:\")\n",
    "print(f\"   ‚Ä¢ Source: {config.source_path}\")\n",
    "print(f\"   ‚Ä¢ Test Output: {config.output_path}\")\n",
    "print(f\"   ‚Ä¢ Table Source: {config.feature_store_table}\")\n",
    "print(f\"   ‚Ä¢ Table Test: {config.feature_store_test_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab61855",
   "metadata": {},
   "source": [
    "## 2. üîå Connexion & Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d905afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation DuckDB + MinIO\n",
    "con = duckdb.connect()\n",
    "con.execute(f\"SET s3_endpoint='{config.minio_endpoint}';\")\n",
    "con.execute(f\"SET s3_access_key_id='{config.minio_access}';\")\n",
    "con.execute(f\"SET s3_secret_access_key='{config.minio_secret}';\")\n",
    "con.execute(\"SET s3_url_style='path'; SET s3_use_ssl='false';\")\n",
    "con.execute(\"SET threads TO 6; SET memory_limit = '4GB';\")\n",
    "\n",
    "# Analyse rapide des donn√©es source\n",
    "date_filter = f\"AND datetime >= '{config.start_date}'\" if config.start_date else \"\"\n",
    "\n",
    "try:\n",
    "    summary = con.execute(f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_rows,\n",
    "            MIN(datetime) as start_date,\n",
    "            MAX(datetime) as end_date,\n",
    "            COUNT(DISTINCT DATE_TRUNC('day', datetime)) as unique_days\n",
    "        FROM read_parquet('{config.source_path}')\n",
    "        WHERE symbol = '{config.symbol}' {date_filter}\n",
    "    \"\"\").fetchone()\n",
    "    \n",
    "    total_rows, start_date, end_date, days = summary\n",
    "    estimated_chunks = (total_rows // config.chunk_size) + 1\n",
    "    \n",
    "    print(f\"üìä DONN√âES SOURCE DISPONIBLES:\")\n",
    "    print(f\"   ‚Ä¢ Lignes totales: {total_rows:,}\")\n",
    "    print(f\"   ‚Ä¢ P√©riode: {start_date} ‚Üí {end_date}\")\n",
    "    print(f\"   ‚Ä¢ Jours uniques: {days:,}\")\n",
    "    print(f\"   ‚Ä¢ Chunks estim√©s: {estimated_chunks:,}\")\n",
    "    print(f\"   ‚Ä¢ Temps estim√©: ~{estimated_chunks * 3:.0f} secondes\")\n",
    "    \n",
    "    # V√©rifier les colonnes disponibles\n",
    "    schema_result = con.execute(f\"\"\"\n",
    "        DESCRIBE SELECT * FROM read_parquet('{config.source_path}') LIMIT 1\n",
    "    \"\"\").fetchall()\n",
    "    \n",
    "    available_columns = [row[0] for row in schema_result]\n",
    "    print(f\"\\nüìä COLONNES DISPONIBLES ({len(available_columns)}):\")\n",
    "    \n",
    "    # Grouper par type\n",
    "    base_cols = [col for col in available_columns if col in ['datetime', 'symbol', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    indicator_cols = [col for col in available_columns if col not in base_cols]\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Colonnes de base: {base_cols}\")\n",
    "    print(f\"   ‚Ä¢ Indicateurs disponibles: {len(indicator_cols)}\")\n",
    "    if len(indicator_cols) > 0:\n",
    "        print(f\"     - Premiers indicateurs: {indicator_cols[:10]}\")\n",
    "        if len(indicator_cols) > 10:\n",
    "            print(f\"     - ... et {len(indicator_cols)-10} autres\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de l'analyse: {e}\")\n",
    "    total_rows, estimated_chunks = 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15680d",
   "metadata": {},
   "source": [
    "## 3. üß™ G√©n√©rateur de Nouvelles Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b2bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentalFeatureGenerator:\n",
    "    \"\"\"\n",
    "    üß™ G√âN√âRATEUR DE FEATURES EXP√âRIMENTALES\n",
    "    \n",
    "    Ce g√©n√©rateur permet d'ajouter facilement de nouvelles features\n",
    "    aux donn√©es existantes tout en pr√©servant la coh√©rence des calculs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: FeatureStoreTestConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def add_experimental_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Ajoute les nouvelles features exp√©rimentales aux donn√©es\n",
    "        \n",
    "        Pour l'instant, aucune nouvelle feature n'est impl√©ment√©e.\n",
    "        Cette fonction servira de template pour de futures features.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"üß™ Ajout de features exp√©rimentales sur {len(df)} lignes...\")\n",
    "        \n",
    "        # Commencer avec les donn√©es originales\n",
    "        enhanced_df = df.clone()\n",
    "        \n",
    "        # === TEMPLATE POUR FUTURES FEATURES ===\n",
    "        # Exemple de structure pour ajouter de nouvelles features:\n",
    "        \n",
    "        # 1. Feature bas√©e sur les volumes\n",
    "        # enhanced_df = enhanced_df.with_columns([\n",
    "        #     # Volume profile ou volume relatif\n",
    "        #     (pl.col('volume') / pl.col('volume').rolling_mean(20)).alias('volume_relative'),\n",
    "        #     # Volume momentum\n",
    "        #     (pl.col('volume') / pl.col('volume').shift(1) - 1).alias('volume_momentum')\n",
    "        # ])\n",
    "        \n",
    "        # 2. Features de volatilit√© avanc√©es\n",
    "        # enhanced_df = enhanced_df.with_columns([\n",
    "        #     # Volatilit√© r√©alis√©e\n",
    "        #     ((pl.col('high') - pl.col('low')) / pl.col('close')).rolling_std(14).alias('realized_volatility'),\n",
    "        #     # Volatilit√© asym√©trique\n",
    "        #     pl.when(pl.col('close') > pl.col('close').shift(1))\n",
    "        #       .then((pl.col('close') / pl.col('close').shift(1) - 1))\n",
    "        #       .otherwise(0).rolling_std(14).alias('upside_volatility')\n",
    "        # ])\n",
    "        \n",
    "        # 3. Features de microstructure\n",
    "        # enhanced_df = enhanced_df.with_columns([\n",
    "        #     # Spread bid-ask estim√©\n",
    "        #     ((pl.col('high') - pl.col('low')) / pl.col('close')).alias('estimated_spread'),\n",
    "        #     # Price impact estim√©\n",
    "        #     (abs(pl.col('close') - pl.col('open')) / pl.col('volume')).alias('price_impact')\n",
    "        # ])\n",
    "        \n",
    "        # === M√âTADONN√âES POUR TRACKING ===\n",
    "        enhanced_df = enhanced_df.with_columns([\n",
    "            # Timestamp de g√©n√©ration\n",
    "            pl.lit(datetime.now().isoformat()).alias('features_generated_at'),\n",
    "            # Version des features\n",
    "            pl.lit('v1.0.0').alias('features_version'),\n",
    "            # Type de table\n",
    "            pl.lit('test').alias('table_type')\n",
    "        ])\n",
    "        \n",
    "        # Compter les nouvelles colonnes\n",
    "        original_cols = len(df.columns)\n",
    "        new_cols = len(enhanced_df.columns)\n",
    "        added_features = new_cols - original_cols\n",
    "        \n",
    "        print(f\"   ‚úÖ {added_features} nouvelles features ajout√©es\")\n",
    "        print(f\"   üìä Total colonnes: {original_cols} ‚Üí {new_cols}\")\n",
    "        \n",
    "        return enhanced_df\n",
    "    \n",
    "    def validate_features(self, df: pl.DataFrame) -> Dict:\n",
    "        \"\"\"Valide la qualit√© des nouvelles features\"\"\"\n",
    "        \n",
    "        validation = {\n",
    "            'total_rows': len(df),\n",
    "            'total_columns': len(df.columns),\n",
    "            'null_counts': {},\n",
    "            'warnings': []\n",
    "        }\n",
    "        \n",
    "        # V√©rifier les valeurs nulles\n",
    "        for col in df.columns:\n",
    "            null_count = df.select(pl.col(col).is_null().sum()).item()\n",
    "            if null_count > 0:\n",
    "                validation['null_counts'][col] = null_count\n",
    "                if null_count > len(df) * 0.1:  # Plus de 10% de nulls\n",
    "                    validation['warnings'].append(f\"Colonne {col}: {null_count} valeurs nulles ({null_count/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        return validation\n",
    "\n",
    "# Initialisation du g√©n√©rateur\n",
    "feature_generator = ExperimentalFeatureGenerator(config)\n",
    "\n",
    "print(\"üß™ G√©n√©rateur de features exp√©rimentales initialis√©\")\n",
    "print(\"üí° Pr√™t pour l'ajout de nouvelles features personnalis√©es\")\n",
    "print(\"üìù Modifiez la m√©thode add_experimental_features() pour ajouter vos features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea5a11",
   "metadata": {},
   "source": [
    "## 4. üîÑ Traitement par Chunks avec Continuit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features_chunked() -> List[pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    üîÑ TRAITEMENT PAR CHUNKS AVEC CONTINUIT√â GARANTIE\n",
    "    \n",
    "    Principe :\n",
    "    1. Charger chunk des features Gold existantes\n",
    "    2. Ajouter buffer de contexte pour continuit√© des calculs\n",
    "    3. G√©n√©rer nouvelles features exp√©rimentales\n",
    "    4. Extraire r√©sultats sans contexte\n",
    "    5. Sauvegarder contexte pour chunk suivant\n",
    "    \"\"\"\n",
    "    \n",
    "    if total_rows == 0:\n",
    "        print(\"‚ùå Pas de donn√©es √† traiter\")\n",
    "        return []\n",
    "    \n",
    "    print(\"üöÄ TRAITEMENT FEATURES PAR CHUNKS\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    all_results = []\n",
    "    context_buffer = None  # Buffer pour continuit√©\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Filtre de date\n",
    "    date_filter = f\"AND datetime >= '{config.start_date}'\" if config.start_date else \"\"\n",
    "    \n",
    "    # Traitement chunk par chunk\n",
    "    for offset in range(0, total_rows, config.chunk_size):\n",
    "        chunk_num = (offset // config.chunk_size) + 1\n",
    "        current_size = min(config.chunk_size, total_rows - offset)\n",
    "        \n",
    "        print(f\"[{chunk_num:>3}/{estimated_chunks}] Chunk {offset:,}-{offset+current_size:,}\", end=\" | \")\n",
    "        \n",
    "        try:\n",
    "            # === CHARGEMENT CHUNK ===\n",
    "            chunk_start = time.time()\n",
    "            \n",
    "            # Calculer offset avec contexte\n",
    "            actual_offset = offset\n",
    "            actual_limit = current_size\n",
    "            \n",
    "            # Ajouter contexte si pas le premier chunk\n",
    "            if offset > 0 and context_buffer is None:\n",
    "                actual_offset = max(0, offset - config.context_buffer)\n",
    "                actual_limit = current_size + (offset - actual_offset)\n",
    "            \n",
    "            # Requ√™te DuckDB\n",
    "            query = f\"\"\"\n",
    "                SELECT *\n",
    "                FROM read_parquet('{config.source_path}')\n",
    "                WHERE symbol = '{config.symbol}' {date_filter}\n",
    "                ORDER BY datetime\n",
    "                LIMIT {actual_limit} OFFSET {actual_offset}\n",
    "            \"\"\"\n",
    "            \n",
    "            chunk_df = pl.from_arrow(con.execute(query).arrow())\n",
    "            \n",
    "            if len(chunk_df) == 0:\n",
    "                print(\"‚ö†Ô∏è Chunk vide\")\n",
    "                break\n",
    "            \n",
    "            # === AJOUT CONTEXTE ===\n",
    "            if context_buffer is not None and offset > 0:\n",
    "                # √âviter doublons temporels\n",
    "                last_context_time = context_buffer['datetime'].max()\n",
    "                chunk_df = chunk_df.filter(pl.col('datetime') > last_context_time)\n",
    "                \n",
    "                if len(chunk_df) > 0:\n",
    "                    chunk_df = pl.concat([context_buffer, chunk_df])\n",
    "            \n",
    "            # === G√âN√âRATION NOUVELLES FEATURES ===\n",
    "            enhanced_df = feature_generator.add_experimental_features(chunk_df)\n",
    "            \n",
    "            # === EXTRACTION R√âSULTATS ===\n",
    "            context_size = len(context_buffer) if context_buffer is not None and offset > 0 else 0\n",
    "            \n",
    "            if context_size > 0:\n",
    "                result_df = enhanced_df.slice(context_size)  # Skip contexte\n",
    "            else:\n",
    "                result_df = enhanced_df\n",
    "            \n",
    "            # === MISE √Ä JOUR CONTEXTE ===\n",
    "            if len(enhanced_df) > config.context_buffer:\n",
    "                context_buffer = enhanced_df.tail(config.context_buffer)\n",
    "            \n",
    "            # === VALIDATION ===\n",
    "            validation = feature_generator.validate_features(result_df)\n",
    "            \n",
    "            # === M√âTRIQUES ===\n",
    "            chunk_time = time.time() - chunk_start\n",
    "            rows_per_sec = len(result_df) / max(chunk_time, 0.001)\n",
    "            \n",
    "            print(f\"{len(result_df):>5} lignes | {validation['total_columns']:>3} cols | ‚ö° {chunk_time:.1f}s | {rows_per_sec:>6.0f} l/s\")\n",
    "            \n",
    "            # Afficher warnings de validation\n",
    "            for warning in validation.get('warnings', []):\n",
    "                print(f\"    ‚ö†Ô∏è {warning}\")\n",
    "            \n",
    "            # Ajouter aux r√©sultats\n",
    "            if len(result_df) > 0:\n",
    "                all_results.append(result_df)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur: {e}\")\n",
    "            break\n",
    "    \n",
    "    # === R√âSUM√â ===\n",
    "    total_time = time.time() - start_time\n",
    "    total_processed = sum(len(df) for df in all_results)\n",
    "    total_columns = len(all_results[0].columns) if all_results else 0\n",
    "    \n",
    "    print(\"=\" * 45)\n",
    "    print(f\"‚úÖ TRAITEMENT TERMIN√â\")\n",
    "    print(f\"üìä Lignes trait√©es: {total_processed:,}\")\n",
    "    print(f\"üìä Colonnes finales: {total_columns}\")\n",
    "    print(f\"üß™ Features ajout√©es: {total_columns - len(available_columns) if 'available_columns' in locals() else 'N/A'}\")\n",
    "    print(f\"‚è±Ô∏è Temps total: {total_time:.1f}s\")\n",
    "    print(f\"‚ö° Performance: {total_processed/max(total_time, 0.1):,.0f} lignes/sec\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Traitement des features\n",
    "if total_rows > 0:\n",
    "    enhanced_chunks = process_features_chunked()\n",
    "else:\n",
    "    enhanced_chunks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e31cc9",
   "metadata": {},
   "source": [
    "## 5. üíæ Sauvegarde Gold Features Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db19f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_gold_test(chunks: List[pl.DataFrame]) -> bool:\n",
    "    \"\"\"Sauvegarde les features enrichies dans Gold Test Layer\"\"\"\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"‚ùå Pas de features √† sauvegarder\")\n",
    "        return False\n",
    "    \n",
    "    print(\"üíæ SAUVEGARDE GOLD FEATURES TEST\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    try:\n",
    "        # Consolidation\n",
    "        final_features = pl.concat(chunks)\n",
    "        print(f\"üìä Consolidation: {len(final_features):,} lignes\")\n",
    "        print(f\"üìä Colonnes totales: {len(final_features.columns)}\")\n",
    "        \n",
    "        # Ajout m√©tadonn√©es Gold Test\n",
    "        final_features = final_features.with_columns([\n",
    "            pl.col('datetime').dt.year().alias('year'),\n",
    "            pl.col('datetime').dt.month().alias('month'),\n",
    "            pl.lit(\"test\").alias(\"layer\"),\n",
    "            pl.lit(\"feature_store\").alias(\"data_type\"),\n",
    "            pl.lit(config.provider).alias(\"provider\"),\n",
    "            pl.lit(config.market).alias(\"market\"),\n",
    "            pl.lit(config.data_frequency).alias(\"data_frequency\"),\n",
    "            pl.lit(config.data_category).alias(\"data_category\"),\n",
    "            pl.lit(config.interval).alias(\"interval\")\n",
    "        ])\n",
    "        \n",
    "        # Chemin Gold Test partitionn√©\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        gold_test_path = f\"{config.output_path}version_{timestamp}/features.parquet\"\n",
    "        \n",
    "        # Export via DuckDB\n",
    "        con.register(\"temp_features\", final_features.to_arrow())\n",
    "        con.execute(f\"\"\"\n",
    "            COPY (SELECT * FROM temp_features ORDER BY datetime)\n",
    "            TO '{gold_test_path}'\n",
    "            (FORMAT PARQUET, COMPRESSION 'snappy')\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"‚úÖ Sauvegarde r√©ussie: {gold_test_path}\")\n",
    "        print(f\"üìä P√©riode: {final_features['datetime'].min()} ‚Üí {final_features['datetime'].max()}\")\n",
    "        print(f\"üèõÔ∏è Architecture: Medallion Gold Test Layer\")\n",
    "        print(f\"üß™ Pr√™t pour backtesting avanc√©\")\n",
    "        \n",
    "        # Sauvegarde m√©tadonn√©es\n",
    "        metadata = {\n",
    "            'table_info': {\n",
    "                'name': config.feature_store_test_table,\n",
    "                'type': 'feature_store_test',\n",
    "                'version': timestamp,\n",
    "                'source_table': config.feature_store_table\n",
    "            },\n",
    "            'data_info': {\n",
    "                'total_rows': len(final_features),\n",
    "                'total_columns': len(final_features.columns),\n",
    "                'period_start': str(final_features['datetime'].min()),\n",
    "                'period_end': str(final_features['datetime'].max()),\n",
    "                'symbol': config.symbol,\n",
    "                'interval': config.interval\n",
    "            },\n",
    "            'features_info': {\n",
    "                'experimental_features': config.experimental_features,\n",
    "                'generation_timestamp': datetime.now().isoformat()\n",
    "            },\n",
    "            'paths': {\n",
    "                'gold_test_path': gold_test_path,\n",
    "                'source_path': config.source_path\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        metadata_path = f\"{config.output_path}version_{timestamp}/metadata.parquet\"\n",
    "        metadata_df = pl.DataFrame([metadata])\n",
    "        con.register(\"temp_metadata\", metadata_df.to_arrow())\n",
    "        con.execute(f\"\"\"\n",
    "            COPY (SELECT * FROM temp_metadata)\n",
    "            TO '{metadata_path}'\n",
    "            (FORMAT PARQUET)\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"üìã M√©tadonn√©es sauvegard√©es: {metadata_path}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur sauvegarde: {e}\")\n",
    "        \n",
    "        # Sauvegarde locale de secours\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            local_path = f\"/tmp/features_test_{config.symbol}_{timestamp}.parquet\"\n",
    "            pl.concat(chunks).write_parquet(local_path)\n",
    "            print(f\"üíæ Sauvegarde locale: {local_path}\")\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "# Sauvegarde\n",
    "if enhanced_chunks:\n",
    "    save_success = save_to_gold_test(enhanced_chunks)\n",
    "else:\n",
    "    save_success = False\n",
    "    print(\"‚ùå Aucune feature √† sauvegarder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d84cfc1",
   "metadata": {},
   "source": [
    "## 6. üìä R√©sum√© et Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b447d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© final\n",
    "print(\"=\" * 60)\n",
    "print(\"üß™ FEATURE STORE TEST - R√âSUM√â FINAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if enhanced_chunks:\n",
    "    total_rows = sum(len(chunk) for chunk in enhanced_chunks)\n",
    "    total_columns = len(enhanced_chunks[0].columns)\n",
    "    original_columns = len(available_columns) if 'available_columns' in locals() else 0\n",
    "    new_features = total_columns - original_columns\n",
    "    \n",
    "    print(f\"üìä TRAITEMENT:\")\n",
    "    print(f\"   ‚Ä¢ P√©riode: {start_date} ‚Üí {end_date}\")\n",
    "    print(f\"   ‚Ä¢ Lignes trait√©es: {total_rows:,}\")\n",
    "    print(f\"   ‚Ä¢ Chunks: {len(enhanced_chunks)}\")\n",
    "    \n",
    "    print(f\"\\nüß™ FEATURES:\")\n",
    "    print(f\"   ‚Ä¢ Colonnes originales: {original_columns}\")\n",
    "    print(f\"   ‚Ä¢ Colonnes finales: {total_columns}\")\n",
    "    print(f\"   ‚Ä¢ Nouvelles features: {new_features}\")\n",
    "    \n",
    "    print(f\"\\nüíæ SAUVEGARDE: {'‚úÖ R√©ussie' if save_success else '‚ùå √âchou√©e'}\")\n",
    "    print(f\"üèõÔ∏è Architecture: Medallion Gold Test Layer\")\n",
    "    print(f\"üìÅ Table: {config.feature_store_test_table}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Aucune feature g√©n√©r√©e\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è CONFIGURATION:\")\n",
    "print(f\"   ‚Ä¢ Provider: {config.provider}\")\n",
    "print(f\"   ‚Ä¢ Symbole: {config.symbol} {config.interval}\")\n",
    "print(f\"   ‚Ä¢ Market: {config.market}\")\n",
    "print(f\"   ‚Ä¢ Frequency: {config.data_frequency}\")\n",
    "print(f\"   ‚Ä¢ Chunks: {config.chunk_size:,} + buffer {config.context_buffer}\")\n",
    "\n",
    "print(f\"\\nüöÄ PROCHAINES √âTAPES:\")\n",
    "print(\"   1. Modifier add_experimental_features() pour ajouter vos features\")\n",
    "print(\"   2. Tester sur petit √©chantillon avant historique complet\")\n",
    "print(\"   3. Utiliser la table test dans strategy_chunked_backtesting\")\n",
    "print(\"   4. Valider les nouvelles features avec backtesting\")\n",
    "\n",
    "print(\"\\nüí° EXEMPLES DE FEATURES √Ä AJOUTER:\")\n",
    "print(\"   ‚Ä¢ Volume relatif et momentum de volume\")\n",
    "print(\"   ‚Ä¢ Volatilit√© r√©alis√©e et asym√©trique\")\n",
    "print(\"   ‚Ä¢ Support/r√©sistance dynamiques\")\n",
    "print(\"   ‚Ä¢ R√©gime de march√© (trend/sideways)\")\n",
    "print(\"   ‚Ä¢ Features de microstructure\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ FEATURE STORE TEST OP√âRATIONNEL\")\n",
    "print(\"üß™ Pr√™t pour d√©veloppement de nouvelles features\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
