{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "226599df",
   "metadata": {},
   "source": [
    "# üöÄ Hermes - Backtesting Chunked de Strat√©gies\n",
    "\n",
    "## Architecture\n",
    "Ce notebook suit l'architecture Medallion de Hermes :\n",
    "- **Source** : Hub Features Gold (indicateurs pr√©-calcul√©s)\n",
    "- **Traitement** : Chunks avec continuit√© pour gros volumes\n",
    "- **Analyse** : VectorBT pour validation des strat√©gies\n",
    "- **Sortie** : Table test pour r√©sultats interm√©diaires\n",
    "\n",
    "## Workflow\n",
    "1. **Configuration** : Imports et param√®tres\n",
    "2. **Connexion Sources** : Hub Features Gold (indicateurs pr√©-calcul√©s)\n",
    "3. **Strat√©gie Chunked** : G√©n√©ration signaux par chunks\n",
    "4. **Validation VectorBT** : Analyse des performances\n",
    "\n",
    "**Important** : Tous les indicateurs doivent √™tre pr√©-calcul√©s dans le Hub Features Gold\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650fd35",
   "metadata": {},
   "source": [
    "## 1. üì¶ Configuration et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16ef6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports charg√©s\n",
      "üêç Python: 3.10.18\n",
      "üìä Polars: 0.20.31\n",
      "üßÆ VectorBT: 0.25.5\n",
      "ü¶Ü DuckDB: 0.9.2\n"
     ]
    }
   ],
   "source": [
    "# Imports essentiels\n",
    "import os\n",
    "import json\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import vectorbt as vbt\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"‚úÖ Imports charg√©s\")\n",
    "print(f\"üêç Python: {os.sys.version.split()[0]}\")\n",
    "print(f\"üìä Polars: {pl.__version__}\")\n",
    "print(f\"üßÆ VectorBT: {vbt.__version__}\")\n",
    "print(f\"ü¶Ü DuckDB: {duckdb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea41c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuration initialis√©e\n",
      "üìä Symbole: BTCUSDT\n",
      "üîÑ Chunk size: 50,000 lignes\n",
      "üõ°Ô∏è Buffer contexte: 100 lignes\n",
      "üìÖ P√©riode: 2023-01-01 ‚Üí fin\n",
      "üí∞ Capital initial: $10,000.00\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class BacktestConfig:\n",
    "    \"\"\"Configuration pour le backtesting chunked\"\"\"\n",
    "    \n",
    "    # === PARAM√àTRES DE DONN√âES ===\n",
    "    provider: str = \"binance\"\n",
    "    market: str = \"spot\"\n",
    "    data_frequency: str = \"monthly\"\n",
    "    data_category: str = \"klines\"\n",
    "    symbol: str = \"BTCUSDT\"\n",
    "    interval: str = \"4h\"\n",
    "    \n",
    "    # === ARCHITECTURE MEDALLION ===\n",
    "    gold_bucket: str = \"gold\"\n",
    "    test_bucket: str = \"test\"\n",
    "    \n",
    "    # Param√®tres de chunking\n",
    "    chunk_size: int = 50_000  # Lignes par chunk\n",
    "    overlap_window: int = 100  # Lignes de contexte entre chunks\n",
    "    \n",
    "    # Fen√™tre temporelle (optionnel - None = tout l'historique)\n",
    "    start_date: Optional[str] = \"2023-01-01\"  # Format: \"YYYY-MM-DD\" ou None\n",
    "    end_date: Optional[str] = None\n",
    "    \n",
    "    # Param√®tres de strat√©gie par d√©faut\n",
    "    rsi_oversold: int = 30\n",
    "    rsi_neutral_low: int = 45\n",
    "    rsi_neutral_high: int = 55\n",
    "    ema_fast: int = 12\n",
    "    ema_slow: int = 26\n",
    "    supertrend_period: int = 10\n",
    "    supertrend_multiplier: float = 3.0\n",
    "\n",
    "    # Buffer contexte - calcul√© automatiquement\n",
    "    min_context_buffer: int = 50  # Minimum de s√©curit√©\n",
    "\n",
    "    def get_required_context_size(self) -> int:\n",
    "        \"\"\"Calcule la taille de contexte requise selon les indicateurs\"\"\"\n",
    "        # Prendre le plus grand indicateur + marge de s√©curit√©\n",
    "        max_indicator_period = max([\n",
    "            self.ema_fast,\n",
    "            self.ema_slow, \n",
    "            self.supertrend_period,\n",
    "            14,  # RSI par d√©faut\n",
    "            20,  # Bollinger Bands par d√©faut\n",
    "            26   # MACD par d√©faut\n",
    "        ])\n",
    "\n",
    "        # Ajouter une marge de s√©curit√© (50% du plus grand indicateur)\n",
    "        safety_margin = int(max_indicator_period * 0.5)\n",
    "        required_size = max_indicator_period + safety_margin\n",
    "        \n",
    "        # S'assurer d'avoir au moins le minimum\n",
    "        return max(required_size, self.min_context_buffer, self.overlap_window)\n",
    "    \n",
    "    # Backtesting\n",
    "    initial_cash: float = 10000.0\n",
    "    fees: float = 0.001  # 0.1%\n",
    "    \n",
    "    # MinIO\n",
    "    minio_endpoint: str = \"127.0.0.1:9000\"\n",
    "    minio_access_key: str = \"minioadm\"\n",
    "    minio_secret_key: str = \"minioadm\"\n",
    "    \n",
    "    # === SOURCES DE DONN√âES (CHEMINS CALCUL√âS) ===\n",
    "    use_test_features: bool = False  # Ajouter les features test aux features de base\n",
    "    \n",
    "    @property\n",
    "    def feature_store_table(self) -> str:\n",
    "        \"\"\"Nom de la table Gold Features principale (base)\"\"\"\n",
    "        return f\"gold_features_{self.market}_{self.data_frequency}_{self.data_category}_{self.symbol}_{self.interval}\"\n",
    "    \n",
    "    @property\n",
    "    def feature_store_test_table(self) -> str:\n",
    "        \"\"\"Nom de la table Gold Features Test (features suppl√©mentaires)\"\"\"\n",
    "        return f\"gold_features_test_{self.market}_{self.data_frequency}_{self.data_category}_{self.symbol}_{self.interval}\"\n",
    "    \n",
    "    @property\n",
    "    def feature_store_path(self) -> str:\n",
    "        \"\"\"Chemin source Features principales (toujours utilis√©es)\"\"\"\n",
    "        return f\"s3://{self.gold_bucket}/{self.feature_store_table}/**/*.parquet\"\n",
    "    \n",
    "    @property\n",
    "    def feature_store_test_path(self) -> str:\n",
    "        \"\"\"Chemin source Features Test (colonnes suppl√©mentaires)\"\"\"\n",
    "        return f\"s3://{self.test_bucket}/{self.feature_store_test_table}/**/*.parquet\"\n",
    "    \n",
    "    @property\n",
    "    def bronze_path(self) -> str:\n",
    "        \"\"\"Chemin Bronze pour r√©f√©rence\"\"\"\n",
    "        return f\"s3://bronze/{self.provider}/data/{self.market}/{self.data_frequency}/{self.data_category}/{self.symbol}/{self.interval}/**/*.parquet\"\n",
    "    \n",
    "    # Sortie\n",
    "    test_table_path: str = \"s3://test/backtest_results/\"\n",
    "    \n",
    "    def get_indicator_columns(self) -> Dict[str, str]:\n",
    "        \"\"\"Mapping des colonnes d'indicateurs\"\"\"\n",
    "        return {\n",
    "            \"ema_fast\": f\"ema_{self.ema_fast}\",\n",
    "            \"ema_slow\": f\"ema_{self.ema_slow}\",\n",
    "            \"rsi_14\": \"rsi_14\",\n",
    "            \"supertrend\": f\"supertrend_{self.supertrend_period}_{self.supertrend_multiplier}\",\n",
    "            \"supertrend_dir\": f\"supertrend_dir_{self.supertrend_period}_{self.supertrend_multiplier}\",\n",
    "            \"bb_upper\": \"bb_upper_20_2\",\n",
    "            \"bb_middle\": \"bb_middle_20_2\", \n",
    "            \"bb_lower\": \"bb_lower_20_2\",\n",
    "            \"macd\": \"macd_12_26_9\",\n",
    "            \"macd_signal\": \"macd_signal_12_26_9\",\n",
    "            \"atr_14\": \"atr_14\"\n",
    "        }\n",
    "\n",
    "# Configuration par d√©faut\n",
    "config = BacktestConfig()\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration initialis√©e\")\n",
    "print(f\"üìä Provider: {config.provider} | Symbole: {config.symbol} | Intervalle: {config.interval}\")\n",
    "print(f\"üîÑ Chunk size: {config.chunk_size:,} lignes\")\n",
    "print(f\"üõ°Ô∏è Buffer contexte: {config.get_required_context_size()} lignes\")\n",
    "print(f\"üìÖ P√©riode: {config.start_date} ‚Üí {config.end_date or 'fin'}\")\n",
    "print(f\"üí∞ Capital initial: ${config.initial_cash:,.2f}\")\n",
    "\n",
    "print(f\"\\nüìÅ CHEMINS ARCHITECTURE HERMES:\")\n",
    "print(f\"   ‚Ä¢ Features Base (toujours): {config.feature_store_path}\")\n",
    "print(f\"   ‚Ä¢ Features Test (ajout): {config.feature_store_test_path if config.use_test_features else 'Non utilis√©es'}\")\n",
    "print(f\"   ‚Ä¢ Table Base: {config.feature_store_table}\")\n",
    "print(f\"   ‚Ä¢ Table Test: {config.feature_store_test_table}\")\n",
    "print(f\"   ‚Ä¢ Bronze (r√©f√©rence): {config.bronze_path}\")\n",
    "\n",
    "print(f\"\\nüéØ ARCHITECTURE DE CONCAT√âNATION:\")\n",
    "print(f\"   ‚Ä¢ Base Features: {config.feature_store_table} (OHLCV + indicateurs valid√©s)\")\n",
    "print(f\"   ‚Ä¢ Test Features: {config.feature_store_test_table} (colonnes suppl√©mentaires)\")\n",
    "print(f\"   ‚Ä¢ Mode actuel: {'Base + Test (concat√©nation)' if config.use_test_features else 'Base uniquement'}\")\n",
    "print(f\"   üí° Changez use_test_features=True pour ajouter les features exp√©rimentales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adbd742",
   "metadata": {},
   "source": [
    "### üèõÔ∏è Architecture de Concat√©nation de Features\n",
    "\n",
    "**Chemins selon Architecture Hermes :**\n",
    "\n",
    "```python\n",
    "# Features Base (toujours charg√©es)\n",
    "s3://gold/gold_features_{market}_{frequency}_{category}_{symbol}_{interval}/**/*.parquet\n",
    "‚Üì\n",
    "s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/**/*.parquet\n",
    "(OHLCV + indicateurs valid√©s : EMA, RSI, SuperTrend, etc.)\n",
    "\n",
    "# Features Test (ajout√©es si use_test_features=True)\n",
    "s3://test/gold_features_test_{market}_{frequency}_{category}_{symbol}_{interval}/**/*.parquet\n",
    "‚Üì  \n",
    "s3://test/gold_features_test_spot_monthly_klines_BTCUSDT_4h/**/*.parquet\n",
    "(Colonnes suppl√©mentaires : nouveaux indicateurs exp√©rimentaux)\n",
    "\n",
    "# Bronze (r√©f√©rence)\n",
    "s3://bronze/{provider}/data/{market}/{frequency}/{category}/{symbol}/{interval}/**/*.parquet\n",
    "```\n",
    "\n",
    "**Mode de Concat√©nation :**\n",
    "- `use_test_features=False` ‚Üí Features base uniquement\n",
    "- `use_test_features=True` ‚Üí Features base + JOIN avec features test (par datetime)\n",
    "\n",
    "**Avantages :**\n",
    "- ‚úÖ **Efficacit√©** : Pas de recalcul des features de base\n",
    "- ‚úÖ **Modularit√©** : Ajout incr√©mental de nouvelles features\n",
    "- ‚úÖ **Performance** : Jointure optimis√©e par DuckDB\n",
    "- ‚úÖ **Flexibilit√©** : Test isol√© des nouvelles features sans impact sur la base\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adadcd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ CONFIGURATION ALTERNATIVE (optionnel)\n",
    "# D√©commentez pour tester avec d'autres param√®tres ou sources\n",
    "\n",
    "# config_test = BacktestConfig(\n",
    "#     symbol=\"ETHUSDT\",           # Autre crypto\n",
    "#     interval=\"1h\",              # Autre intervalle  \n",
    "#     data_frequency=\"daily\",     # Autre fr√©quence\n",
    "#     use_test_features=True,     # Ajouter features exp√©rimentales\n",
    "#     start_date=\"2024-01-01\"     # P√©riode plus courte\n",
    "# )\n",
    "# \n",
    "# print(\"üîÑ Configuration alternative:\")\n",
    "# print(f\"   ‚Ä¢ Features Base: {config_test.feature_store_path}\")\n",
    "# print(f\"   ‚Ä¢ Features Test: {config_test.feature_store_test_path}\")\n",
    "# print(f\"   ‚Ä¢ Mode: {'Base + Test' if config_test.use_test_features else 'Base uniquement'}\")\n",
    "\n",
    "print(\"üí° Pour tester avec features suppl√©mentaires:\")\n",
    "print(\"   1. Cr√©ez d'abord la table gold_features_test avec vos nouvelles colonnes\")\n",
    "print(\"   2. Changez config.use_test_features = True\") \n",
    "print(\"   3. Le syst√®me chargera automatiquement Base + Test en jointure\")\n",
    "print(\"   4. Ou utilisez une config alternative comme ci-dessus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2026e64",
   "metadata": {},
   "source": [
    "## 2. üîå Connexion aux Sources de Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc991dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç TEST DE CONNEXION ET VALIDATION DU CHEMIN\n",
      "=============================================\n",
      "üîå Configuration connexion DuckDB ‚Üí MinIO...\n",
      "‚úÖ Connexion DuckDB configur√©e\n",
      "\n",
      "üß™ Test 1: s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/**/*.parquet\n",
      "   ‚úÖ 5 fichier(s) trouv√©(s)\n",
      "     üìÅ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=10/data_0.parquet\n",
      "     üìÅ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=11/data_0.parquet\n",
      "     üìÅ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=12/data_0.parquet\n",
      "     ... et 2 autres\n",
      "   üìä Test lecture: 17,604 lignes, 1 symbole(s)\n",
      "   üéØ Chemin optimal trouv√© !\n",
      "\n",
      "üéØ Chemin final utilis√©: s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/**/*.parquet\n",
      "\n",
      "=============================================\n",
      "üìä Analyse des partitions disponibles...\n",
      "üìÅ 97 partitions trouv√©es\n",
      "üìÅ Fichiers trouv√©s:\n",
      "   ‚Ä¢ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=10/data_0.parquet\n",
      "   ‚Ä¢ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=11/data_0.parquet\n",
      "   ‚Ä¢ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=12/data_0.parquet\n",
      "   ‚Ä¢ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=8/data_0.parquet\n",
      "   ‚Ä¢ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=9/data_0.parquet\n",
      "   ‚Ä¢ ... et 92 autres\n",
      "üìà Analyse du contenu des donn√©es...\n",
      "üìä R√©sum√© des donn√©es:\n",
      "   ‚Ä¢ Total lignes: 5,844\n",
      "   ‚Ä¢ P√©riode: 2023-01-01 00:00:00 ‚Üí 2025-08-31 20:00:00\n",
      "   ‚Ä¢ Jours uniques: 974\n",
      "   ‚Ä¢ Chunks estim√©s: 1\n",
      "üìä R√©sum√© des donn√©es:\n",
      "   ‚Ä¢ Total lignes: 5,844\n",
      "   ‚Ä¢ P√©riode: 2023-01-01 00:00:00 ‚Üí 2025-08-31 20:00:00\n",
      "   ‚Ä¢ Jours uniques: 974\n",
      "   ‚Ä¢ Chunks estim√©s: 1\n"
     ]
    }
   ],
   "source": [
    "class HermesDataLoader:\n",
    "    \"\"\"Gestionnaire de connexion aux donn√©es Hermes avec support concat√©nation features\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BacktestConfig):\n",
    "        self.config = config\n",
    "        self.con = None\n",
    "        self.indicator_cols = config.get_indicator_columns()\n",
    "        \n",
    "        # Colonnes de base OHLCV\n",
    "        self.price_cols = ['datetime', 'open', 'high', 'low', 'close', 'volume']\n",
    "        \n",
    "        # Validation de la disponibilit√© des tables\n",
    "        self.base_table_available = False\n",
    "        self.test_table_available = False\n",
    "        \n",
    "    def setup_connection(self):\n",
    "        \"\"\"Configure la connexion DuckDB vers MinIO\"\"\"\n",
    "        print(\"üîå Configuration connexion DuckDB ‚Üí MinIO...\")\n",
    "        \n",
    "        self.con = duckdb.connect()\n",
    "        \n",
    "        # Configuration S3/MinIO\n",
    "        self.con.execute(f\"\"\"\n",
    "            SET s3_access_key_id='{self.config.minio_access_key}';\n",
    "            SET s3_secret_access_key='{self.config.minio_secret_key}';\n",
    "            SET s3_endpoint='{self.config.minio_endpoint}';\n",
    "            SET s3_url_style='path';\n",
    "            SET s3_use_ssl='false';\n",
    "        \"\"\")\n",
    "        \n",
    "        # Optimisations m√©moire\n",
    "        self.con.execute(\"\"\"\n",
    "            SET threads TO 6;\n",
    "            SET memory_limit = '4GB';\n",
    "            SET enable_progress_bar = true;\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"‚úÖ Connexion DuckDB configur√©e\")\n",
    "    \n",
    "    def get_partition_info(self) -> List[Dict]:\n",
    "        \"\"\"R√©cup√®re les informations des partitions disponibles\"\"\"\n",
    "        if not self.con:\n",
    "            self.setup_connection()\n",
    "        \n",
    "        print(\"üìä Analyse des partitions disponibles...\")\n",
    "        \n",
    "        try:\n",
    "            # R√©cup√©rer la liste des fichiers avec m√©tadonn√©es\n",
    "            result = self.con.execute(f\"\"\"\n",
    "                SELECT file as filename\n",
    "                FROM glob('{self.config.feature_store_path}')\n",
    "                ORDER BY file\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            partitions = []\n",
    "            for row in result:\n",
    "                partitions.append({\n",
    "                    'path': row[0],\n",
    "                    'size_mb': 0,  # Taille non disponible avec glob simple\n",
    "                    'last_modified': 'unknown'\n",
    "                })\n",
    "            \n",
    "            print(f\"üìÅ {len(partitions)} partitions trouv√©es\")\n",
    "            if partitions:\n",
    "                print(f\"üìÅ Fichiers trouv√©s:\")\n",
    "                for i, p in enumerate(partitions[:5]):  # Afficher les 5 premiers\n",
    "                    print(f\"   ‚Ä¢ {p['path']}\")\n",
    "                if len(partitions) > 5:\n",
    "                    print(f\"   ‚Ä¢ ... et {len(partitions)-5} autres\")\n",
    "            \n",
    "            return partitions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur lors de l'analyse des partitions: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def validate_tables_availability(self) -> Dict[str, bool]:\n",
    "        \"\"\"Valide la disponibilit√© des tables base et test\"\"\"\n",
    "        if not self.con:\n",
    "            self.setup_connection()\n",
    "        \n",
    "        print(\"üîç Validation des tables disponibles...\")\n",
    "        \n",
    "        # Test table base\n",
    "        try:\n",
    "            base_result = self.con.execute(f\"\"\"\n",
    "                SELECT COUNT(*) FROM read_parquet('{self.config.feature_store_path}') LIMIT 1\n",
    "            \"\"\").fetchone()\n",
    "            self.base_table_available = True\n",
    "            print(f\"   ‚úÖ Table base disponible: {self.config.feature_store_table}\")\n",
    "        except Exception as e:\n",
    "            self.base_table_available = False\n",
    "            print(f\"   ‚ùå Table base indisponible: {e}\")\n",
    "        \n",
    "        # Test table test si demand√©e\n",
    "        if self.config.use_test_features:\n",
    "            try:\n",
    "                test_result = self.con.execute(f\"\"\"\n",
    "                    SELECT COUNT(*) FROM read_parquet('{self.config.feature_store_test_path}') LIMIT 1\n",
    "                \"\"\").fetchone()\n",
    "                self.test_table_available = True\n",
    "                print(f\"   ‚úÖ Table test disponible: {self.config.feature_store_test_table}\")\n",
    "            except Exception as e:\n",
    "                self.test_table_available = False\n",
    "                print(f\"   ‚ö†Ô∏è Table test indisponible: {e}\")\n",
    "        else:\n",
    "            print(f\"   ‚ÑπÔ∏è Table test non demand√©e (use_test_features=False)\")\n",
    "        \n",
    "        return {\n",
    "            'base_available': self.base_table_available,\n",
    "            'test_available': self.test_table_available\n",
    "        }\n",
    "    \n",
    "    def get_combined_query(self, where_clause: str = \"\", limit_clause: str = \"\") -> str:\n",
    "        \"\"\"G√©n√®re la requ√™te combin√©e base + test si disponible\"\"\"\n",
    "        \n",
    "        base_query = f\"SELECT * FROM read_parquet('{self.config.feature_store_path}')\"\n",
    "        \n",
    "        if self.config.use_test_features and self.test_table_available:\n",
    "            # Requ√™te avec JOIN entre base et test\n",
    "            combined_query = f\"\"\"\n",
    "                SELECT \n",
    "                    base.*,\n",
    "                    test.* EXCLUDE (datetime, symbol, open, high, low, close, volume)\n",
    "                FROM ({base_query}) base\n",
    "                LEFT JOIN (\n",
    "                    SELECT * FROM read_parquet('{self.config.feature_store_test_path}')\n",
    "                ) test ON base.datetime = test.datetime AND base.symbol = test.symbol\n",
    "            \"\"\"\n",
    "            print(f\"üîó Mode: Base + Test (JOIN par datetime)\")\n",
    "        else:\n",
    "            combined_query = base_query\n",
    "            print(f\"üìä Mode: Base uniquement\")\n",
    "        \n",
    "        # Ajouter les clauses WHERE et LIMIT\n",
    "        if where_clause:\n",
    "            combined_query = f\"SELECT * FROM ({combined_query}) main {where_clause}\"\n",
    "        \n",
    "        if limit_clause:\n",
    "            combined_query += f\" {limit_clause}\"\n",
    "        \n",
    "        return combined_query\n",
    "    \n",
    "    def get_data_summary(self) -> Dict:\n",
    "        \"\"\"R√©cup√®re un r√©sum√© des donn√©es disponibles (combin√©es si applicable)\"\"\"\n",
    "        if not self.con:\n",
    "            self.setup_connection()\n",
    "        \n",
    "        # Valider les tables d'abord\n",
    "        self.validate_tables_availability()\n",
    "        \n",
    "        if not self.base_table_available:\n",
    "            print(\"‚ùå Table base indisponible - impossible de continuer\")\n",
    "            return {}\n",
    "        \n",
    "        print(\"üìà Analyse du contenu des donn√©es...\")\n",
    "        \n",
    "        # Requ√™te avec filtre temporel si sp√©cifi√©\n",
    "        where_clause = f\"WHERE symbol = '{self.config.symbol}'\"\n",
    "        if self.config.start_date:\n",
    "            where_clause += f\" AND datetime >= '{self.config.start_date}'\"\n",
    "        if self.config.end_date:\n",
    "            where_clause += f\" AND datetime <= '{self.config.end_date}'\"\n",
    "        \n",
    "        try:\n",
    "            # Utiliser la requ√™te combin√©e\n",
    "            combined_query = self.get_combined_query(where_clause)\n",
    "            \n",
    "            result = self.con.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_rows,\n",
    "                    MIN(datetime) as start_date,\n",
    "                    MAX(datetime) as end_date,\n",
    "                    COUNT(DISTINCT date_trunc('day', datetime)) as unique_days\n",
    "                FROM ({combined_query}) data\n",
    "            \"\"\").fetchone()\n",
    "            \n",
    "            summary = {\n",
    "                'total_rows': result[0],\n",
    "                'start_date': result[1],\n",
    "                'end_date': result[2],\n",
    "                'unique_days': result[3],\n",
    "                'using_test_features': self.config.use_test_features and self.test_table_available\n",
    "            }\n",
    "            \n",
    "            print(f\"üìä R√©sum√© des donn√©es {'(Base + Test)' if summary['using_test_features'] else '(Base)'}:\")\n",
    "            print(f\"   ‚Ä¢ Total lignes: {summary['total_rows']:,}\")\n",
    "            print(f\"   ‚Ä¢ P√©riode: {summary['start_date']} ‚Üí {summary['end_date']}\")\n",
    "            print(f\"   ‚Ä¢ Jours uniques: {summary['unique_days']:,}\")\n",
    "            \n",
    "            # Estimation des chunks\n",
    "            estimated_chunks = (summary['total_rows'] // self.config.chunk_size) + 1\n",
    "            print(f\"   ‚Ä¢ Chunks estim√©s: {estimated_chunks:,}\")\n",
    "            \n",
    "            return summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur lors de l'analyse: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Initialisation du loader\n",
    "data_loader = HermesDataLoader(config)\n",
    "\n",
    "# üîç Test de connexion et validation des chemins (base + test)\n",
    "print(\"üîç TEST DE CONNEXION ET VALIDATION DES CHEMINS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test direct des chemins avec glob\n",
    "try:\n",
    "    if not data_loader.con:\n",
    "        data_loader.setup_connection()\n",
    "    \n",
    "    # Test table BASE (obligatoire)\n",
    "    print(f\"\\nüìä TEST TABLE BASE:\")\n",
    "    base_test_paths = [\n",
    "        \"s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/**/*.parquet\",  # Pattern actuel\n",
    "        \"s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/*.parquet\",     # Pattern direct\n",
    "        \"s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/*\",             # Tous les fichiers\n",
    "    ]\n",
    "    \n",
    "    base_path_found = False\n",
    "    for i, test_path in enumerate(base_test_paths, 1):\n",
    "        print(f\"üß™ Test {i}: {test_path}\")\n",
    "        try:\n",
    "            # Test avec glob pour lister les fichiers\n",
    "            files_result = data_loader.con.execute(f\"\"\"\n",
    "                SELECT file as filename\n",
    "                FROM glob('{test_path}')\n",
    "                LIMIT 5\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            if files_result:\n",
    "                print(f\"   ‚úÖ {len(files_result)} fichier(s) base trouv√©(s)\")\n",
    "                \n",
    "                # Si on trouve des fichiers, tester la lecture\n",
    "                try:\n",
    "                    test_read = data_loader.con.execute(f\"\"\"\n",
    "                        SELECT COUNT(*) as row_count, COUNT(DISTINCT symbol) as symbols\n",
    "                        FROM read_parquet('{test_path}')\n",
    "                        LIMIT 1\n",
    "                    \"\"\").fetchone()\n",
    "                    \n",
    "                    if test_read:\n",
    "                        print(f\"   üìä Test lecture base: {test_read[0]:,} lignes, {test_read[1]} symbole(s)\")\n",
    "                        # Mettre √† jour la config avec le chemin qui fonctionne\n",
    "                        # Extraire le pattern et l'appliquer √† la propri√©t√©\n",
    "                        config.feature_store_path = test_path\n",
    "                        base_path_found = True\n",
    "                        print(f\"   üéØ Chemin base optimal trouv√© !\")\n",
    "                        break\n",
    "                        \n",
    "                except Exception as read_error:\n",
    "                    print(f\"   ‚ùå Erreur lecture base: {read_error}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Aucun fichier base trouv√©\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erreur test base: {e}\")\n",
    "    \n",
    "    # Test table TEST (si demand√©e)\n",
    "    if config.use_test_features:\n",
    "        print(f\"\\nüß™ TEST TABLE TEST:\")\n",
    "        test_table_paths = [\n",
    "            \"s3://test/gold_features_test_spot_monthly_klines_BTCUSDT_4h/**/*.parquet\",\n",
    "            \"s3://test/gold_features_test_spot_monthly_klines_BTCUSDT_4h/*.parquet\",\n",
    "            \"s3://test/gold_features_test_spot_monthly_klines_BTCUSDT_4h/*\",\n",
    "        ]\n",
    "        \n",
    "        test_path_found = False\n",
    "        for i, test_path in enumerate(test_table_paths, 1):\n",
    "            print(f\"üß™ Test {i}: {test_path}\")\n",
    "            try:\n",
    "                files_result = data_loader.con.execute(f\"\"\"\n",
    "                    SELECT file as filename\n",
    "                    FROM glob('{test_path}')\n",
    "                    LIMIT 3\n",
    "                \"\"\").fetchall()\n",
    "                \n",
    "                if files_result:\n",
    "                    print(f\"   ‚úÖ {len(files_result)} fichier(s) test trouv√©(s)\")\n",
    "                    try:\n",
    "                        test_read = data_loader.con.execute(f\"\"\"\n",
    "                            SELECT COUNT(*) as row_count\n",
    "                            FROM read_parquet('{test_path}')\n",
    "                            LIMIT 1\n",
    "                        \"\"\").fetchone()\n",
    "                        \n",
    "                        if test_read:\n",
    "                            print(f\"   üìä Test lecture test: {test_read[0]:,} lignes\")\n",
    "                            test_path_found = True\n",
    "                            print(f\"   üéØ Chemin test trouv√© !\")\n",
    "                            break\n",
    "                            \n",
    "                    except Exception as read_error:\n",
    "                        print(f\"   ‚ùå Erreur lecture test: {read_error}\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Aucun fichier test trouv√©\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Erreur test: {e}\")\n",
    "        \n",
    "        if not test_path_found:\n",
    "            print(f\"   ‚ö†Ô∏è Table test non trouv√©e - passage en mode base uniquement\")\n",
    "            config.use_test_features = False\n",
    "    else:\n",
    "        print(f\"\\nüí° Table test non demand√©e (use_test_features=False)\")\n",
    "    \n",
    "    print(f\"\\nüéØ CONFIGURATION FINALE:\")\n",
    "    print(f\"   ‚Ä¢ Chemin base: {config.feature_store_path}\")\n",
    "    if config.use_test_features:\n",
    "        print(f\"   ‚Ä¢ Chemin test: {config.feature_store_test_path}\")\n",
    "        print(f\"   ‚Ä¢ Mode: Base + Test (concat√©nation)\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Mode: Base uniquement\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors du test de connexion: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Maintenant charger les infos avec le bon chemin\n",
    "partitions = data_loader.get_partition_info()\n",
    "data_summary = data_loader.get_data_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df3c10",
   "metadata": {},
   "source": [
    "## 3. üìä Processeur de Backtesting Chunked avec Garanties de Coh√©rence\n",
    "\n",
    "### Syst√®me de traitement par chunks avec continuit√© des signaux et positions\n",
    "\n",
    "**üõ°Ô∏è Garanties de Coh√©rence Impl√©ment√©es** :\n",
    "\n",
    "1. **Buffer de Contexte √âtendu** : 50+ lignes de contexte entre chunks\n",
    "2. **Validation des Signaux `shift()`** : V√©rification que les valeurs pr√©c√©dentes existent\n",
    "3. **D√©tection de Chevauchements** : Gestion automatique des doublons temporels\n",
    "4. **Validation Continue** : Contr√¥les √† chaque √©tape du traitement\n",
    "5. **Diagnostic Pr√©alable** : V√©rification de la coh√©rence des donn√©es source\n",
    "\n",
    "**Note** : Ce processeur utilise uniquement les indicateurs pr√©-calcul√©s dans le Hub Features Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Backtester chunked initialis√© avec garanties de coh√©rence\n",
      "‚öôÔ∏è Configuration: 50,000 lignes/chunk avec 100 lignes de contexte\n",
      "üí∞ Capital initial: $10,000.00\n",
      "üìä Tous les indicateurs doivent √™tre pr√©-calcul√©s dans le Hub Features Gold\n",
      "‚úÖ Syst√®me de validation de continuit√© des signaux activ√©\n"
     ]
    }
   ],
   "source": [
    "class HermesChunkedBacktester:\n",
    "    \"\"\"Backtesting chunked avec continuit√© pour strat√©gies Hermes\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BacktestConfig, data_loader: HermesDataLoader):\n",
    "        self.config = config\n",
    "        self.data_loader = data_loader\n",
    "        self.indicator_cols = config.get_indicator_columns()\n",
    "        \n",
    "        # √âtat persistant entre chunks\n",
    "        self.state = {\n",
    "            'context_rows': None,        # Lignes de contexte pour continuit√©\n",
    "            'last_position': None,       # 'long', 'short' ou None\n",
    "            'cumulative_cash': config.initial_cash,\n",
    "            'cumulative_value': config.initial_cash,\n",
    "            'total_trades': 0,\n",
    "            'chunk_results': [],         # R√©sultats par chunk\n",
    "            'chunk_counter': 0,\n",
    "            'context_buffer_size': config.get_required_context_size()  #  Calcul automatique du buffer plus grand pour les signaux\n",
    "        }\n",
    "    \n",
    "    def compute_strategy_signals(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Calcule les signaux de la strat√©gie Smart Momentum avec validation de coh√©rence\"\"\"\n",
    "        \n",
    "        # R√©cup√©rer les colonnes d'indicateurs\n",
    "        ema_fast_col = self.indicator_cols[\"ema_fast\"]\n",
    "        ema_slow_col = self.indicator_cols[\"ema_slow\"]\n",
    "        rsi_col = self.indicator_cols[\"rsi_14\"]\n",
    "        supertrend_dir_col = self.indicator_cols[\"supertrend_dir\"]\n",
    "        \n",
    "        # V√©rifier que les colonnes existent\n",
    "        missing_cols = []\n",
    "        for col_name, col_actual in self.indicator_cols.items():\n",
    "            if col_actual not in df.columns:\n",
    "                missing_cols.append(f\"{col_name} -> {col_actual}\")\n",
    "        \n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"‚ùå Colonnes d'indicateurs manquantes: {missing_cols}\")\n",
    "        \n",
    "        # Calcul des signaux avec validation de continuit√©\n",
    "        signals_df = df.with_columns([\n",
    "            # === VALIDATION DE CONTINUIT√â ===\n",
    "            # Marquer les lignes o√π shift(1) sera valide\n",
    "            (pl.int_range(pl.len()) > 0).alias(\"has_previous_value\"),\n",
    "            \n",
    "            # === CONDITIONS EMA ===\n",
    "            # Condition actuelle : EMA rapide > EMA lente\n",
    "            (pl.col(ema_fast_col) > pl.col(ema_slow_col)).alias(\"ema_fast_above_slow\"),\n",
    "            \n",
    "            # Condition pr√©c√©dente : EMA rapide <= EMA lente (avec gestion des nulls)\n",
    "            (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)).alias(\"ema_was_below_or_equal\"),\n",
    "            \n",
    "            # Crossover EMA seulement si on a une valeur pr√©c√©dente valide\n",
    "            ((pl.col(ema_fast_col) > pl.col(ema_slow_col)) & \n",
    "             (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)) &\n",
    "             (pl.int_range(pl.len()) > 0)).alias(\"ema_bullish_cross\"),\n",
    "            \n",
    "            # === CONDITIONS RSI ===\n",
    "            ((pl.col(rsi_col) >= self.config.rsi_neutral_low) & \n",
    "             (pl.col(rsi_col) <= self.config.rsi_neutral_high)).alias(\"rsi_neutral\"),\n",
    "            \n",
    "            # === CONDITIONS SUPERTREND ===\n",
    "            (pl.col(supertrend_dir_col) == 1).alias(\"supertrend_bullish\"),\n",
    "            \n",
    "            # SuperTrend exit avec validation de continuit√©\n",
    "            ((pl.col(supertrend_dir_col).shift(1) == 1) & \n",
    "             (pl.col(supertrend_dir_col) == -1) &\n",
    "             (pl.int_range(pl.len()) > 0)).alias(\"supertrend_exit\")\n",
    "        ])\n",
    "        \n",
    "        # Signaux finaux avec validation\n",
    "        final_signals = signals_df.with_columns([\n",
    "            # Signal d'entr√©e (achat) - uniquement si toutes conditions remplies\n",
    "            (pl.col(\"ema_bullish_cross\") & \n",
    "             pl.col(\"rsi_neutral\") & \n",
    "             pl.col(\"supertrend_bullish\")).alias(\"buy_signal\"),\n",
    "            \n",
    "            # Signal de sortie (vente) - uniquement avec continuit√© valid√©e\n",
    "            (pl.col(\"supertrend_exit\")).alias(\"sell_signal\")\n",
    "        ])\n",
    "        \n",
    "        return final_signals\n",
    "    \n",
    "    def load_chunk_with_context(self, offset: int, limit: int) -> pl.DataFrame:\n",
    "        \"\"\"Charge un chunk avec le contexte n√©cessaire pour garantir la coh√©rence (base + test)\"\"\"\n",
    "        \n",
    "        # Construire la requ√™te avec filtre temporel\n",
    "        where_clause = f\"WHERE symbol = '{self.config.symbol}'\"\n",
    "        if self.config.start_date:\n",
    "            where_clause += f\" AND datetime >= '{self.config.start_date}'\"\n",
    "        if self.config.end_date:\n",
    "            where_clause += f\" AND datetime <= '{self.config.end_date}'\"\n",
    "        \n",
    "        # AM√âLIORATION : Charger plus de contexte pour les premiers chunks\n",
    "        actual_offset = offset\n",
    "        actual_limit = limit\n",
    "        \n",
    "        # Si ce n'est pas le premier chunk, commencer plus t√¥t pour avoir du contexte\n",
    "        if offset > 0 and self.state['context_rows'] is None:\n",
    "            # Charger du contexte suppl√©mentaire depuis la base\n",
    "            context_needed = self.state['context_buffer_size']\n",
    "            actual_offset = max(0, offset - context_needed)\n",
    "            actual_limit = limit + (offset - actual_offset)\n",
    "        \n",
    "        # Utiliser la requ√™te combin√©e (base + test si disponible)\n",
    "        base_query = self.data_loader.get_combined_query(where_clause)\n",
    "        \n",
    "        # Requ√™te DuckDB pour le chunk avec contexte\n",
    "        query = f\"\"\"\n",
    "            SELECT * FROM ({base_query}) data\n",
    "            ORDER BY datetime\n",
    "            LIMIT {actual_limit}\n",
    "            OFFSET {actual_offset}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ex√©cuter la requ√™te\n",
    "        result = self.data_loader.con.execute(query).arrow()\n",
    "        chunk_df = pl.from_arrow(result)\n",
    "        \n",
    "        # Ajouter le contexte des lignes pr√©c√©dentes si disponible\n",
    "        if self.state['context_rows'] is not None and offset > 0:\n",
    "            # V√©rifier la continuit√© temporelle\n",
    "            if len(self.state['context_rows']) > 0 and len(chunk_df) > 0:\n",
    "                last_context_time = self.state['context_rows']['datetime'].max()\n",
    "                first_chunk_time = chunk_df['datetime'].min()\n",
    "                \n",
    "                if last_context_time >= first_chunk_time:\n",
    "                    print(f\"‚ö†Ô∏è Chevauchement temporel d√©tect√© - ajustement automatique\")\n",
    "                    # Filtrer les doublons\n",
    "                    chunk_df = chunk_df.filter(pl.col('datetime') > last_context_time)\n",
    "            \n",
    "            # Concat√©ner avec le contexte\n",
    "            if len(chunk_df) > 0:\n",
    "                chunk_df = pl.concat([self.state['context_rows'], chunk_df])\n",
    "        \n",
    "        return chunk_df\n",
    "    \n",
    "    def process_chunk(self, chunk_df: pl.DataFrame, is_first_chunk: bool) -> Dict:\n",
    "        \"\"\"Traite un chunk avec calcul des signaux et backtesting\"\"\"\n",
    "        \n",
    "        if len(chunk_df) == 0:\n",
    "            return {\n",
    "                'chunk_id': self.state['chunk_counter'],\n",
    "                'rows_processed': 0,\n",
    "                'buy_signals': 0,\n",
    "                'sell_signals': 0,\n",
    "                'start_time': None,\n",
    "                'end_time': None,\n",
    "                'data': None,\n",
    "                'warnings': ['Chunk vide']\n",
    "            }\n",
    "        \n",
    "        # 1. Calculer les signaux de strat√©gie avec validation de coh√©rence\n",
    "        try:\n",
    "            signals_df = self.compute_strategy_signals(chunk_df)\n",
    "        except ValueError as e:\n",
    "            print(f\"‚ùå Erreur dans le calcul des signaux: {e}\")\n",
    "            return {\n",
    "                'chunk_id': self.state['chunk_counter'],\n",
    "                'rows_processed': 0,\n",
    "                'buy_signals': 0,\n",
    "                'sell_signals': 0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "        \n",
    "        # 2. Ajuster les signaux selon l'√©tat pr√©c√©dent\n",
    "        if not is_first_chunk and self.state['last_position'] is not None:\n",
    "            # Si on √©tait en position, ne pas g√©n√©rer d'entr√©e imm√©diate\n",
    "            if self.state['last_position'] == 'long':\n",
    "                signals_df = signals_df.with_columns(\n",
    "                    pl.when(pl.int_range(pl.len()) == 0)\n",
    "                    .then(False)\n",
    "                    .otherwise(pl.col(\"buy_signal\"))\n",
    "                    .alias(\"buy_signal\")\n",
    "                )\n",
    "        \n",
    "        # 3. Extraire les r√©sultats sans le contexte\n",
    "        context_size = len(self.state['context_rows']) if self.state['context_rows'] is not None and not is_first_chunk else 0\n",
    "        \n",
    "        if context_size > 0:\n",
    "            result_df = signals_df.slice(context_size)\n",
    "        else:\n",
    "            result_df = signals_df\n",
    "        \n",
    "        # 4. Validation des signaux calcul√©s\n",
    "        warnings = []\n",
    "        if len(result_df) > 0:\n",
    "            # V√©rifier qu'on n'a pas de signaux sur la premi√®re ligne d'un chunk (sauf premier chunk)\n",
    "            if not is_first_chunk and context_size == 0:\n",
    "                first_row_signals = result_df.head(1)\n",
    "                if (first_row_signals.select(pl.col(\"buy_signal\").sum()).item() > 0 or \n",
    "                    first_row_signals.select(pl.col(\"sell_signal\").sum()).item() > 0):\n",
    "                    warnings.append(\"Signaux d√©tect√©s sur premi√®re ligne sans contexte\")\n",
    "        \n",
    "        # 5. Compter les signaux\n",
    "        buy_signals = result_df.select(pl.col(\"buy_signal\").sum()).item() if len(result_df) > 0 else 0\n",
    "        sell_signals = result_df.select(pl.col(\"sell_signal\").sum()).item() if len(result_df) > 0 else 0\n",
    "        \n",
    "        # 6. Mettre √† jour l'√©tat pour le chunk suivant avec plus de contexte\n",
    "        if len(signals_df) > 0:\n",
    "            self.state['context_rows'] = signals_df.tail(self.state['context_buffer_size'])\n",
    "        \n",
    "        # Simuler la position (logique simplifi√©e)\n",
    "        if buy_signals > 0 and self.state['last_position'] != 'long':\n",
    "            self.state['last_position'] = 'long'\n",
    "        elif sell_signals > 0 and self.state['last_position'] == 'long':\n",
    "            self.state['last_position'] = None\n",
    "            self.state['total_trades'] += 1\n",
    "        \n",
    "        # 7. Retourner les r√©sultats du chunk avec m√©tadonn√©es de validation\n",
    "        chunk_result = {\n",
    "            'chunk_id': self.state['chunk_counter'],\n",
    "            'rows_processed': len(result_df),\n",
    "            'buy_signals': buy_signals,\n",
    "            'sell_signals': sell_signals,\n",
    "            'start_time': result_df.select(pl.col(\"datetime\").min()).item() if len(result_df) > 0 else None,\n",
    "            'end_time': result_df.select(pl.col(\"datetime\").max()).item() if len(result_df) > 0 else None,\n",
    "            'data': result_df,  # Conserver les donn√©es pour sauvegarde\n",
    "            'context_size': context_size,\n",
    "            'warnings': warnings,\n",
    "            'continuity_validated': context_size > 0 or is_first_chunk\n",
    "        }\n",
    "        \n",
    "        self.state['chunk_counter'] += 1\n",
    "        \n",
    "        return chunk_result\n",
    "\n",
    "# Initialisation du backtester\n",
    "backtester = HermesChunkedBacktester(config, data_loader)\n",
    "\n",
    "print(\"üöÄ Backtester chunked initialis√© avec garanties de coh√©rence\")\n",
    "print(f\"‚öôÔ∏è Configuration: {config.chunk_size:,} lignes/chunk avec {backtester.state['context_buffer_size']} lignes de contexte\")\n",
    "print(f\"üí∞ Capital initial: ${config.initial_cash:,.2f}\")\n",
    "print(\"üìä Features de base toujours charg√©es depuis la table Gold\")\n",
    "if config.use_test_features:\n",
    "    print(\"üß™ Features test ajout√©es par JOIN (nouvelles colonnes exp√©rimentales)\")\n",
    "else:\n",
    "    print(\"üè≠ Mode production: features de base uniquement\")\n",
    "print(\"‚úÖ Syst√®me de validation de continuit√© des signaux activ√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce5e05c",
   "metadata": {},
   "source": [
    "## 4. üîÑ Ex√©cution du Backtesting Chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19f8c0",
   "metadata": {},
   "source": [
    "## 4.1 üîç Diagnostic de Coh√©rence des Donn√©es\n",
    "\n",
    "Avant d'ex√©cuter le backtesting, v√©rifions la coh√©rence des donn√©es et la continuit√© temporelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3ad4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DIAGNOSTIC DE COHERENCE DES DONN√âES\n",
      "========================================\n",
      "üìÖ V√©rification de la continuit√© temporelle...\n",
      "   ‚Ä¢ Lignes analys√©es: 9,999\n",
      "   ‚Ä¢ Intervalles uniques: 4\n",
      "   ‚Ä¢ Intervalle min: 4.00 heures\n",
      "   ‚Ä¢ Intervalle max: 32.00 heures\n",
      "   ‚Ä¢ Intervalle moyen: 4.01 heures\n",
      "\n",
      "üìä V√©rification des indicateurs requis...\n",
      "   ‚Ä¢ Colonnes disponibles: 35\n",
      "   ‚Ä¢ Indicateurs requis: 11\n",
      "   ‚Ä¢ Indicateurs manquants: 0\n",
      "   ‚úÖ Tous les indicateurs requis sont pr√©sents\n",
      "\n",
      "üß™ Test de calcul de signaux sur √©chantillon...\n",
      "   ‚úÖ Test r√©ussi - 1 signaux d'achat, 11 signaux de vente sur √©chantillon\n",
      "\n",
      "üìã R√âSUM√â DU DIAGNOSTIC:\n",
      "   ‚Ä¢ Continuit√© temporelle: ‚ùå\n",
      "   ‚Ä¢ Indicateurs complets: ‚úÖ\n",
      "\n",
      "üí° RECOMMANDATIONS:\n",
      "   ‚ö†Ô∏è Intervalles temporels irr√©guliers d√©tect√©s - v√©rifier la qualit√© des donn√©es\n",
      "   ‚úÖ Test r√©ussi - 1 signaux d'achat, 11 signaux de vente sur √©chantillon\n",
      "\n",
      "üìã R√âSUM√â DU DIAGNOSTIC:\n",
      "   ‚Ä¢ Continuit√© temporelle: ‚ùå\n",
      "   ‚Ä¢ Indicateurs complets: ‚úÖ\n",
      "\n",
      "üí° RECOMMANDATIONS:\n",
      "   ‚ö†Ô∏è Intervalles temporels irr√©guliers d√©tect√©s - v√©rifier la qualit√© des donn√©es\n"
     ]
    }
   ],
   "source": [
    "def run_coherence_diagnostic() -> Dict:\n",
    "    \"\"\"Diagnostic de coh√©rence des donn√©es avant backtesting\"\"\"\n",
    "    \n",
    "    print(\"üîç DIAGNOSTIC DE COHERENCE DES DONN√âES\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if not data_loader.con:\n",
    "        data_loader.setup_connection()\n",
    "    \n",
    "    diagnostic_results = {\n",
    "        'temporal_continuity': True,\n",
    "        'indicator_completeness': True,\n",
    "        'data_gaps': [],\n",
    "        'missing_indicators': [],\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 1. V√©rifier la continuit√© temporelle\n",
    "        print(\"üìÖ V√©rification de la continuit√© temporelle...\")\n",
    "        \n",
    "        temporal_query = f\"\"\"\n",
    "            WITH time_diffs AS (\n",
    "                SELECT \n",
    "                    datetime,\n",
    "                    LAG(datetime) OVER (ORDER BY datetime) as prev_datetime,\n",
    "                    datetime - LAG(datetime) OVER (ORDER BY datetime) as time_diff_interval,\n",
    "                    EXTRACT(EPOCH FROM (datetime - LAG(datetime) OVER (ORDER BY datetime))) / 3600.0 as time_diff_hours\n",
    "                FROM read_parquet('{config.feature_store_path}')\n",
    "                WHERE symbol = '{config.symbol}'\n",
    "                ORDER BY datetime\n",
    "                LIMIT 10000  -- √âchantillon pour diagnostic\n",
    "            )\n",
    "            SELECT \n",
    "                COUNT(*) as total_rows,\n",
    "                COUNT(DISTINCT time_diff_hours) as unique_intervals,\n",
    "                MIN(time_diff_hours) as min_interval_hours,\n",
    "                MAX(time_diff_hours) as max_interval_hours,\n",
    "                AVG(time_diff_hours) as avg_interval_hours\n",
    "            FROM time_diffs \n",
    "            WHERE time_diff_hours IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        result = data_loader.con.execute(temporal_query).fetchone()\n",
    "        total_rows, unique_intervals, min_interval_hours, max_interval_hours, avg_interval_hours = result\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Lignes analys√©es: {total_rows:,}\")\n",
    "        print(f\"   ‚Ä¢ Intervalles uniques: {unique_intervals}\")\n",
    "        print(f\"   ‚Ä¢ Intervalle min: {min_interval_hours:.2f} heures\")\n",
    "        print(f\"   ‚Ä¢ Intervalle max: {max_interval_hours:.2f} heures\")\n",
    "        print(f\"   ‚Ä¢ Intervalle moyen: {avg_interval_hours:.2f} heures\")\n",
    "        \n",
    "        if unique_intervals > 2:  # Tol√©rance pour quelques variations\n",
    "            diagnostic_results['temporal_continuity'] = False\n",
    "            diagnostic_results['recommendations'].append(\n",
    "                \"‚ö†Ô∏è Intervalles temporels irr√©guliers d√©tect√©s - v√©rifier la qualit√© des donn√©es\"\n",
    "            )\n",
    "        \n",
    "        # 2. V√©rifier la pr√©sence des indicateurs requis\n",
    "        print(\"\\nüìä V√©rification des indicateurs requis...\")\n",
    "        \n",
    "        schema_query = f\"\"\"\n",
    "            DESCRIBE SELECT * FROM read_parquet('{config.feature_store_path}') LIMIT 1\n",
    "        \"\"\"\n",
    "        \n",
    "        available_columns = [row[0] for row in data_loader.con.execute(schema_query).fetchall()]\n",
    "        required_indicators = list(config.get_indicator_columns().values())\n",
    "        \n",
    "        missing_indicators = [ind for ind in required_indicators if ind not in available_columns]\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Colonnes disponibles: {len(available_columns)}\")\n",
    "        print(f\"   ‚Ä¢ Indicateurs requis: {len(required_indicators)}\")\n",
    "        print(f\"   ‚Ä¢ Indicateurs manquants: {len(missing_indicators)}\")\n",
    "        \n",
    "        if missing_indicators:\n",
    "            diagnostic_results['indicator_completeness'] = False\n",
    "            diagnostic_results['missing_indicators'] = missing_indicators\n",
    "            print(f\"   ‚ùå Indicateurs manquants: {missing_indicators}\")\n",
    "            diagnostic_results['recommendations'].append(\n",
    "                f\"‚ùå Recalculer ou ajouter les indicateurs manquants: {missing_indicators}\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"   ‚úÖ Tous les indicateurs requis sont pr√©sents\")\n",
    "        \n",
    "        # 3. Tester un √©chantillon de calcul de signaux\n",
    "        print(\"\\nüß™ Test de calcul de signaux sur √©chantillon...\")\n",
    "        \n",
    "        sample_query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM read_parquet('{config.feature_store_path}')\n",
    "            WHERE symbol = '{config.symbol}'\n",
    "            ORDER BY datetime\n",
    "            LIMIT 1000\n",
    "        \"\"\"\n",
    "        \n",
    "        sample_result = data_loader.con.execute(sample_query).arrow()\n",
    "        sample_df = pl.from_arrow(sample_result)\n",
    "        \n",
    "        if len(sample_df) > 0:\n",
    "            try:\n",
    "                # Test du calcul de signaux\n",
    "                test_signals = backtester.compute_strategy_signals(sample_df)\n",
    "                \n",
    "                buy_count = test_signals.select(pl.col(\"buy_signal\").sum()).item()\n",
    "                sell_count = test_signals.select(pl.col(\"sell_signal\").sum()).item()\n",
    "                \n",
    "                print(f\"   ‚úÖ Test r√©ussi - {buy_count} signaux d'achat, {sell_count} signaux de vente sur √©chantillon\")\n",
    "                \n",
    "                # V√©rifier les signaux sur la premi√®re ligne (probl√®me de shift)\n",
    "                first_row_signals = test_signals.head(1)\n",
    "                first_buy = first_row_signals.select(pl.col(\"buy_signal\")).item()\n",
    "                first_sell = first_row_signals.select(pl.col(\"sell_signal\")).item()\n",
    "                \n",
    "                if first_buy or first_sell:\n",
    "                    diagnostic_results['recommendations'].append(\n",
    "                        \"‚ö†Ô∏è Signaux d√©tect√©s sur premi√®re ligne - v√©rifier la logique de shift()\"\n",
    "                    )\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Erreur dans le calcul de signaux: {e}\")\n",
    "                diagnostic_results['recommendations'].append(f\"‚ùå Erreur calcul signaux: {e}\")\n",
    "        \n",
    "        # 4. R√©sum√© du diagnostic\n",
    "        print(f\"\\nüìã R√âSUM√â DU DIAGNOSTIC:\")\n",
    "        print(f\"   ‚Ä¢ Continuit√© temporelle: {'‚úÖ' if diagnostic_results['temporal_continuity'] else '‚ùå'}\")\n",
    "        print(f\"   ‚Ä¢ Indicateurs complets: {'‚úÖ' if diagnostic_results['indicator_completeness'] else '‚ùå'}\")\n",
    "        \n",
    "        if diagnostic_results['recommendations']:\n",
    "            print(f\"\\nüí° RECOMMANDATIONS:\")\n",
    "            for rec in diagnostic_results['recommendations']:\n",
    "                print(f\"   {rec}\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ TOUTES LES V√âRIFICATIONS PASS√âES - PR√äT POUR LE BACKTESTING\")\n",
    "        \n",
    "        return diagnostic_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du diagnostic: {e}\")\n",
    "        diagnostic_results['recommendations'].append(f\"‚ùå Erreur diagnostic: {e}\")\n",
    "        return diagnostic_results\n",
    "\n",
    "# Ex√©cution du diagnostic\n",
    "diagnostic = run_coherence_diagnostic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4988618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DIAGNOSTIC AVANC√â DES SIGNAUX\n",
      "===================================\n",
      "üìä √âchantillon analys√©: 5,000 lignes\n",
      "\n",
      "üìà COLONNES D'INDICATEURS:\n",
      "   ‚Ä¢ EMA Fast (ema_12): ‚úÖ\n",
      "   ‚Ä¢ EMA Slow (ema_26): ‚úÖ\n",
      "   ‚Ä¢ RSI (rsi_14): ‚úÖ\n",
      "   ‚Ä¢ SuperTrend Dir (supertrend_dir_10_3.0): ‚úÖ\n",
      "\n",
      "üìä ANALYSE DES CONDITIONS (sur 5,000 lignes):\n",
      "   ‚Ä¢ EMA Fast > Slow: 2,575 (51.5%)\n",
      "   ‚Ä¢ EMA Bullish Cross: 73 (1.5%)\n",
      "   ‚Ä¢ RSI Neutral (45-55): 1,392 (27.8%)\n",
      "   ‚Ä¢ SuperTrend Bullish: 2,531 (50.6%)\n",
      "   ‚Ä¢ üéØ SIGNAUX D'ACHAT FINAUX: 9\n",
      "\n",
      "üìä STATISTIQUES RSI:\n",
      "   ‚Ä¢ Min: 7.7\n",
      "   ‚Ä¢ Q25: 41.4\n",
      "   ‚Ä¢ Moyenne: nan\n",
      "   ‚Ä¢ Q75: 60.3\n",
      "   ‚Ä¢ Max: 95.0\n",
      "   ‚Ä¢ Plage neutre configur√©e: 45-55\n",
      "\n",
      "üí° SUGGESTIONS D'AM√âLIORATION:\n",
      "\n",
      "===================================\n",
      "üìä √âchantillon analys√©: 5,000 lignes\n",
      "\n",
      "üìà COLONNES D'INDICATEURS:\n",
      "   ‚Ä¢ EMA Fast (ema_12): ‚úÖ\n",
      "   ‚Ä¢ EMA Slow (ema_26): ‚úÖ\n",
      "   ‚Ä¢ RSI (rsi_14): ‚úÖ\n",
      "   ‚Ä¢ SuperTrend Dir (supertrend_dir_10_3.0): ‚úÖ\n",
      "\n",
      "üìä ANALYSE DES CONDITIONS (sur 5,000 lignes):\n",
      "   ‚Ä¢ EMA Fast > Slow: 2,575 (51.5%)\n",
      "   ‚Ä¢ EMA Bullish Cross: 73 (1.5%)\n",
      "   ‚Ä¢ RSI Neutral (45-55): 1,392 (27.8%)\n",
      "   ‚Ä¢ SuperTrend Bullish: 2,531 (50.6%)\n",
      "   ‚Ä¢ üéØ SIGNAUX D'ACHAT FINAUX: 9\n",
      "\n",
      "üìä STATISTIQUES RSI:\n",
      "   ‚Ä¢ Min: 7.7\n",
      "   ‚Ä¢ Q25: 41.4\n",
      "   ‚Ä¢ Moyenne: nan\n",
      "   ‚Ä¢ Q75: 60.3\n",
      "   ‚Ä¢ Max: 95.0\n",
      "   ‚Ä¢ Plage neutre configur√©e: 45-55\n",
      "\n",
      "üí° SUGGESTIONS D'AM√âLIORATION:\n",
      "\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# üîç DIAGNOSTIC AVANC√â : Pourquoi aucun signal n'est g√©n√©r√© ?\n",
    "print(\"üîç DIAGNOSTIC AVANC√â DES SIGNAUX\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Charger un √©chantillon plus large pour diagnostic\n",
    "sample_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_parquet('{config.feature_store_path}')\n",
    "    WHERE symbol = '{config.symbol}'\n",
    "    ORDER BY datetime\n",
    "    LIMIT 5000\n",
    "\"\"\"\n",
    "\n",
    "sample_result = data_loader.con.execute(sample_query).arrow()\n",
    "sample_df = pl.from_arrow(sample_result)\n",
    "\n",
    "print(f\"üìä √âchantillon analys√©: {len(sample_df):,} lignes\")\n",
    "\n",
    "if len(sample_df) > 0:\n",
    "    # Calculer les signaux avec diagnostics d√©taill√©s\n",
    "    ema_fast_col = config.get_indicator_columns()[\"ema_fast\"]\n",
    "    ema_slow_col = config.get_indicator_columns()[\"ema_slow\"]\n",
    "    rsi_col = config.get_indicator_columns()[\"rsi_14\"]\n",
    "    supertrend_dir_col = config.get_indicator_columns()[\"supertrend_dir\"]\n",
    "    \n",
    "    # V√©rifier les colonnes d'indicateurs\n",
    "    print(f\"\\nüìà COLONNES D'INDICATEURS:\")\n",
    "    print(f\"   ‚Ä¢ EMA Fast ({ema_fast_col}): {'‚úÖ' if ema_fast_col in sample_df.columns else '‚ùå MANQUANT'}\")\n",
    "    print(f\"   ‚Ä¢ EMA Slow ({ema_slow_col}): {'‚úÖ' if ema_slow_col in sample_df.columns else '‚ùå MANQUANT'}\")\n",
    "    print(f\"   ‚Ä¢ RSI ({rsi_col}): {'‚úÖ' if rsi_col in sample_df.columns else '‚ùå MANQUANT'}\")\n",
    "    print(f\"   ‚Ä¢ SuperTrend Dir ({supertrend_dir_col}): {'‚úÖ' if supertrend_dir_col in sample_df.columns else '‚ùå MANQUANT'}\")\n",
    "    \n",
    "    # Analyser les conditions individuelles\n",
    "    if all(col in sample_df.columns for col in [ema_fast_col, ema_slow_col, rsi_col, supertrend_dir_col]):\n",
    "        # Calculer les conditions individuelles\n",
    "        analysis_df = sample_df.with_columns([\n",
    "            # Conditions EMA\n",
    "            (pl.col(ema_fast_col) > pl.col(ema_slow_col)).alias(\"ema_fast_above_slow\"),\n",
    "            (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)).alias(\"ema_was_below_or_equal\"),\n",
    "            \n",
    "            # Crossover EMA\n",
    "            ((pl.col(ema_fast_col) > pl.col(ema_slow_col)) & \n",
    "             (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)) &\n",
    "             (pl.int_range(pl.len()) > 0)).alias(\"ema_bullish_cross\"),\n",
    "            \n",
    "            # Conditions RSI\n",
    "            ((pl.col(rsi_col) >= config.rsi_neutral_low) & \n",
    "             (pl.col(rsi_col) <= config.rsi_neutral_high)).alias(\"rsi_neutral\"),\n",
    "            \n",
    "            # Conditions SuperTrend\n",
    "            (pl.col(supertrend_dir_col) == 1).alias(\"supertrend_bullish\"),\n",
    "            \n",
    "            # Signal final\n",
    "            ((pl.col(ema_fast_col) > pl.col(ema_slow_col)) & \n",
    "             (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)) &\n",
    "             (pl.int_range(pl.len()) > 0) &\n",
    "             (pl.col(rsi_col) >= config.rsi_neutral_low) & \n",
    "             (pl.col(rsi_col) <= config.rsi_neutral_high) &\n",
    "             (pl.col(supertrend_dir_col) == 1)).alias(\"buy_signal\")\n",
    "        ])\n",
    "        \n",
    "        # Statistiques des conditions\n",
    "        print(f\"\\nüìä ANALYSE DES CONDITIONS (sur {len(analysis_df):,} lignes):\")\n",
    "        \n",
    "        ema_fast_above = analysis_df.select(pl.col(\"ema_fast_above_slow\").sum()).item()\n",
    "        ema_crossover = analysis_df.select(pl.col(\"ema_bullish_cross\").sum()).item()\n",
    "        rsi_neutral_count = analysis_df.select(pl.col(\"rsi_neutral\").sum()).item()\n",
    "        supertrend_bull = analysis_df.select(pl.col(\"supertrend_bullish\").sum()).item()\n",
    "        buy_signals = analysis_df.select(pl.col(\"buy_signal\").sum()).item()\n",
    "        \n",
    "        print(f\"   ‚Ä¢ EMA Fast > Slow: {ema_fast_above:,} ({ema_fast_above/len(analysis_df)*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ EMA Bullish Cross: {ema_crossover:,} ({ema_crossover/len(analysis_df)*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ RSI Neutral ({config.rsi_neutral_low}-{config.rsi_neutral_high}): {rsi_neutral_count:,} ({rsi_neutral_count/len(analysis_df)*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ SuperTrend Bullish: {supertrend_bull:,} ({supertrend_bull/len(analysis_df)*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ üéØ SIGNAUX D'ACHAT FINAUX: {buy_signals:,}\")\n",
    "        \n",
    "        # Analyser les valeurs RSI pour comprendre le probl√®me\n",
    "        rsi_stats = sample_df.select([\n",
    "            pl.col(rsi_col).min().alias(\"rsi_min\"),\n",
    "            pl.col(rsi_col).max().alias(\"rsi_max\"),\n",
    "            pl.col(rsi_col).mean().alias(\"rsi_mean\"),\n",
    "            pl.col(rsi_col).quantile(0.25).alias(\"rsi_q25\"),\n",
    "            pl.col(rsi_col).quantile(0.75).alias(\"rsi_q75\")\n",
    "        ]).to_dicts()[0]\n",
    "        \n",
    "        print(f\"\\nüìä STATISTIQUES RSI:\")\n",
    "        print(f\"   ‚Ä¢ Min: {rsi_stats['rsi_min']:.1f}\")\n",
    "        print(f\"   ‚Ä¢ Q25: {rsi_stats['rsi_q25']:.1f}\")\n",
    "        print(f\"   ‚Ä¢ Moyenne: {rsi_stats['rsi_mean']:.1f}\")\n",
    "        print(f\"   ‚Ä¢ Q75: {rsi_stats['rsi_q75']:.1f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {rsi_stats['rsi_max']:.1f}\")\n",
    "        print(f\"   ‚Ä¢ Plage neutre configur√©e: {config.rsi_neutral_low}-{config.rsi_neutral_high}\")\n",
    "        \n",
    "        if rsi_stats['rsi_mean'] < config.rsi_neutral_low or rsi_stats['rsi_mean'] > config.rsi_neutral_high:\n",
    "            print(f\"   ‚ö†Ô∏è La moyenne RSI ({rsi_stats['rsi_mean']:.1f}) est en dehors de la plage neutre!\")\n",
    "            \n",
    "        # Suggestions d'am√©lioration\n",
    "        print(f\"\\nüí° SUGGESTIONS D'AM√âLIORATION:\")\n",
    "        if ema_crossover == 0:\n",
    "            print(\"   ‚Ä¢ Aucun croisement EMA d√©tect√© - v√©rifier les p√©riodes EMA\")\n",
    "        if rsi_neutral_count < len(analysis_df) * 0.1:\n",
    "            print(f\"   ‚Ä¢ Plage RSI trop restrictive - essayer {config.rsi_neutral_low-10}-{config.rsi_neutral_high+10}\")\n",
    "        if supertrend_bull < len(analysis_df) * 0.3:\n",
    "            print(\"   ‚Ä¢ SuperTrend rarement bullish - ajuster les param√®tres\")\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ùå Colonnes d'indicateurs manquantes - impossible d'analyser les conditions\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f794330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DIAGNOSTIC SUPERTREND\n",
      "=========================\n",
      "üìä Valeurs uniques de supertrend_dir_10_3.0:\n",
      "   ‚Ä¢ Valeur -1.0: 2,459 occurrences (49.2%)\n",
      "   ‚Ä¢ Valeur 1.0: 2,531 occurrences (50.6%)\n",
      "   ‚Ä¢ Valeur nan: 10 occurrences (0.2%)\n",
      "   ‚Ä¢ Valeurs nulles: 0\n",
      "\n",
      "üìà Derni√®res valeurs SuperTrend:\n",
      "   ‚Ä¢ 2019-11-29 12:00:00: 1.0\n",
      "   ‚Ä¢ 2019-11-29 16:00:00: 1.0\n",
      "   ‚Ä¢ 2019-11-29 20:00:00: 1.0\n",
      "   ‚Ä¢ 2019-11-30 00:00:00: 1.0\n",
      "   ‚Ä¢ 2019-11-30 04:00:00: 1.0\n",
      "   ‚Ä¢ 2019-11-30 08:00:00: 1.0\n",
      "   ‚Ä¢ 2019-11-30 12:00:00: 1.0\n",
      "   ‚Ä¢ 2019-11-30 16:00:00: 1.0\n",
      "   ‚Ä¢ 2019-11-30 20:00:00: 1.0\n",
      "   ‚Ä¢ 2019-12-01 00:00:00: 1.0\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "# üîç DIAGNOSTIC SUPERTREND SP√âCIFIQUE\n",
    "print(\"üîç DIAGNOSTIC SUPERTREND\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "supertrend_dir_col = config.get_indicator_columns()[\"supertrend_dir\"]\n",
    "\n",
    "# Analyser les valeurs SuperTrend de fa√ßon simplifi√©e\n",
    "unique_values = sample_df.select(pl.col(supertrend_dir_col).unique()).to_series().to_list()\n",
    "print(f\"üìä Valeurs uniques de {supertrend_dir_col}:\")\n",
    "for value in unique_values:\n",
    "    if value is not None:\n",
    "        count = sample_df.filter(pl.col(supertrend_dir_col) == value).height\n",
    "        percentage = (count / len(sample_df)) * 100\n",
    "        print(f\"   ‚Ä¢ Valeur {value}: {count:,} occurrences ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        null_count = sample_df.filter(pl.col(supertrend_dir_col).is_null()).height\n",
    "        print(f\"   ‚Ä¢ Valeur NULL: {null_count:,} occurrences ({null_count/len(sample_df)*100:.1f}%)\")\n",
    "\n",
    "# V√©rifier s'il y a des NaN/null\n",
    "null_count = sample_df.select(pl.col(supertrend_dir_col).is_null().sum()).item()\n",
    "print(f\"   ‚Ä¢ Valeurs nulles: {null_count:,}\")\n",
    "\n",
    "# V√©rifier les derni√®res valeurs pour tendance r√©cente\n",
    "recent_supertrend = sample_df.tail(100).select([\n",
    "    pl.col('datetime'),\n",
    "    pl.col(supertrend_dir_col)\n",
    "]).tail(10)\n",
    "\n",
    "print(f\"\\nüìà Derni√®res valeurs SuperTrend:\")\n",
    "for row in recent_supertrend.to_dicts():\n",
    "    print(f\"   ‚Ä¢ {row['datetime']}: {row[supertrend_dir_col]}\")\n",
    "\n",
    "print(\"=\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b81e8f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ TEST STRAT√âGIE SIMPLIFI√âE\n",
      "==============================\n",
      "üìä R√âSULTATS STRAT√âGIE SIMPLIFI√âE:\n",
      "   ‚Ä¢ Croisements EMA bullish: 73\n",
      "   ‚Ä¢ RSI OK (30-70): 4,194 (83.9%)\n",
      "   ‚Ä¢ üéØ Signaux d'ACHAT: 69\n",
      "   ‚Ä¢ üéØ Signaux de VENTE: 72\n",
      "\n",
      "‚úÖ SUCC√àS ! La strat√©gie simplifi√©e g√©n√®re des signaux.\n",
      "üí° Le probl√®me vient bien du SuperTrend qui contient uniquement des NaN\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# üöÄ STRAT√âGIE SIMPLIFI√âE SANS SUPERTREND (TEST)\n",
    "print(\"üöÄ TEST STRAT√âGIE SIMPLIFI√âE\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Modifier temporairement la strat√©gie pour ignorer SuperTrend\n",
    "def compute_simple_strategy_signals(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Version simplifi√©e sans SuperTrend pour test\"\"\"\n",
    "    \n",
    "    ema_fast_col = config.get_indicator_columns()[\"ema_fast\"]\n",
    "    ema_slow_col = config.get_indicator_columns()[\"ema_slow\"]\n",
    "    rsi_col = config.get_indicator_columns()[\"rsi_14\"]\n",
    "    \n",
    "    # Calcul des signaux simplifi√©s (sans SuperTrend)\n",
    "    signals_df = df.with_columns([\n",
    "        # === CONDITIONS EMA ===\n",
    "        (pl.col(ema_fast_col) > pl.col(ema_slow_col)).alias(\"ema_fast_above_slow\"),\n",
    "        (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)).alias(\"ema_was_below_or_equal\"),\n",
    "        \n",
    "        # Crossover EMA avec validation de continuit√©\n",
    "        ((pl.col(ema_fast_col) > pl.col(ema_slow_col)) & \n",
    "         (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)) &\n",
    "         (pl.int_range(pl.len()) > 0)).alias(\"ema_bullish_cross\"),\n",
    "        \n",
    "        # === CONDITIONS RSI (assouplies) ===\n",
    "        ((pl.col(rsi_col) >= 30) & (pl.col(rsi_col) <= 70)).alias(\"rsi_ok\"),\n",
    "        \n",
    "        # === SIGNAL SIMPLIFI√â ===\n",
    "        # Achat : Croisement EMA bullish + RSI OK\n",
    "        ((pl.col(ema_fast_col) > pl.col(ema_slow_col)) & \n",
    "         (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)) &\n",
    "         (pl.int_range(pl.len()) > 0) &\n",
    "         (pl.col(rsi_col) >= 30) & (pl.col(rsi_col) <= 70)).alias(\"buy_signal\"),\n",
    "        \n",
    "        # Vente : Croisement EMA bearish\n",
    "        ((pl.col(ema_fast_col) <= pl.col(ema_slow_col)) & \n",
    "         (pl.col(ema_fast_col).shift(1) > pl.col(ema_slow_col).shift(1)) &\n",
    "         (pl.int_range(pl.len()) > 0)).alias(\"sell_signal\")\n",
    "    ])\n",
    "    \n",
    "    return signals_df\n",
    "\n",
    "# Test sur √©chantillon\n",
    "test_simple = compute_simple_strategy_signals(sample_df)\n",
    "\n",
    "buy_count_simple = test_simple.select(pl.col(\"buy_signal\").sum()).item()\n",
    "sell_count_simple = test_simple.select(pl.col(\"sell_signal\").sum()).item()\n",
    "ema_cross_count = test_simple.select(pl.col(\"ema_bullish_cross\").sum()).item()\n",
    "rsi_ok_count = test_simple.select(pl.col(\"rsi_ok\").sum()).item()\n",
    "\n",
    "print(f\"üìä R√âSULTATS STRAT√âGIE SIMPLIFI√âE:\")\n",
    "print(f\"   ‚Ä¢ Croisements EMA bullish: {ema_cross_count:,}\")\n",
    "print(f\"   ‚Ä¢ RSI OK (30-70): {rsi_ok_count:,} ({rsi_ok_count/len(sample_df)*100:.1f}%)\")  \n",
    "print(f\"   ‚Ä¢ üéØ Signaux d'ACHAT: {buy_count_simple:,}\")\n",
    "print(f\"   ‚Ä¢ üéØ Signaux de VENTE: {sell_count_simple:,}\")\n",
    "\n",
    "if buy_count_simple > 0:\n",
    "    print(f\"\\n‚úÖ SUCC√àS ! La strat√©gie simplifi√©e g√©n√®re des signaux.\")\n",
    "    print(f\"üí° Le probl√®me vient bien du SuperTrend qui contient uniquement des NaN\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è M√™me la strat√©gie simplifi√©e ne g√©n√®re pas de signaux.\")\n",
    "    print(f\"üí° V√©rifier les donn√©es EMA et RSI\")\n",
    "\n",
    "print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "255aca40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√âMARRAGE DU BACKTESTING CHUNKED AVEC VALIDATION\n",
      "============================================================\n",
      "üìä Total √† traiter: 5,844 lignes\n",
      "üîÑ Chunks estim√©s: 1\n",
      "üõ°Ô∏è Buffer de contexte: 100 lignes\n",
      "‚è±Ô∏è D√©but: 00:40:37\n",
      "\n",
      "[  1/1] Chunk 0-5,844 |  5844 lignes | üìà 10 achats | üìâ 66 ventes | ‚ö°  13145 l/s | ‚úÖ no-ctx | üîÑ   1 trades\n",
      "\n",
      "============================================================\n",
      "‚úÖ BACKTESTING CHUNKED TERMIN√â AVEC VALIDATION\n",
      "============================================================\n",
      "üìä Lignes trait√©es: 5,844\n",
      "üìà Total signaux achat: 10\n",
      "üìâ Total signaux vente: 66\n",
      "üîÑ Total trades compl√©t√©s: 1\n",
      "‚è±Ô∏è Temps total: 0.4s\n",
      "‚ö° Performance: 13,140 lignes/sec\n",
      "\n",
      "üõ°Ô∏è RAPPORT DE VALIDATION:\n",
      "   ‚Ä¢ Chunks trait√©s: 1\n",
      "   ‚Ä¢ Probl√®mes de continuit√©: 0\n",
      "   ‚Ä¢ Warnings total: 0\n",
      "   ‚úÖ Coh√©rence parfaite - tous les signaux sont fiables\n",
      "|  5844 lignes | üìà 10 achats | üìâ 66 ventes | ‚ö°  13145 l/s | ‚úÖ no-ctx | üîÑ   1 trades\n",
      "\n",
      "============================================================\n",
      "‚úÖ BACKTESTING CHUNKED TERMIN√â AVEC VALIDATION\n",
      "============================================================\n",
      "üìä Lignes trait√©es: 5,844\n",
      "üìà Total signaux achat: 10\n",
      "üìâ Total signaux vente: 66\n",
      "üîÑ Total trades compl√©t√©s: 1\n",
      "‚è±Ô∏è Temps total: 0.4s\n",
      "‚ö° Performance: 13,140 lignes/sec\n",
      "\n",
      "üõ°Ô∏è RAPPORT DE VALIDATION:\n",
      "   ‚Ä¢ Chunks trait√©s: 1\n",
      "   ‚Ä¢ Probl√®mes de continuit√©: 0\n",
      "   ‚Ä¢ Warnings total: 0\n",
      "   ‚úÖ Coh√©rence parfaite - tous les signaux sont fiables\n"
     ]
    }
   ],
   "source": [
    "def run_chunked_backtest() -> List[Dict]:\n",
    "    \"\"\"Ex√©cute le backtesting par chunks sur toutes les donn√©es avec validation de coh√©rence\"\"\"\n",
    "    \n",
    "    print(\"üöÄ D√âMARRAGE DU BACKTESTING CHUNKED AVEC VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # R√©cup√©rer le nombre total de lignes\n",
    "    if not data_summary:\n",
    "        print(\"‚ùå Pas d'informations sur les donn√©es\")\n",
    "        return []\n",
    "    \n",
    "    total_rows = data_summary['total_rows']\n",
    "    estimated_chunks = (total_rows // config.chunk_size) + 1\n",
    "    \n",
    "    print(f\"üìä Total √† traiter: {total_rows:,} lignes\")\n",
    "    print(f\"üîÑ Chunks estim√©s: {estimated_chunks:,}\")\n",
    "    print(f\"üõ°Ô∏è Buffer de contexte: {backtester.state['context_buffer_size']} lignes\")\n",
    "    print(f\"‚è±Ô∏è D√©but: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print()\n",
    "    \n",
    "    all_results = []\n",
    "    start_time = datetime.now()\n",
    "    total_warnings = 0\n",
    "    continuity_issues = 0\n",
    "    \n",
    "    # Traitement chunk par chunk\n",
    "    for offset in range(0, total_rows, config.chunk_size):\n",
    "        chunk_num = (offset // config.chunk_size) + 1\n",
    "        current_chunk_size = min(config.chunk_size, total_rows - offset)\n",
    "        \n",
    "        print(f\"[{chunk_num:>3}/{estimated_chunks}] \", end=\"\")\n",
    "        print(f\"Chunk {offset:,}-{offset + current_chunk_size:,} \", end=\"\")\n",
    "        \n",
    "        try:\n",
    "            # Charger le chunk avec contexte\n",
    "            chunk_start = datetime.now()\n",
    "            chunk_df = backtester.load_chunk_with_context(offset, current_chunk_size)\n",
    "            \n",
    "            if len(chunk_df) == 0:\n",
    "                print(\"‚ö†Ô∏è Chunk vide - arr√™t\")\n",
    "                break\n",
    "            \n",
    "            # Traiter le chunk\n",
    "            is_first = (offset == 0)\n",
    "            chunk_result = backtester.process_chunk(chunk_df, is_first)\n",
    "            \n",
    "            # V√©rifier les erreurs\n",
    "            if 'error' in chunk_result:\n",
    "                print(f\"‚ùå Erreur: {chunk_result['error']}\")\n",
    "                break\n",
    "            \n",
    "            # Calculer les m√©triques du chunk\n",
    "            chunk_time = (datetime.now() - chunk_start).total_seconds()\n",
    "            rows_per_sec = chunk_result['rows_processed'] / max(chunk_time, 0.001)\n",
    "            \n",
    "            # Affichage des m√©triques avec validation\n",
    "            continuity_status = \"‚úÖ\" if chunk_result.get('continuity_validated', False) else \"‚ö†Ô∏è\"\n",
    "            context_info = f\"ctx:{chunk_result.get('context_size', 0)}\" if chunk_result.get('context_size', 0) > 0 else \"no-ctx\"\n",
    "            \n",
    "            print(f\"| {chunk_result['rows_processed']:>5} lignes \", end=\"\")\n",
    "            print(f\"| üìà {chunk_result['buy_signals']:>2} achats \", end=\"\")\n",
    "            print(f\"| üìâ {chunk_result['sell_signals']:>2} ventes \", end=\"\")\n",
    "            print(f\"| ‚ö° {rows_per_sec:>6.0f} l/s \", end=\"\")\n",
    "            print(f\"| {continuity_status} {context_info} \", end=\"\")\n",
    "            print(f\"| üîÑ {backtester.state['total_trades']:>3} trades\")\n",
    "            \n",
    "            # Gestion des warnings\n",
    "            if 'warnings' in chunk_result and chunk_result['warnings']:\n",
    "                for warning in chunk_result['warnings']:\n",
    "                    print(f\"    ‚ö†Ô∏è {warning}\")\n",
    "                    total_warnings += 1\n",
    "            \n",
    "            if not chunk_result.get('continuity_validated', False):\n",
    "                continuity_issues += 1\n",
    "            \n",
    "            all_results.append(chunk_result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Statistiques finales avec validation\n",
    "    total_time = (datetime.now() - start_time).total_seconds()\n",
    "    total_processed = sum(r['rows_processed'] for r in all_results)\n",
    "    total_buy_signals = sum(r['buy_signals'] for r in all_results)\n",
    "    total_sell_signals = sum(r['sell_signals'] for r in all_results)\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚úÖ BACKTESTING CHUNKED TERMIN√â AVEC VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìä Lignes trait√©es: {total_processed:,}\")\n",
    "    print(f\"üìà Total signaux achat: {total_buy_signals:,}\")\n",
    "    print(f\"üìâ Total signaux vente: {total_sell_signals:,}\")\n",
    "    print(f\"üîÑ Total trades compl√©t√©s: {backtester.state['total_trades']:,}\")\n",
    "    print(f\"‚è±Ô∏è Temps total: {total_time:.1f}s\")\n",
    "    print(f\"‚ö° Performance: {total_processed/max(total_time, 0.001):,.0f} lignes/sec\")\n",
    "    \n",
    "    # Rapport de validation\n",
    "    print(f\"\\nüõ°Ô∏è RAPPORT DE VALIDATION:\")\n",
    "    print(f\"   ‚Ä¢ Chunks trait√©s: {len(all_results):,}\")\n",
    "    print(f\"   ‚Ä¢ Probl√®mes de continuit√©: {continuity_issues:,}\")\n",
    "    print(f\"   ‚Ä¢ Warnings total: {total_warnings:,}\")\n",
    "    \n",
    "    if continuity_issues == 0 and total_warnings == 0:\n",
    "        print(\"   ‚úÖ Coh√©rence parfaite - tous les signaux sont fiables\")\n",
    "    elif continuity_issues > 0:\n",
    "        print(f\"   ‚ö†Ô∏è {continuity_issues} chunks avec probl√®mes de continuit√©\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Ex√©cution du backtesting avec validation\n",
    "backtest_results = run_chunked_backtest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f2fc8",
   "metadata": {},
   "source": [
    "## 5. üíæ Sauvegarde des R√©sultats dans Table Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89a96af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ SAUVEGARDE DES R√âSULTATS\n",
      "==============================\n",
      "üìä Donn√©es combin√©es: 5,844 lignes\n",
      "üìÖ P√©riode: 2023-01-01 00:00:00 ‚Üí 2025-08-31 20:00:00\n",
      "‚úÖ Sauvegarde locale: /tmp/backtest_BTCUSDT_20251004_004046.parquet\n",
      "‚ö†Ô∏è MinIO non disponible: Invalid Error: Unexpected response while initializing S3 multipart upload\n",
      "üìÅ Utilisation sauvegarde locale: /tmp/backtest_BTCUSDT_20251004_004046.parquet\n",
      "‚úÖ R√©sultats sauvegard√©s: /tmp/backtest_BTCUSDT_20251004_004046.parquet\n",
      "üìÅ Taille: ~1.6 MB\n",
      "‚ùå Erreur lors de la sauvegarde: Invalid Error: Unexpected response while initializing S3 multipart upload\n"
     ]
    }
   ],
   "source": [
    "def save_results_to_test_table(results: List[Dict]) -> bool:\n",
    "    \"\"\"Sauvegarde les r√©sultats dans la table test pour analyse VectorBT\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ùå Pas de r√©sultats √† sauvegarder\")\n",
    "        return False\n",
    "    \n",
    "    print(\"üíæ SAUVEGARDE DES R√âSULTATS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    try:\n",
    "        # Combiner tous les DataFrames de r√©sultats\n",
    "        all_data = []\n",
    "        for result in results:\n",
    "            if 'data' in result and result['data'] is not None:\n",
    "                # Ajouter les m√©tadonn√©es du chunk\n",
    "                chunk_data = result['data'].with_columns([\n",
    "                    pl.lit(result['chunk_id']).alias('chunk_id'),\n",
    "                    pl.lit(datetime.now().isoformat()).alias('backtest_timestamp'),\n",
    "                    pl.lit(config.symbol).alias('symbol'),\n",
    "                    pl.lit(f\"{config.strategy_name if hasattr(config, 'strategy_name') else 'smart_momentum'}\").alias('strategy_name')\n",
    "                ])\n",
    "                all_data.append(chunk_data)\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"‚ùå Pas de donn√©es √† combiner\")\n",
    "            return False\n",
    "        \n",
    "        # Combiner toutes les donn√©es\n",
    "        final_df = pl.concat(all_data)\n",
    "        \n",
    "        print(f\"üìä Donn√©es combin√©es: {len(final_df):,} lignes\")\n",
    "        print(f\"üìÖ P√©riode: {final_df['datetime'].min()} ‚Üí {final_df['datetime'].max()}\")\n",
    "        \n",
    "        # G√©n√©rer le chemin de sortie avec timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = f\"{config.test_table_path}backtest_{config.symbol}_{timestamp}.parquet\"\n",
    "        \n",
    "        # Sauvegarder localement en attendant la correction MinIO\n",
    "        local_output_path = f\"/tmp/backtest_{config.symbol}_{timestamp}.parquet\"\n",
    "        \n",
    "        # Sauvegarder en local d'abord\n",
    "        final_df.write_parquet(local_output_path, compression='snappy')\n",
    "        print(f\"‚úÖ Sauvegarde locale: {local_output_path}\")\n",
    "        \n",
    "        # Essayer MinIO en option\n",
    "        try:\n",
    "            temp_table = \"temp_backtest_results\"\n",
    "            data_loader.con.register(temp_table, final_df.to_arrow())\n",
    "            \n",
    "            export_query = f\"\"\"\n",
    "                COPY (SELECT * FROM {temp_table})\n",
    "                TO '{output_path}'\n",
    "                (FORMAT PARQUET, COMPRESSION 'snappy')\n",
    "            \"\"\"\n",
    "            data_loader.con.execute(export_query)\n",
    "            print(f\"‚úÖ Sauvegarde MinIO: {output_path}\")\n",
    "        except Exception as minio_error:\n",
    "            print(f\"‚ö†Ô∏è MinIO non disponible: {minio_error}\")\n",
    "            print(f\"üìÅ Utilisation sauvegarde locale: {local_output_path}\")\n",
    "            output_path = local_output_path\n",
    "        \n",
    "        print(f\"‚úÖ R√©sultats sauvegard√©s: {output_path}\")\n",
    "        print(f\"üìÅ Taille: ~{final_df.estimated_size('mb'):.1f} MB\")\n",
    "        \n",
    "        # Sauvegarder √©galement les m√©tadonn√©es\n",
    "        metadata = {\n",
    "            'config': {\n",
    "                'symbol': config.symbol,\n",
    "                'chunk_size': config.chunk_size,\n",
    "                'start_date': config.start_date,\n",
    "                'end_date': config.end_date,\n",
    "                'initial_cash': config.initial_cash,\n",
    "                'fees': config.fees\n",
    "            },\n",
    "            'results': {\n",
    "                'total_rows': len(final_df),\n",
    "                'total_chunks': len(results),\n",
    "                'total_buy_signals': sum(r['buy_signals'] for r in results),\n",
    "                'total_sell_signals': sum(r['sell_signals'] for r in results),\n",
    "                'total_trades': backtester.state['total_trades']\n",
    "            },\n",
    "            'paths': {\n",
    "                'data_path': output_path,\n",
    "                'source_path': config.feature_store_path\n",
    "            },\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        metadata_path = f\"{config.test_table_path}metadata_{config.symbol}_{timestamp}.json\"\n",
    "        \n",
    "        # Sauvegarder les m√©tadonn√©es (m√©thode simplifi√©e)\n",
    "        metadata_df = pl.DataFrame([metadata])\n",
    "        data_loader.con.register(\"temp_metadata\", metadata_df.to_arrow())\n",
    "        data_loader.con.execute(f\"\"\"\n",
    "            COPY (SELECT * FROM temp_metadata)\n",
    "            TO '{metadata_path.replace('.json', '.parquet')}'\n",
    "            (FORMAT PARQUET)\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"üìã M√©tadonn√©es sauvegard√©es: {metadata_path.replace('.json', '.parquet')}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la sauvegarde: {e}\")\n",
    "        return False\n",
    "\n",
    "# Sauvegarder les r√©sultats\n",
    "save_success = save_results_to_test_table(backtest_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d8beb",
   "metadata": {},
   "source": [
    "## 6. üìà Validation et Analyse avec VectorBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cea9a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà ANALYSE VECTORBT\n",
      "====================\n",
      "üìä Donn√©es pour VectorBT: 5,844 lignes\n",
      "üìÖ P√©riode: 2023-01-01 00:00:00 ‚Üí 2025-08-31 20:00:00\n",
      "\n",
      "üìä R√âSULTATS VECTORBT:\n",
      "üí∞ Capital initial: $10,000.00\n",
      "üí∞ Capital final: $13,705.34\n",
      "üìà Rendement total: 37.05%\n",
      "üîÑ Nombre de trades: 10\n",
      "üíπ Trade moyen: 370.53\n",
      "üéØ Taux de r√©ussite: 40.0%\n",
      "üìâ Drawdown max: 0.00%\n",
      "üìâ Drawdown max: 0.00%\n",
      "\n",
      "üìä STATISTIQUES AVANC√âES:\n",
      "Start                         2023-01-01 00:00:00\n",
      "End                           2025-08-31 20:00:00\n",
      "Period                          974 days 00:00:00\n",
      "Start Value                               10000.0\n",
      "End Value                            13705.338196\n",
      "Total Return [%]                        37.053382\n",
      "Benchmark Return [%]                   554.727443\n",
      "Max Gross Exposure [%]                      100.0\n",
      "Total Fees Paid                        236.514826\n",
      "Max Drawdown [%]                        11.481144\n",
      "Max Drawdown Duration           319 days 16:00:00\n",
      "Total Trades                                   10\n",
      "Total Closed Trades                            10\n",
      "Total Open Trades                               0\n",
      "Open Trade PnL                                0.0\n",
      "Win Rate [%]                                 40.0\n",
      "Best Trade [%]                          18.690917\n",
      "Worst Trade [%]                         -3.870799\n",
      "Avg Winning Trade [%]                   12.991374\n",
      "Avg Losing Trade [%]                    -2.765411\n",
      "Avg Winning Trade Duration       14 days 20:00:00\n",
      "Avg Losing Trade Duration         2 days 18:00:00\n",
      "Profit Factor                            2.807848\n",
      "Expectancy                              370.53382\n",
      "Sharpe Ratio                             1.020706\n",
      "Calmar Ratio                             1.092036\n",
      "Omega Ratio                              1.273007\n",
      "Sortino Ratio                            1.452173\n",
      "dtype: object\n",
      "\n",
      "üìä STATISTIQUES AVANC√âES:\n",
      "Start                         2023-01-01 00:00:00\n",
      "End                           2025-08-31 20:00:00\n",
      "Period                          974 days 00:00:00\n",
      "Start Value                               10000.0\n",
      "End Value                            13705.338196\n",
      "Total Return [%]                        37.053382\n",
      "Benchmark Return [%]                   554.727443\n",
      "Max Gross Exposure [%]                      100.0\n",
      "Total Fees Paid                        236.514826\n",
      "Max Drawdown [%]                        11.481144\n",
      "Max Drawdown Duration           319 days 16:00:00\n",
      "Total Trades                                   10\n",
      "Total Closed Trades                            10\n",
      "Total Open Trades                               0\n",
      "Open Trade PnL                                0.0\n",
      "Win Rate [%]                                 40.0\n",
      "Best Trade [%]                          18.690917\n",
      "Worst Trade [%]                         -3.870799\n",
      "Avg Winning Trade [%]                   12.991374\n",
      "Avg Losing Trade [%]                    -2.765411\n",
      "Avg Winning Trade Duration       14 days 20:00:00\n",
      "Avg Losing Trade Duration         2 days 18:00:00\n",
      "Profit Factor                            2.807848\n",
      "Expectancy                              370.53382\n",
      "Sharpe Ratio                             1.020706\n",
      "Calmar Ratio                             1.092036\n",
      "Omega Ratio                              1.273007\n",
      "Sortino Ratio                            1.452173\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def analyze_with_vectorbt() -> Optional[vbt.Portfolio]:\n",
    "    \"\"\"Analyse des r√©sultats avec VectorBT\"\"\"\n",
    "    \n",
    "    if not backtest_results:\n",
    "        print(\"‚ùå Pas de r√©sultats √† analyser\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üìà ANALYSE VECTORBT\")\n",
    "    print(\"=\" * 20)\n",
    "    \n",
    "    try:\n",
    "        # Combiner toutes les donn√©es pour VectorBT\n",
    "        all_data = []\n",
    "        for result in backtest_results:\n",
    "            if 'data' in result and result['data'] is not None:\n",
    "                all_data.append(result['data'])\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"‚ùå Pas de donn√©es √† analyser\")\n",
    "            return None\n",
    "        \n",
    "        # Combiner et convertir en pandas pour VectorBT\n",
    "        combined_df = pl.concat(all_data)\n",
    "        df_pd = combined_df.to_pandas().set_index('datetime')\n",
    "        \n",
    "        print(f\"üìä Donn√©es pour VectorBT: {len(df_pd):,} lignes\")\n",
    "        print(f\"üìÖ P√©riode: {df_pd.index.min()} ‚Üí {df_pd.index.max()}\")\n",
    "        \n",
    "        # Cr√©er le portfolio VectorBT\n",
    "        portfolio = vbt.Portfolio.from_signals(\n",
    "            close=df_pd['close'],\n",
    "            entries=df_pd['buy_signal'],\n",
    "            exits=df_pd['sell_signal'],\n",
    "            init_cash=config.initial_cash,\n",
    "            fees=config.fees,\n",
    "            freq='4H'  # Ajuster selon vos donn√©es\n",
    "        )\n",
    "        \n",
    "        # Statistiques de base\n",
    "        print(f\"\\nüìä R√âSULTATS VECTORBT:\")\n",
    "        print(f\"üí∞ Capital initial: ${config.initial_cash:,.2f}\")\n",
    "        print(f\"üí∞ Capital final: ${portfolio.final_value():,.2f}\")\n",
    "        print(f\"üìà Rendement total: {(portfolio.final_value() / config.initial_cash - 1) * 100:.2f}%\")\n",
    "        print(f\"üîÑ Nombre de trades: {portfolio.trades.count()}\")\n",
    "        \n",
    "        if portfolio.trades.count() > 0:\n",
    "            print(f\"üíπ Trade moyen: {portfolio.trades.pnl.mean():.2f}\")\n",
    "            print(f\"üéØ Taux de r√©ussite: {portfolio.trades.win_rate() * 100:.1f}%\")\n",
    "            try:\n",
    "                print(f\"üìâ Drawdown max: {portfolio.drawdown().max() * 100:.2f}%\")\n",
    "            except:\n",
    "                print(f\"üìâ Drawdown max: N/A\")\n",
    "        \n",
    "        # Statistiques avanc√©es\n",
    "        stats = portfolio.stats()\n",
    "        print(f\"\\nüìä STATISTIQUES AVANC√âES:\")\n",
    "        print(stats)\n",
    "        \n",
    "        return portfolio\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de l'analyse VectorBT: {e}\")\n",
    "        return None\n",
    "\n",
    "# Analyse avec VectorBT\n",
    "portfolio = analyze_with_vectorbt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32d2c243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Impossible de g√©n√©rer les graphiques\n"
     ]
    }
   ],
   "source": [
    "# Visualisation des r√©sultats\n",
    "if portfolio is not None:\n",
    "    print(\"üé® G√©n√©ration des graphiques...\")\n",
    "    \n",
    "    # Graphique principal du portfolio\n",
    "    fig = portfolio.plot()\n",
    "    fig.show()\n",
    "    \n",
    "    # Graphique des trades\n",
    "    if portfolio.trades.count() > 0:\n",
    "        trades_fig = portfolio.trades.plot()\n",
    "        trades_fig.show()\n",
    "    \n",
    "    print(\"‚úÖ Graphiques g√©n√©r√©s avec succ√®s\")\n",
    "else:\n",
    "    print(\"‚ùå Impossible de g√©n√©rer les graphiques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260c15e",
   "metadata": {},
   "source": [
    "## 7. üìã R√©sum√© et Prochaines √âtapes\n",
    "\n",
    "### ‚úÖ Ce qui a √©t√© accompli\n",
    "- Backtesting chunked avec continuit√© des indicateurs\n",
    "- Traitement de gros volumes avec m√©moire constante\n",
    "- Sauvegarde des r√©sultats dans tables test\n",
    "- Analyse et validation avec VectorBT\n",
    "\n",
    "### üöÄ Prochaines √©tapes sugg√©r√©es\n",
    "1. **Optimisation des param√®tres** : Utiliser les r√©sultats pour ajuster la strat√©gie\n",
    "2. **Backtesting multi-timeframes** : Tester sur diff√©rentes p√©riodes\n",
    "3. **Strat√©gies avanc√©es** : Int√©grer de nouvelles conditions\n",
    "4. **Production** : D√©ployer la strat√©gie valid√©e\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a44fc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ R√âSUM√â DU BACKTESTING CHUNKED\n",
      "\n",
      "üìä Donn√©es trait√©es: 5,844 lignes en 1 chunks\n",
      "üìà Signaux g√©n√©r√©s: 76\n",
      "üîÑ Trades compl√©t√©s: 1\n",
      "üí∞ Performance: 37.05%\n",
      "üìä Sharpe Ratio: 1.02\n",
      "üíæ R√©sultats sauvegard√©s: ‚ùå\n",
      "üìà Analyse VectorBT: ‚úÖ\n",
      "\n",
      "‚úÖ Backtesting termin√© avec succ√®s !\n",
      "üöÄ Pr√™t pour l'optimisation et la production\n"
     ]
    }
   ],
   "source": [
    "# R√©sum√© final\n",
    "print(\"\" * 60)\n",
    "print(\"üéØ R√âSUM√â DU BACKTESTING CHUNKED\")\n",
    "print(\"\" * 60)\n",
    "\n",
    "if backtest_results:\n",
    "    total_processed = sum(r['rows_processed'] for r in backtest_results)\n",
    "    total_chunks = len(backtest_results)\n",
    "    total_signals = sum(r['buy_signals'] + r['sell_signals'] for r in backtest_results)\n",
    "    \n",
    "    print(f\"üìä Donn√©es trait√©es: {total_processed:,} lignes en {total_chunks} chunks\")\n",
    "    print(f\"üìà Signaux g√©n√©r√©s: {total_signals:,}\")\n",
    "    print(f\"üîÑ Trades compl√©t√©s: {backtester.state['total_trades']:,}\")\n",
    "    \n",
    "    if portfolio is not None:\n",
    "        print(f\"üí∞ Performance: {(portfolio.final_value() / config.initial_cash - 1) * 100:.2f}%\")\n",
    "        print(f\"üìä Sharpe Ratio: {portfolio.sharpe_ratio():.2f}\" if hasattr(portfolio, 'sharpe_ratio') else \"\")\n",
    "    \n",
    "    print(f\"üíæ R√©sultats sauvegard√©s: {'‚úÖ' if save_success else '‚ùå'}\")\n",
    "    print(f\"üìà Analyse VectorBT: {'‚úÖ' if portfolio is not None else '‚ùå'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Aucun r√©sultat g√©n√©r√©\")\n",
    "\n",
    "print(\"\\n‚úÖ Backtesting termin√© avec succ√®s !\")\n",
    "print(\"üöÄ Pr√™t pour l'optimisation et la production\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hermes-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
