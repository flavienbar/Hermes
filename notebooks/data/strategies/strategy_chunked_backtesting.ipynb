{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "226599df",
   "metadata": {},
   "source": [
    "# ðŸš€ Hermes - Backtesting Chunked de StratÃ©gies\n",
    "\n",
    "## Architecture\n",
    "Ce notebook suit l'architecture Medallion de Hermes :\n",
    "- **Source** : Hub Features Gold (indicateurs prÃ©-calculÃ©s)\n",
    "- **Traitement** : Chunks avec continuitÃ© pour gros volumes\n",
    "- **Analyse** : VectorBT pour validation des stratÃ©gies\n",
    "- **Sortie** : Table test pour rÃ©sultats intermÃ©diaires\n",
    "\n",
    "## Workflow\n",
    "1. **Configuration** : Imports et paramÃ¨tres\n",
    "2. **Connexion Sources** : Hub Features Gold (indicateurs prÃ©-calculÃ©s)\n",
    "3. **StratÃ©gie Chunked** : GÃ©nÃ©ration signaux par chunks\n",
    "4. **Validation VectorBT** : Analyse des performances\n",
    "\n",
    "**Important** : Tous les indicateurs doivent Ãªtre prÃ©-calculÃ©s dans le Hub Features Gold\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650fd35",
   "metadata": {},
   "source": [
    "## 1. ðŸ“¦ Configuration et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16ef6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports chargÃ©s\n",
      "ðŸ Python: 3.10.18\n",
      "ðŸ“Š Polars: 0.20.31\n",
      "ðŸ§® VectorBT: 0.25.5\n",
      "ðŸ¦† DuckDB: 0.9.2\n"
     ]
    }
   ],
   "source": [
    "# Imports essentiels\n",
    "import os\n",
    "import json\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import vectorbt as vbt\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"âœ… Imports chargÃ©s\")\n",
    "print(f\"ðŸ Python: {os.sys.version.split()[0]}\")\n",
    "print(f\"ðŸ“Š Polars: {pl.__version__}\")\n",
    "print(f\"ðŸ§® VectorBT: {vbt.__version__}\")\n",
    "print(f\"ðŸ¦† DuckDB: {duckdb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ea41c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Configuration initialisÃ©e\n",
      "ðŸ“Š Symbole: BTCUSDT\n",
      "ðŸ”„ Chunk size: 50,000 lignes\n",
      "ðŸ›¡ï¸ Buffer contexte: 100 lignes\n",
      "ðŸ“… PÃ©riode: 2023-01-01 â†’ fin\n",
      "ðŸ’° Capital initial: $10,000.00\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class BacktestConfig:\n",
    "    \"\"\"Configuration pour le backtesting chunked\"\"\"\n",
    "    \n",
    "    # Source de donnÃ©es\n",
    "    symbol: str = \"BTCUSDT\"\n",
    "    feature_store_path: str = \"s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/**/*.parquet\"\n",
    "    \n",
    "    # ParamÃ¨tres de chunking\n",
    "    chunk_size: int = 50_000  # Lignes par chunk\n",
    "    overlap_window: int = 100  # Lignes de contexte entre chunks\n",
    "    \n",
    "    # FenÃªtre temporelle (optionnel - None = tout l'historique)\n",
    "    start_date: Optional[str] = \"2023-01-01\"  # Format: \"YYYY-MM-DD\" ou None\n",
    "    end_date: Optional[str] = None\n",
    "    \n",
    "    # ParamÃ¨tres de stratÃ©gie par dÃ©faut\n",
    "    rsi_oversold: int = 30\n",
    "    rsi_neutral_low: int = 45\n",
    "    rsi_neutral_high: int = 55\n",
    "    ema_fast: int = 12\n",
    "    ema_slow: int = 26\n",
    "    supertrend_period: int = 10\n",
    "    supertrend_multiplier: float = 3.0\n",
    "\n",
    "    # Buffer contexte - calculÃ© automatiquement\n",
    "    min_context_buffer: int = 50  # Minimum de sÃ©curitÃ©\n",
    "\n",
    "    def get_required_context_size(self) -> int:\n",
    "        \"\"\"Calcule la taille de contexte requise selon les indicateurs\"\"\"\n",
    "        # Prendre le plus grand indicateur + marge de sÃ©curitÃ©\n",
    "        max_indicator_period = max([\n",
    "            self.ema_fast,\n",
    "            self.ema_slow, \n",
    "            self.supertrend_period,\n",
    "            14,  # RSI par dÃ©faut\n",
    "            20,  # Bollinger Bands par dÃ©faut\n",
    "            26   # MACD par dÃ©faut\n",
    "        ])\n",
    "\n",
    "        # Ajouter une marge de sÃ©curitÃ© (50% du plus grand indicateur)\n",
    "        safety_margin = int(max_indicator_period * 0.5)\n",
    "        required_size = max_indicator_period + safety_margin\n",
    "        \n",
    "        # S'assurer d'avoir au moins le minimum\n",
    "        return max(required_size, self.min_context_buffer, self.overlap_window)\n",
    "    \n",
    "    # Backtesting\n",
    "    initial_cash: float = 10000.0\n",
    "    fees: float = 0.001  # 0.1%\n",
    "    \n",
    "    # MinIO\n",
    "    minio_endpoint: str = \"127.0.0.1:9000\"\n",
    "    minio_access_key: str = \"minioadm\"\n",
    "    minio_secret_key: str = \"minioadm\"\n",
    "    \n",
    "    # Sortie\n",
    "    test_table_path: str = \"s3://test/backtest_results/\"\n",
    "    \n",
    "    def get_indicator_columns(self) -> Dict[str, str]:\n",
    "        \"\"\"Mapping des colonnes d'indicateurs\"\"\"\n",
    "        return {\n",
    "            \"ema_fast\": f\"ema_{self.ema_fast}\",\n",
    "            \"ema_slow\": f\"ema_{self.ema_slow}\",\n",
    "            \"rsi_14\": \"rsi_14\",\n",
    "            \"supertrend\": f\"supertrend_{self.supertrend_period}_{self.supertrend_multiplier}\",\n",
    "            \"supertrend_dir\": f\"supertrend_dir_{self.supertrend_period}_{self.supertrend_multiplier}\",\n",
    "            \"bb_upper\": \"bb_upper_20_2\",\n",
    "            \"bb_middle\": \"bb_middle_20_2\", \n",
    "            \"bb_lower\": \"bb_lower_20_2\",\n",
    "            \"macd\": \"macd_12_26_9\",\n",
    "            \"macd_signal\": \"macd_signal_12_26_9\",\n",
    "            \"atr_14\": \"atr_14\"\n",
    "        }\n",
    "\n",
    "# Configuration par dÃ©faut\n",
    "config = BacktestConfig()\n",
    "\n",
    "print(\"âš™ï¸ Configuration initialisÃ©e\")\n",
    "print(f\"ðŸ“Š Symbole: {config.symbol}\")\n",
    "print(f\"ðŸ”„ Chunk size: {config.chunk_size:,} lignes\")\n",
    "print(f\"ðŸ›¡ï¸ Buffer contexte: {config.get_required_context_size()} lignes\")\n",
    "print(f\"ðŸ“… PÃ©riode: {config.start_date} â†’ {config.end_date or 'fin'}\")\n",
    "print(f\"ðŸ’° Capital initial: ${config.initial_cash:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2026e64",
   "metadata": {},
   "source": [
    "## 2. ðŸ”Œ Connexion aux Sources de DonnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc991dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” TEST DE CONNEXION ET VALIDATION DU CHEMIN\n",
      "=============================================\n",
      "ðŸ”Œ Configuration connexion DuckDB â†’ MinIO...\n",
      "âœ… Connexion DuckDB configurÃ©e\n",
      "\n",
      "ðŸ§ª Test 1: s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/**/*.parquet\n",
      "   âœ… 5 fichier(s) trouvÃ©(s)\n",
      "     ðŸ“ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=10/data_0.parquet\n",
      "     ðŸ“ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=11/data_0.parquet\n",
      "     ðŸ“ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=12/data_0.parquet\n",
      "     ... et 2 autres\n",
      "   ðŸ“Š Test lecture: 17,604 lignes, 1 symbole(s)\n",
      "   ðŸŽ¯ Chemin optimal trouvÃ© !\n",
      "\n",
      "ðŸŽ¯ Chemin final utilisÃ©: s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/**/*.parquet\n",
      "\n",
      "=============================================\n",
      "ðŸ“Š Analyse des partitions disponibles...\n",
      "ðŸ“ 97 partitions trouvÃ©es\n",
      "ðŸ“ Fichiers trouvÃ©s:\n",
      "   â€¢ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=10/data_0.parquet\n",
      "   â€¢ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=11/data_0.parquet\n",
      "   â€¢ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=12/data_0.parquet\n",
      "   â€¢ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=8/data_0.parquet\n",
      "   â€¢ s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=9/data_0.parquet\n",
      "   â€¢ ... et 92 autres\n",
      "ðŸ“ˆ Analyse du contenu des donnÃ©es...\n",
      "ðŸ“Š RÃ©sumÃ© des donnÃ©es:\n",
      "   â€¢ Total lignes: 5,844\n",
      "   â€¢ PÃ©riode: 2023-01-01 00:00:00 â†’ 2025-08-31 20:00:00\n",
      "   â€¢ Jours uniques: 974\n",
      "   â€¢ Chunks estimÃ©s: 1\n",
      "ðŸ“Š RÃ©sumÃ© des donnÃ©es:\n",
      "   â€¢ Total lignes: 5,844\n",
      "   â€¢ PÃ©riode: 2023-01-01 00:00:00 â†’ 2025-08-31 20:00:00\n",
      "   â€¢ Jours uniques: 974\n",
      "   â€¢ Chunks estimÃ©s: 1\n"
     ]
    }
   ],
   "source": [
    "class HermesDataLoader:\n",
    "    \"\"\"Gestionnaire de connexion aux donnÃ©es Hermes\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BacktestConfig):\n",
    "        self.config = config\n",
    "        self.con = None\n",
    "        self.indicator_cols = config.get_indicator_columns()\n",
    "        \n",
    "        # Colonnes de base OHLCV\n",
    "        self.price_cols = ['datetime', 'open', 'high', 'low', 'close', 'volume']\n",
    "        \n",
    "    def setup_connection(self):\n",
    "        \"\"\"Configure la connexion DuckDB vers MinIO\"\"\"\n",
    "        print(\"ðŸ”Œ Configuration connexion DuckDB â†’ MinIO...\")\n",
    "        \n",
    "        self.con = duckdb.connect()\n",
    "        \n",
    "        # Configuration S3/MinIO\n",
    "        self.con.execute(f\"\"\"\n",
    "            SET s3_access_key_id='{self.config.minio_access_key}';\n",
    "            SET s3_secret_access_key='{self.config.minio_secret_key}';\n",
    "            SET s3_endpoint='{self.config.minio_endpoint}';\n",
    "            SET s3_url_style='path';\n",
    "            SET s3_use_ssl='false';\n",
    "        \"\"\")\n",
    "        \n",
    "        # Optimisations mÃ©moire\n",
    "        self.con.execute(\"\"\"\n",
    "            SET threads TO 6;\n",
    "            SET memory_limit = '4GB';\n",
    "            SET enable_progress_bar = true;\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"âœ… Connexion DuckDB configurÃ©e\")\n",
    "    \n",
    "    def get_partition_info(self) -> List[Dict]:\n",
    "        \"\"\"RÃ©cupÃ¨re les informations des partitions disponibles\"\"\"\n",
    "        if not self.con:\n",
    "            self.setup_connection()\n",
    "        \n",
    "        print(\"ðŸ“Š Analyse des partitions disponibles...\")\n",
    "        \n",
    "        try:\n",
    "            # RÃ©cupÃ©rer la liste des fichiers avec mÃ©tadonnÃ©es\n",
    "            result = self.con.execute(f\"\"\"\n",
    "                SELECT file as filename\n",
    "                FROM glob('{self.config.feature_store_path}')\n",
    "                ORDER BY file\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            partitions = []\n",
    "            for row in result:\n",
    "                partitions.append({\n",
    "                    'path': row[0],\n",
    "                    'size_mb': 0,  # Taille non disponible avec glob simple\n",
    "                    'last_modified': 'unknown'\n",
    "                })\n",
    "            \n",
    "            print(f\"ðŸ“ {len(partitions)} partitions trouvÃ©es\")\n",
    "            if partitions:\n",
    "                print(f\"ðŸ“ Fichiers trouvÃ©s:\")\n",
    "                for i, p in enumerate(partitions[:5]):  # Afficher les 5 premiers\n",
    "                    print(f\"   â€¢ {p['path']}\")\n",
    "                if len(partitions) > 5:\n",
    "                    print(f\"   â€¢ ... et {len(partitions)-5} autres\")\n",
    "            \n",
    "            return partitions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur lors de l'analyse des partitions: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_data_summary(self) -> Dict:\n",
    "        \"\"\"RÃ©cupÃ¨re un rÃ©sumÃ© des donnÃ©es disponibles\"\"\"\n",
    "        if not self.con:\n",
    "            self.setup_connection()\n",
    "        \n",
    "        print(\"ðŸ“ˆ Analyse du contenu des donnÃ©es...\")\n",
    "        \n",
    "        # RequÃªte avec filtre temporel si spÃ©cifiÃ©\n",
    "        where_clause = f\"WHERE symbol = '{self.config.symbol}'\"\n",
    "        if self.config.start_date:\n",
    "            where_clause += f\" AND datetime >= '{self.config.start_date}'\"\n",
    "        if self.config.end_date:\n",
    "            where_clause += f\" AND datetime <= '{self.config.end_date}'\"\n",
    "        \n",
    "        try:\n",
    "            result = self.con.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_rows,\n",
    "                    MIN(datetime) as start_date,\n",
    "                    MAX(datetime) as end_date,\n",
    "                    COUNT(DISTINCT date_trunc('day', datetime)) as unique_days\n",
    "                FROM read_parquet('{self.config.feature_store_path}')\n",
    "                {where_clause}\n",
    "            \"\"\").fetchone()\n",
    "            \n",
    "            summary = {\n",
    "                'total_rows': result[0],\n",
    "                'start_date': result[1],\n",
    "                'end_date': result[2],\n",
    "                'unique_days': result[3]\n",
    "            }\n",
    "            \n",
    "            print(f\"ðŸ“Š RÃ©sumÃ© des donnÃ©es:\")\n",
    "            print(f\"   â€¢ Total lignes: {summary['total_rows']:,}\")\n",
    "            print(f\"   â€¢ PÃ©riode: {summary['start_date']} â†’ {summary['end_date']}\")\n",
    "            print(f\"   â€¢ Jours uniques: {summary['unique_days']:,}\")\n",
    "            \n",
    "            # Estimation des chunks\n",
    "            estimated_chunks = (summary['total_rows'] // self.config.chunk_size) + 1\n",
    "            print(f\"   â€¢ Chunks estimÃ©s: {estimated_chunks:,}\")\n",
    "            \n",
    "            return summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur lors de l'analyse: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Initialisation du loader\n",
    "data_loader = HermesDataLoader(config)\n",
    "\n",
    "# ðŸ” Test de connexion et validation du chemin\n",
    "print(\"ðŸ” TEST DE CONNEXION ET VALIDATION DU CHEMIN\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Test direct du chemin avec glob\n",
    "try:\n",
    "    if not data_loader.con:\n",
    "        data_loader.setup_connection()\n",
    "    \n",
    "    # Tester diffÃ©rents patterns de chemin\n",
    "    test_paths = [\n",
    "        \"s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/**/*.parquet\",  # Pattern actuel\n",
    "        \"s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/*.parquet\",     # Pattern direct\n",
    "        \"s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/*\",             # Tous les fichiers\n",
    "    ]\n",
    "    \n",
    "    for i, test_path in enumerate(test_paths, 1):\n",
    "        print(f\"\\nðŸ§ª Test {i}: {test_path}\")\n",
    "        try:\n",
    "            # Test avec glob pour lister les fichiers\n",
    "            files_result = data_loader.con.execute(f\"\"\"\n",
    "                SELECT file as filename\n",
    "                FROM glob('{test_path}')\n",
    "                LIMIT 5\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            if files_result:\n",
    "                print(f\"   âœ… {len(files_result)} fichier(s) trouvÃ©(s)\")\n",
    "                for file_info in files_result[:3]:  # Afficher les 3 premiers\n",
    "                    print(f\"     ðŸ“ {file_info[0]}\")\n",
    "                if len(files_result) > 3:\n",
    "                    print(f\"     ... et {len(files_result)-3} autres\")\n",
    "                \n",
    "                # Si on trouve des fichiers, tester la lecture\n",
    "                try:\n",
    "                    test_read = data_loader.con.execute(f\"\"\"\n",
    "                        SELECT COUNT(*) as row_count, COUNT(DISTINCT symbol) as symbols\n",
    "                        FROM read_parquet('{test_path}')\n",
    "                        LIMIT 1\n",
    "                    \"\"\").fetchone()\n",
    "                    \n",
    "                    if test_read:\n",
    "                        print(f\"   ðŸ“Š Test lecture: {test_read[0]:,} lignes, {test_read[1]} symbole(s)\")\n",
    "                        # Mettre Ã  jour la config avec le chemin qui fonctionne\n",
    "                        config.feature_store_path = test_path\n",
    "                        print(f\"   ðŸŽ¯ Chemin optimal trouvÃ© !\")\n",
    "                        break\n",
    "                        \n",
    "                except Exception as read_error:\n",
    "                    print(f\"   âŒ Erreur lecture: {read_error}\")\n",
    "            else:\n",
    "                print(f\"   âŒ Aucun fichier trouvÃ©\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Erreur test: {e}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Chemin final utilisÃ©: {config.feature_store_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur lors du test de connexion: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 45)\n",
    "\n",
    "# Maintenant charger les infos avec le bon chemin\n",
    "partitions = data_loader.get_partition_info()\n",
    "data_summary = data_loader.get_data_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df3c10",
   "metadata": {},
   "source": [
    "## 3. ðŸ“Š Processeur de Backtesting Chunked avec Garanties de CohÃ©rence\n",
    "\n",
    "### SystÃ¨me de traitement par chunks avec continuitÃ© des signaux et positions\n",
    "\n",
    "**ðŸ›¡ï¸ Garanties de CohÃ©rence ImplÃ©mentÃ©es** :\n",
    "\n",
    "1. **Buffer de Contexte Ã‰tendu** : 50+ lignes de contexte entre chunks\n",
    "2. **Validation des Signaux `shift()`** : VÃ©rification que les valeurs prÃ©cÃ©dentes existent\n",
    "3. **DÃ©tection de Chevauchements** : Gestion automatique des doublons temporels\n",
    "4. **Validation Continue** : ContrÃ´les Ã  chaque Ã©tape du traitement\n",
    "5. **Diagnostic PrÃ©alable** : VÃ©rification de la cohÃ©rence des donnÃ©es source\n",
    "\n",
    "**Note** : Ce processeur utilise uniquement les indicateurs prÃ©-calculÃ©s dans le Hub Features Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1746505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Backtester chunked initialisÃ© avec garanties de cohÃ©rence\n",
      "âš™ï¸ Configuration: 50,000 lignes/chunk avec 100 lignes de contexte\n",
      "ðŸ’° Capital initial: $10,000.00\n",
      "ðŸ“Š Tous les indicateurs doivent Ãªtre prÃ©-calculÃ©s dans le Hub Features Gold\n",
      "âœ… SystÃ¨me de validation de continuitÃ© des signaux activÃ©\n"
     ]
    }
   ],
   "source": [
    "class HermesChunkedBacktester:\n",
    "    \"\"\"Backtesting chunked avec continuitÃ© pour stratÃ©gies Hermes\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BacktestConfig, data_loader: HermesDataLoader):\n",
    "        self.config = config\n",
    "        self.data_loader = data_loader\n",
    "        self.indicator_cols = config.get_indicator_columns()\n",
    "        \n",
    "        # Ã‰tat persistant entre chunks\n",
    "        self.state = {\n",
    "            'context_rows': None,        # Lignes de contexte pour continuitÃ©\n",
    "            'last_position': None,       # 'long', 'short' ou None\n",
    "            'cumulative_cash': config.initial_cash,\n",
    "            'cumulative_value': config.initial_cash,\n",
    "            'total_trades': 0,\n",
    "            'chunk_results': [],         # RÃ©sultats par chunk\n",
    "            'chunk_counter': 0,\n",
    "            'context_buffer_size': config.get_required_context_size()  #  Calcul automatique du buffer plus grand pour les signaux\n",
    "        }\n",
    "    \n",
    "    def compute_strategy_signals(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Calcule les signaux de la stratÃ©gie Smart Momentum avec validation de cohÃ©rence\"\"\"\n",
    "        \n",
    "        # RÃ©cupÃ©rer les colonnes d'indicateurs\n",
    "        ema_fast_col = self.indicator_cols[\"ema_fast\"]\n",
    "        ema_slow_col = self.indicator_cols[\"ema_slow\"]\n",
    "        rsi_col = self.indicator_cols[\"rsi_14\"]\n",
    "        supertrend_dir_col = self.indicator_cols[\"supertrend_dir\"]\n",
    "        \n",
    "        # VÃ©rifier que les colonnes existent\n",
    "        missing_cols = []\n",
    "        for col_name, col_actual in self.indicator_cols.items():\n",
    "            if col_actual not in df.columns:\n",
    "                missing_cols.append(f\"{col_name} -> {col_actual}\")\n",
    "        \n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"âŒ Colonnes d'indicateurs manquantes: {missing_cols}\")\n",
    "        \n",
    "        # Calcul des signaux avec validation de continuitÃ©\n",
    "        signals_df = df.with_columns([\n",
    "            # === VALIDATION DE CONTINUITÃ‰ ===\n",
    "            # Marquer les lignes oÃ¹ shift(1) sera valide\n",
    "            (pl.int_range(pl.len()) > 0).alias(\"has_previous_value\"),\n",
    "            \n",
    "            # === CONDITIONS EMA ===\n",
    "            # Condition actuelle : EMA rapide > EMA lente\n",
    "            (pl.col(ema_fast_col) > pl.col(ema_slow_col)).alias(\"ema_fast_above_slow\"),\n",
    "            \n",
    "            # Condition prÃ©cÃ©dente : EMA rapide <= EMA lente (avec gestion des nulls)\n",
    "            (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)).alias(\"ema_was_below_or_equal\"),\n",
    "            \n",
    "            # Crossover EMA seulement si on a une valeur prÃ©cÃ©dente valide\n",
    "            ((pl.col(ema_fast_col) > pl.col(ema_slow_col)) & \n",
    "             (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)) &\n",
    "             (pl.int_range(pl.len()) > 0)).alias(\"ema_bullish_cross\"),\n",
    "            \n",
    "            # === CONDITIONS RSI ===\n",
    "            ((pl.col(rsi_col) >= self.config.rsi_neutral_low) & \n",
    "             (pl.col(rsi_col) <= self.config.rsi_neutral_high)).alias(\"rsi_neutral\"),\n",
    "            \n",
    "            # === CONDITIONS SUPERTREND ===\n",
    "            (pl.col(supertrend_dir_col) == 1).alias(\"supertrend_bullish\"),\n",
    "            \n",
    "            # SuperTrend exit avec validation de continuitÃ©\n",
    "            ((pl.col(supertrend_dir_col).shift(1) == 1) & \n",
    "             (pl.col(supertrend_dir_col) == -1) &\n",
    "             (pl.int_range(pl.len()) > 0)).alias(\"supertrend_exit\")\n",
    "        ])\n",
    "        \n",
    "        # Signaux finaux avec validation\n",
    "        final_signals = signals_df.with_columns([\n",
    "            # Signal d'entrÃ©e (achat) - uniquement si toutes conditions remplies\n",
    "            (pl.col(\"ema_bullish_cross\") & \n",
    "             pl.col(\"rsi_neutral\") & \n",
    "             pl.col(\"supertrend_bullish\")).alias(\"buy_signal\"),\n",
    "            \n",
    "            # Signal de sortie (vente) - uniquement avec continuitÃ© validÃ©e\n",
    "            (pl.col(\"supertrend_exit\")).alias(\"sell_signal\")\n",
    "        ])\n",
    "        \n",
    "        return final_signals\n",
    "    \n",
    "    def load_chunk_with_context(self, offset: int, limit: int) -> pl.DataFrame:\n",
    "        \"\"\"Charge un chunk avec le contexte nÃ©cessaire pour garantir la cohÃ©rence\"\"\"\n",
    "        \n",
    "        # Construire la requÃªte avec filtre temporel\n",
    "        where_clause = f\"WHERE symbol = '{self.config.symbol}'\"\n",
    "        if self.config.start_date:\n",
    "            where_clause += f\" AND datetime >= '{self.config.start_date}'\"\n",
    "        if self.config.end_date:\n",
    "            where_clause += f\" AND datetime <= '{self.config.end_date}'\"\n",
    "        \n",
    "        # AMÃ‰LIORATION : Charger plus de contexte pour les premiers chunks\n",
    "        actual_offset = offset\n",
    "        actual_limit = limit\n",
    "        \n",
    "        # Si ce n'est pas le premier chunk, commencer plus tÃ´t pour avoir du contexte\n",
    "        if offset > 0 and self.state['context_rows'] is None:\n",
    "            # Charger du contexte supplÃ©mentaire depuis la base\n",
    "            context_needed = self.state['context_buffer_size']\n",
    "            actual_offset = max(0, offset - context_needed)\n",
    "            actual_limit = limit + (offset - actual_offset)\n",
    "        \n",
    "        # RequÃªte DuckDB pour le chunk avec contexte\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM read_parquet('{self.config.feature_store_path}')\n",
    "            {where_clause}\n",
    "            ORDER BY datetime\n",
    "            LIMIT {actual_limit}\n",
    "            OFFSET {actual_offset}\n",
    "        \"\"\"\n",
    "        \n",
    "        # ExÃ©cuter la requÃªte\n",
    "        result = self.data_loader.con.execute(query).arrow()\n",
    "        chunk_df = pl.from_arrow(result)\n",
    "        \n",
    "        # Ajouter le contexte des lignes prÃ©cÃ©dentes si disponible\n",
    "        if self.state['context_rows'] is not None and offset > 0:\n",
    "            # VÃ©rifier la continuitÃ© temporelle\n",
    "            if len(self.state['context_rows']) > 0 and len(chunk_df) > 0:\n",
    "                last_context_time = self.state['context_rows']['datetime'].max()\n",
    "                first_chunk_time = chunk_df['datetime'].min()\n",
    "                \n",
    "                if last_context_time >= first_chunk_time:\n",
    "                    print(f\"âš ï¸ Chevauchement temporel dÃ©tectÃ© - ajustement automatique\")\n",
    "                    # Filtrer les doublons\n",
    "                    chunk_df = chunk_df.filter(pl.col('datetime') > last_context_time)\n",
    "            \n",
    "            # ConcatÃ©ner avec le contexte\n",
    "            if len(chunk_df) > 0:\n",
    "                chunk_df = pl.concat([self.state['context_rows'], chunk_df])\n",
    "        \n",
    "        return chunk_df\n",
    "    \n",
    "    def process_chunk(self, chunk_df: pl.DataFrame, is_first_chunk: bool) -> Dict:\n",
    "        \"\"\"Traite un chunk avec calcul des signaux et backtesting\"\"\"\n",
    "        \n",
    "        if len(chunk_df) == 0:\n",
    "            return {\n",
    "                'chunk_id': self.state['chunk_counter'],\n",
    "                'rows_processed': 0,\n",
    "                'buy_signals': 0,\n",
    "                'sell_signals': 0,\n",
    "                'start_time': None,\n",
    "                'end_time': None,\n",
    "                'data': None,\n",
    "                'warnings': ['Chunk vide']\n",
    "            }\n",
    "        \n",
    "        # 1. Calculer les signaux de stratÃ©gie avec validation de cohÃ©rence\n",
    "        try:\n",
    "            signals_df = self.compute_strategy_signals(chunk_df)\n",
    "        except ValueError as e:\n",
    "            print(f\"âŒ Erreur dans le calcul des signaux: {e}\")\n",
    "            return {\n",
    "                'chunk_id': self.state['chunk_counter'],\n",
    "                'rows_processed': 0,\n",
    "                'buy_signals': 0,\n",
    "                'sell_signals': 0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "        \n",
    "        # 2. Ajuster les signaux selon l'Ã©tat prÃ©cÃ©dent\n",
    "        if not is_first_chunk and self.state['last_position'] is not None:\n",
    "            # Si on Ã©tait en position, ne pas gÃ©nÃ©rer d'entrÃ©e immÃ©diate\n",
    "            if self.state['last_position'] == 'long':\n",
    "                signals_df = signals_df.with_columns(\n",
    "                    pl.when(pl.int_range(pl.len()) == 0)\n",
    "                    .then(False)\n",
    "                    .otherwise(pl.col(\"buy_signal\"))\n",
    "                    .alias(\"buy_signal\")\n",
    "                )\n",
    "        \n",
    "        # 3. Extraire les rÃ©sultats sans le contexte\n",
    "        context_size = len(self.state['context_rows']) if self.state['context_rows'] is not None and not is_first_chunk else 0\n",
    "        \n",
    "        if context_size > 0:\n",
    "            result_df = signals_df.slice(context_size)\n",
    "        else:\n",
    "            result_df = signals_df\n",
    "        \n",
    "        # 4. Validation des signaux calculÃ©s\n",
    "        warnings = []\n",
    "        if len(result_df) > 0:\n",
    "            # VÃ©rifier qu'on n'a pas de signaux sur la premiÃ¨re ligne d'un chunk (sauf premier chunk)\n",
    "            if not is_first_chunk and context_size == 0:\n",
    "                first_row_signals = result_df.head(1)\n",
    "                if (first_row_signals.select(pl.col(\"buy_signal\").sum()).item() > 0 or \n",
    "                    first_row_signals.select(pl.col(\"sell_signal\").sum()).item() > 0):\n",
    "                    warnings.append(\"Signaux dÃ©tectÃ©s sur premiÃ¨re ligne sans contexte\")\n",
    "        \n",
    "        # 5. Compter les signaux\n",
    "        buy_signals = result_df.select(pl.col(\"buy_signal\").sum()).item() if len(result_df) > 0 else 0\n",
    "        sell_signals = result_df.select(pl.col(\"sell_signal\").sum()).item() if len(result_df) > 0 else 0\n",
    "        \n",
    "        # 6. Mettre Ã  jour l'Ã©tat pour le chunk suivant avec plus de contexte\n",
    "        if len(signals_df) > 0:\n",
    "            self.state['context_rows'] = signals_df.tail(self.state['context_buffer_size'])\n",
    "        \n",
    "        # Simuler la position (logique simplifiÃ©e)\n",
    "        if buy_signals > 0 and self.state['last_position'] != 'long':\n",
    "            self.state['last_position'] = 'long'\n",
    "        elif sell_signals > 0 and self.state['last_position'] == 'long':\n",
    "            self.state['last_position'] = None\n",
    "            self.state['total_trades'] += 1\n",
    "        \n",
    "        # 7. Retourner les rÃ©sultats du chunk avec mÃ©tadonnÃ©es de validation\n",
    "        chunk_result = {\n",
    "            'chunk_id': self.state['chunk_counter'],\n",
    "            'rows_processed': len(result_df),\n",
    "            'buy_signals': buy_signals,\n",
    "            'sell_signals': sell_signals,\n",
    "            'start_time': result_df.select(pl.col(\"datetime\").min()).item() if len(result_df) > 0 else None,\n",
    "            'end_time': result_df.select(pl.col(\"datetime\").max()).item() if len(result_df) > 0 else None,\n",
    "            'data': result_df,  # Conserver les donnÃ©es pour sauvegarde\n",
    "            'context_size': context_size,\n",
    "            'warnings': warnings,\n",
    "            'continuity_validated': context_size > 0 or is_first_chunk\n",
    "        }\n",
    "        \n",
    "        self.state['chunk_counter'] += 1\n",
    "        \n",
    "        return chunk_result\n",
    "\n",
    "# Initialisation du backtester\n",
    "backtester = HermesChunkedBacktester(config, data_loader)\n",
    "\n",
    "print(\"ðŸš€ Backtester chunked initialisÃ© avec garanties de cohÃ©rence\")\n",
    "print(f\"âš™ï¸ Configuration: {config.chunk_size:,} lignes/chunk avec {backtester.state['context_buffer_size']} lignes de contexte\")\n",
    "print(f\"ðŸ’° Capital initial: ${config.initial_cash:,.2f}\")\n",
    "print(\"ðŸ“Š Tous les indicateurs doivent Ãªtre prÃ©-calculÃ©s dans le Hub Features Gold\")\n",
    "print(\"âœ… SystÃ¨me de validation de continuitÃ© des signaux activÃ©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce5e05c",
   "metadata": {},
   "source": [
    "## 4. ðŸ”„ ExÃ©cution du Backtesting Chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19f8c0",
   "metadata": {},
   "source": [
    "## 4.1 ðŸ” Diagnostic de CohÃ©rence des DonnÃ©es\n",
    "\n",
    "Avant d'exÃ©cuter le backtesting, vÃ©rifions la cohÃ©rence des donnÃ©es et la continuitÃ© temporelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3ad4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” DIAGNOSTIC DE COHERENCE DES DONNÃ‰ES\n",
      "========================================\n",
      "ðŸ“… VÃ©rification de la continuitÃ© temporelle...\n",
      "   â€¢ Lignes analysÃ©es: 9,999\n",
      "   â€¢ Intervalles uniques: 4\n",
      "   â€¢ Intervalle min: 4.00 heures\n",
      "   â€¢ Intervalle max: 32.00 heures\n",
      "   â€¢ Intervalle moyen: 4.01 heures\n",
      "\n",
      "ðŸ“Š VÃ©rification des indicateurs requis...\n",
      "   â€¢ Colonnes disponibles: 35\n",
      "   â€¢ Indicateurs requis: 11\n",
      "   â€¢ Indicateurs manquants: 0\n",
      "   âœ… Tous les indicateurs requis sont prÃ©sents\n",
      "\n",
      "ðŸ§ª Test de calcul de signaux sur Ã©chantillon...\n",
      "   âœ… Test rÃ©ussi - 1 signaux d'achat, 11 signaux de vente sur Ã©chantillon\n",
      "\n",
      "ðŸ“‹ RÃ‰SUMÃ‰ DU DIAGNOSTIC:\n",
      "   â€¢ ContinuitÃ© temporelle: âŒ\n",
      "   â€¢ Indicateurs complets: âœ…\n",
      "\n",
      "ðŸ’¡ RECOMMANDATIONS:\n",
      "   âš ï¸ Intervalles temporels irrÃ©guliers dÃ©tectÃ©s - vÃ©rifier la qualitÃ© des donnÃ©es\n",
      "   âœ… Test rÃ©ussi - 1 signaux d'achat, 11 signaux de vente sur Ã©chantillon\n",
      "\n",
      "ðŸ“‹ RÃ‰SUMÃ‰ DU DIAGNOSTIC:\n",
      "   â€¢ ContinuitÃ© temporelle: âŒ\n",
      "   â€¢ Indicateurs complets: âœ…\n",
      "\n",
      "ðŸ’¡ RECOMMANDATIONS:\n",
      "   âš ï¸ Intervalles temporels irrÃ©guliers dÃ©tectÃ©s - vÃ©rifier la qualitÃ© des donnÃ©es\n"
     ]
    }
   ],
   "source": [
    "def run_coherence_diagnostic() -> Dict:\n",
    "    \"\"\"Diagnostic de cohÃ©rence des donnÃ©es avant backtesting\"\"\"\n",
    "    \n",
    "    print(\"ðŸ” DIAGNOSTIC DE COHERENCE DES DONNÃ‰ES\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if not data_loader.con:\n",
    "        data_loader.setup_connection()\n",
    "    \n",
    "    diagnostic_results = {\n",
    "        'temporal_continuity': True,\n",
    "        'indicator_completeness': True,\n",
    "        'data_gaps': [],\n",
    "        'missing_indicators': [],\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 1. VÃ©rifier la continuitÃ© temporelle\n",
    "        print(\"ðŸ“… VÃ©rification de la continuitÃ© temporelle...\")\n",
    "        \n",
    "        temporal_query = f\"\"\"\n",
    "            WITH time_diffs AS (\n",
    "                SELECT \n",
    "                    datetime,\n",
    "                    LAG(datetime) OVER (ORDER BY datetime) as prev_datetime,\n",
    "                    datetime - LAG(datetime) OVER (ORDER BY datetime) as time_diff_interval,\n",
    "                    EXTRACT(EPOCH FROM (datetime - LAG(datetime) OVER (ORDER BY datetime))) / 3600.0 as time_diff_hours\n",
    "                FROM read_parquet('{config.feature_store_path}')\n",
    "                WHERE symbol = '{config.symbol}'\n",
    "                ORDER BY datetime\n",
    "                LIMIT 10000  -- Ã‰chantillon pour diagnostic\n",
    "            )\n",
    "            SELECT \n",
    "                COUNT(*) as total_rows,\n",
    "                COUNT(DISTINCT time_diff_hours) as unique_intervals,\n",
    "                MIN(time_diff_hours) as min_interval_hours,\n",
    "                MAX(time_diff_hours) as max_interval_hours,\n",
    "                AVG(time_diff_hours) as avg_interval_hours\n",
    "            FROM time_diffs \n",
    "            WHERE time_diff_hours IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        result = data_loader.con.execute(temporal_query).fetchone()\n",
    "        total_rows, unique_intervals, min_interval_hours, max_interval_hours, avg_interval_hours = result\n",
    "        \n",
    "        print(f\"   â€¢ Lignes analysÃ©es: {total_rows:,}\")\n",
    "        print(f\"   â€¢ Intervalles uniques: {unique_intervals}\")\n",
    "        print(f\"   â€¢ Intervalle min: {min_interval_hours:.2f} heures\")\n",
    "        print(f\"   â€¢ Intervalle max: {max_interval_hours:.2f} heures\")\n",
    "        print(f\"   â€¢ Intervalle moyen: {avg_interval_hours:.2f} heures\")\n",
    "        \n",
    "        if unique_intervals > 2:  # TolÃ©rance pour quelques variations\n",
    "            diagnostic_results['temporal_continuity'] = False\n",
    "            diagnostic_results['recommendations'].append(\n",
    "                \"âš ï¸ Intervalles temporels irrÃ©guliers dÃ©tectÃ©s - vÃ©rifier la qualitÃ© des donnÃ©es\"\n",
    "            )\n",
    "        \n",
    "        # 2. VÃ©rifier la prÃ©sence des indicateurs requis\n",
    "        print(\"\\nðŸ“Š VÃ©rification des indicateurs requis...\")\n",
    "        \n",
    "        schema_query = f\"\"\"\n",
    "            DESCRIBE SELECT * FROM read_parquet('{config.feature_store_path}') LIMIT 1\n",
    "        \"\"\"\n",
    "        \n",
    "        available_columns = [row[0] for row in data_loader.con.execute(schema_query).fetchall()]\n",
    "        required_indicators = list(config.get_indicator_columns().values())\n",
    "        \n",
    "        missing_indicators = [ind for ind in required_indicators if ind not in available_columns]\n",
    "        \n",
    "        print(f\"   â€¢ Colonnes disponibles: {len(available_columns)}\")\n",
    "        print(f\"   â€¢ Indicateurs requis: {len(required_indicators)}\")\n",
    "        print(f\"   â€¢ Indicateurs manquants: {len(missing_indicators)}\")\n",
    "        \n",
    "        if missing_indicators:\n",
    "            diagnostic_results['indicator_completeness'] = False\n",
    "            diagnostic_results['missing_indicators'] = missing_indicators\n",
    "            print(f\"   âŒ Indicateurs manquants: {missing_indicators}\")\n",
    "            diagnostic_results['recommendations'].append(\n",
    "                f\"âŒ Recalculer ou ajouter les indicateurs manquants: {missing_indicators}\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"   âœ… Tous les indicateurs requis sont prÃ©sents\")\n",
    "        \n",
    "        # 3. Tester un Ã©chantillon de calcul de signaux\n",
    "        print(\"\\nðŸ§ª Test de calcul de signaux sur Ã©chantillon...\")\n",
    "        \n",
    "        sample_query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM read_parquet('{config.feature_store_path}')\n",
    "            WHERE symbol = '{config.symbol}'\n",
    "            ORDER BY datetime\n",
    "            LIMIT 1000\n",
    "        \"\"\"\n",
    "        \n",
    "        sample_result = data_loader.con.execute(sample_query).arrow()\n",
    "        sample_df = pl.from_arrow(sample_result)\n",
    "        \n",
    "        if len(sample_df) > 0:\n",
    "            try:\n",
    "                # Test du calcul de signaux\n",
    "                test_signals = backtester.compute_strategy_signals(sample_df)\n",
    "                \n",
    "                buy_count = test_signals.select(pl.col(\"buy_signal\").sum()).item()\n",
    "                sell_count = test_signals.select(pl.col(\"sell_signal\").sum()).item()\n",
    "                \n",
    "                print(f\"   âœ… Test rÃ©ussi - {buy_count} signaux d'achat, {sell_count} signaux de vente sur Ã©chantillon\")\n",
    "                \n",
    "                # VÃ©rifier les signaux sur la premiÃ¨re ligne (problÃ¨me de shift)\n",
    "                first_row_signals = test_signals.head(1)\n",
    "                first_buy = first_row_signals.select(pl.col(\"buy_signal\")).item()\n",
    "                first_sell = first_row_signals.select(pl.col(\"sell_signal\")).item()\n",
    "                \n",
    "                if first_buy or first_sell:\n",
    "                    diagnostic_results['recommendations'].append(\n",
    "                        \"âš ï¸ Signaux dÃ©tectÃ©s sur premiÃ¨re ligne - vÃ©rifier la logique de shift()\"\n",
    "                    )\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Erreur dans le calcul de signaux: {e}\")\n",
    "                diagnostic_results['recommendations'].append(f\"âŒ Erreur calcul signaux: {e}\")\n",
    "        \n",
    "        # 4. RÃ©sumÃ© du diagnostic\n",
    "        print(f\"\\nðŸ“‹ RÃ‰SUMÃ‰ DU DIAGNOSTIC:\")\n",
    "        print(f\"   â€¢ ContinuitÃ© temporelle: {'âœ…' if diagnostic_results['temporal_continuity'] else 'âŒ'}\")\n",
    "        print(f\"   â€¢ Indicateurs complets: {'âœ…' if diagnostic_results['indicator_completeness'] else 'âŒ'}\")\n",
    "        \n",
    "        if diagnostic_results['recommendations']:\n",
    "            print(f\"\\nðŸ’¡ RECOMMANDATIONS:\")\n",
    "            for rec in diagnostic_results['recommendations']:\n",
    "                print(f\"   {rec}\")\n",
    "        else:\n",
    "            print(f\"\\nâœ… TOUTES LES VÃ‰RIFICATIONS PASSÃ‰ES - PRÃŠT POUR LE BACKTESTING\")\n",
    "        \n",
    "        return diagnostic_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur lors du diagnostic: {e}\")\n",
    "        diagnostic_results['recommendations'].append(f\"âŒ Erreur diagnostic: {e}\")\n",
    "        return diagnostic_results\n",
    "\n",
    "# ExÃ©cution du diagnostic\n",
    "diagnostic = run_coherence_diagnostic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4988618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” DIAGNOSTIC AVANCÃ‰ DES SIGNAUX\n",
      "===================================\n",
      "ðŸ“Š Ã‰chantillon analysÃ©: 5,000 lignes\n",
      "\n",
      "ðŸ“ˆ COLONNES D'INDICATEURS:\n",
      "   â€¢ EMA Fast (ema_12): âœ…\n",
      "   â€¢ EMA Slow (ema_26): âœ…\n",
      "   â€¢ RSI (rsi_14): âœ…\n",
      "   â€¢ SuperTrend Dir (supertrend_dir_10_3.0): âœ…\n",
      "\n",
      "ðŸ“Š ANALYSE DES CONDITIONS (sur 5,000 lignes):\n",
      "   â€¢ EMA Fast > Slow: 2,575 (51.5%)\n",
      "   â€¢ EMA Bullish Cross: 73 (1.5%)\n",
      "   â€¢ RSI Neutral (45-55): 1,392 (27.8%)\n",
      "   â€¢ SuperTrend Bullish: 2,531 (50.6%)\n",
      "   â€¢ ðŸŽ¯ SIGNAUX D'ACHAT FINAUX: 9\n",
      "\n",
      "ðŸ“Š STATISTIQUES RSI:\n",
      "   â€¢ Min: 7.7\n",
      "   â€¢ Q25: 41.4\n",
      "   â€¢ Moyenne: nan\n",
      "   â€¢ Q75: 60.3\n",
      "   â€¢ Max: 95.0\n",
      "   â€¢ Plage neutre configurÃ©e: 45-55\n",
      "\n",
      "ðŸ’¡ SUGGESTIONS D'AMÃ‰LIORATION:\n",
      "\n",
      "===================================\n",
      "ðŸ“Š Ã‰chantillon analysÃ©: 5,000 lignes\n",
      "\n",
      "ðŸ“ˆ COLONNES D'INDICATEURS:\n",
      "   â€¢ EMA Fast (ema_12): âœ…\n",
      "   â€¢ EMA Slow (ema_26): âœ…\n",
      "   â€¢ RSI (rsi_14): âœ…\n",
      "   â€¢ SuperTrend Dir (supertrend_dir_10_3.0): âœ…\n",
      "\n",
      "ðŸ“Š ANALYSE DES CONDITIONS (sur 5,000 lignes):\n",
      "   â€¢ EMA Fast > Slow: 2,575 (51.5%)\n",
      "   â€¢ EMA Bullish Cross: 73 (1.5%)\n",
      "   â€¢ RSI Neutral (45-55): 1,392 (27.8%)\n",
      "   â€¢ SuperTrend Bullish: 2,531 (50.6%)\n",
      "   â€¢ ðŸŽ¯ SIGNAUX D'ACHAT FINAUX: 9\n",
      "\n",
      "ðŸ“Š STATISTIQUES RSI:\n",
      "   â€¢ Min: 7.7\n",
      "   â€¢ Q25: 41.4\n",
      "   â€¢ Moyenne: nan\n",
      "   â€¢ Q75: 60.3\n",
      "   â€¢ Max: 95.0\n",
      "   â€¢ Plage neutre configurÃ©e: 45-55\n",
      "\n",
      "ðŸ’¡ SUGGESTIONS D'AMÃ‰LIORATION:\n",
      "\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# ðŸ” DIAGNOSTIC AVANCÃ‰ : Pourquoi aucun signal n'est gÃ©nÃ©rÃ© ?\n",
    "print(\"ðŸ” DIAGNOSTIC AVANCÃ‰ DES SIGNAUX\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Charger un Ã©chantillon plus large pour diagnostic\n",
    "sample_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_parquet('{config.feature_store_path}')\n",
    "    WHERE symbol = '{config.symbol}'\n",
    "    ORDER BY datetime\n",
    "    LIMIT 5000\n",
    "\"\"\"\n",
    "\n",
    "sample_result = data_loader.con.execute(sample_query).arrow()\n",
    "sample_df = pl.from_arrow(sample_result)\n",
    "\n",
    "print(f\"ðŸ“Š Ã‰chantillon analysÃ©: {len(sample_df):,} lignes\")\n",
    "\n",
    "if len(sample_df) > 0:\n",
    "    # Calculer les signaux avec diagnostics dÃ©taillÃ©s\n",
    "    ema_fast_col = config.get_indicator_columns()[\"ema_fast\"]\n",
    "    ema_slow_col = config.get_indicator_columns()[\"ema_slow\"]\n",
    "    rsi_col = config.get_indicator_columns()[\"rsi_14\"]\n",
    "    supertrend_dir_col = config.get_indicator_columns()[\"supertrend_dir\"]\n",
    "    \n",
    "    # VÃ©rifier les colonnes d'indicateurs\n",
    "    print(f\"\\nðŸ“ˆ COLONNES D'INDICATEURS:\")\n",
    "    print(f\"   â€¢ EMA Fast ({ema_fast_col}): {'âœ…' if ema_fast_col in sample_df.columns else 'âŒ MANQUANT'}\")\n",
    "    print(f\"   â€¢ EMA Slow ({ema_slow_col}): {'âœ…' if ema_slow_col in sample_df.columns else 'âŒ MANQUANT'}\")\n",
    "    print(f\"   â€¢ RSI ({rsi_col}): {'âœ…' if rsi_col in sample_df.columns else 'âŒ MANQUANT'}\")\n",
    "    print(f\"   â€¢ SuperTrend Dir ({supertrend_dir_col}): {'âœ…' if supertrend_dir_col in sample_df.columns else 'âŒ MANQUANT'}\")\n",
    "    \n",
    "    # Analyser les conditions individuelles\n",
    "    if all(col in sample_df.columns for col in [ema_fast_col, ema_slow_col, rsi_col, supertrend_dir_col]):\n",
    "        # Calculer les conditions individuelles\n",
    "        analysis_df = sample_df.with_columns([\n",
    "            # Conditions EMA\n",
    "            (pl.col(ema_fast_col) > pl.col(ema_slow_col)).alias(\"ema_fast_above_slow\"),\n",
    "            (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)).alias(\"ema_was_below_or_equal\"),\n",
    "            \n",
    "            # Crossover EMA\n",
    "            ((pl.col(ema_fast_col) > pl.col(ema_slow_col)) & \n",
    "             (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)) &\n",
    "             (pl.int_range(pl.len()) > 0)).alias(\"ema_bullish_cross\"),\n",
    "            \n",
    "            # Conditions RSI\n",
    "            ((pl.col(rsi_col) >= config.rsi_neutral_low) & \n",
    "             (pl.col(rsi_col) <= config.rsi_neutral_high)).alias(\"rsi_neutral\"),\n",
    "            \n",
    "            # Conditions SuperTrend\n",
    "            (pl.col(supertrend_dir_col) == 1).alias(\"supertrend_bullish\"),\n",
    "            \n",
    "            # Signal final\n",
    "            ((pl.col(ema_fast_col) > pl.col(ema_slow_col)) & \n",
    "             (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)) &\n",
    "             (pl.int_range(pl.len()) > 0) &\n",
    "             (pl.col(rsi_col) >= config.rsi_neutral_low) & \n",
    "             (pl.col(rsi_col) <= config.rsi_neutral_high) &\n",
    "             (pl.col(supertrend_dir_col) == 1)).alias(\"buy_signal\")\n",
    "        ])\n",
    "        \n",
    "        # Statistiques des conditions\n",
    "        print(f\"\\nðŸ“Š ANALYSE DES CONDITIONS (sur {len(analysis_df):,} lignes):\")\n",
    "        \n",
    "        ema_fast_above = analysis_df.select(pl.col(\"ema_fast_above_slow\").sum()).item()\n",
    "        ema_crossover = analysis_df.select(pl.col(\"ema_bullish_cross\").sum()).item()\n",
    "        rsi_neutral_count = analysis_df.select(pl.col(\"rsi_neutral\").sum()).item()\n",
    "        supertrend_bull = analysis_df.select(pl.col(\"supertrend_bullish\").sum()).item()\n",
    "        buy_signals = analysis_df.select(pl.col(\"buy_signal\").sum()).item()\n",
    "        \n",
    "        print(f\"   â€¢ EMA Fast > Slow: {ema_fast_above:,} ({ema_fast_above/len(analysis_df)*100:.1f}%)\")\n",
    "        print(f\"   â€¢ EMA Bullish Cross: {ema_crossover:,} ({ema_crossover/len(analysis_df)*100:.1f}%)\")\n",
    "        print(f\"   â€¢ RSI Neutral ({config.rsi_neutral_low}-{config.rsi_neutral_high}): {rsi_neutral_count:,} ({rsi_neutral_count/len(analysis_df)*100:.1f}%)\")\n",
    "        print(f\"   â€¢ SuperTrend Bullish: {supertrend_bull:,} ({supertrend_bull/len(analysis_df)*100:.1f}%)\")\n",
    "        print(f\"   â€¢ ðŸŽ¯ SIGNAUX D'ACHAT FINAUX: {buy_signals:,}\")\n",
    "        \n",
    "        # Analyser les valeurs RSI pour comprendre le problÃ¨me\n",
    "        rsi_stats = sample_df.select([\n",
    "            pl.col(rsi_col).min().alias(\"rsi_min\"),\n",
    "            pl.col(rsi_col).max().alias(\"rsi_max\"),\n",
    "            pl.col(rsi_col).mean().alias(\"rsi_mean\"),\n",
    "            pl.col(rsi_col).quantile(0.25).alias(\"rsi_q25\"),\n",
    "            pl.col(rsi_col).quantile(0.75).alias(\"rsi_q75\")\n",
    "        ]).to_dicts()[0]\n",
    "        \n",
    "        print(f\"\\nðŸ“Š STATISTIQUES RSI:\")\n",
    "        print(f\"   â€¢ Min: {rsi_stats['rsi_min']:.1f}\")\n",
    "        print(f\"   â€¢ Q25: {rsi_stats['rsi_q25']:.1f}\")\n",
    "        print(f\"   â€¢ Moyenne: {rsi_stats['rsi_mean']:.1f}\")\n",
    "        print(f\"   â€¢ Q75: {rsi_stats['rsi_q75']:.1f}\")\n",
    "        print(f\"   â€¢ Max: {rsi_stats['rsi_max']:.1f}\")\n",
    "        print(f\"   â€¢ Plage neutre configurÃ©e: {config.rsi_neutral_low}-{config.rsi_neutral_high}\")\n",
    "        \n",
    "        if rsi_stats['rsi_mean'] < config.rsi_neutral_low or rsi_stats['rsi_mean'] > config.rsi_neutral_high:\n",
    "            print(f\"   âš ï¸ La moyenne RSI ({rsi_stats['rsi_mean']:.1f}) est en dehors de la plage neutre!\")\n",
    "            \n",
    "        # Suggestions d'amÃ©lioration\n",
    "        print(f\"\\nðŸ’¡ SUGGESTIONS D'AMÃ‰LIORATION:\")\n",
    "        if ema_crossover == 0:\n",
    "            print(\"   â€¢ Aucun croisement EMA dÃ©tectÃ© - vÃ©rifier les pÃ©riodes EMA\")\n",
    "        if rsi_neutral_count < len(analysis_df) * 0.1:\n",
    "            print(f\"   â€¢ Plage RSI trop restrictive - essayer {config.rsi_neutral_low-10}-{config.rsi_neutral_high+10}\")\n",
    "        if supertrend_bull < len(analysis_df) * 0.3:\n",
    "            print(\"   â€¢ SuperTrend rarement bullish - ajuster les paramÃ¨tres\")\n",
    "            \n",
    "    else:\n",
    "        print(\"âŒ Colonnes d'indicateurs manquantes - impossible d'analyser les conditions\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f794330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” DIAGNOSTIC SUPERTREND\n",
      "=========================\n",
      "ðŸ“Š Valeurs uniques de supertrend_dir_10_3.0:\n",
      "   â€¢ Valeur -1.0: 2,459 occurrences (49.2%)\n",
      "   â€¢ Valeur 1.0: 2,531 occurrences (50.6%)\n",
      "   â€¢ Valeur nan: 10 occurrences (0.2%)\n",
      "   â€¢ Valeurs nulles: 0\n",
      "\n",
      "ðŸ“ˆ DerniÃ¨res valeurs SuperTrend:\n",
      "   â€¢ 2019-11-29 12:00:00: 1.0\n",
      "   â€¢ 2019-11-29 16:00:00: 1.0\n",
      "   â€¢ 2019-11-29 20:00:00: 1.0\n",
      "   â€¢ 2019-11-30 00:00:00: 1.0\n",
      "   â€¢ 2019-11-30 04:00:00: 1.0\n",
      "   â€¢ 2019-11-30 08:00:00: 1.0\n",
      "   â€¢ 2019-11-30 12:00:00: 1.0\n",
      "   â€¢ 2019-11-30 16:00:00: 1.0\n",
      "   â€¢ 2019-11-30 20:00:00: 1.0\n",
      "   â€¢ 2019-12-01 00:00:00: 1.0\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "# ðŸ” DIAGNOSTIC SUPERTREND SPÃ‰CIFIQUE\n",
    "print(\"ðŸ” DIAGNOSTIC SUPERTREND\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "supertrend_dir_col = config.get_indicator_columns()[\"supertrend_dir\"]\n",
    "\n",
    "# Analyser les valeurs SuperTrend de faÃ§on simplifiÃ©e\n",
    "unique_values = sample_df.select(pl.col(supertrend_dir_col).unique()).to_series().to_list()\n",
    "print(f\"ðŸ“Š Valeurs uniques de {supertrend_dir_col}:\")\n",
    "for value in unique_values:\n",
    "    if value is not None:\n",
    "        count = sample_df.filter(pl.col(supertrend_dir_col) == value).height\n",
    "        percentage = (count / len(sample_df)) * 100\n",
    "        print(f\"   â€¢ Valeur {value}: {count:,} occurrences ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        null_count = sample_df.filter(pl.col(supertrend_dir_col).is_null()).height\n",
    "        print(f\"   â€¢ Valeur NULL: {null_count:,} occurrences ({null_count/len(sample_df)*100:.1f}%)\")\n",
    "\n",
    "# VÃ©rifier s'il y a des NaN/null\n",
    "null_count = sample_df.select(pl.col(supertrend_dir_col).is_null().sum()).item()\n",
    "print(f\"   â€¢ Valeurs nulles: {null_count:,}\")\n",
    "\n",
    "# VÃ©rifier les derniÃ¨res valeurs pour tendance rÃ©cente\n",
    "recent_supertrend = sample_df.tail(100).select([\n",
    "    pl.col('datetime'),\n",
    "    pl.col(supertrend_dir_col)\n",
    "]).tail(10)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DerniÃ¨res valeurs SuperTrend:\")\n",
    "for row in recent_supertrend.to_dicts():\n",
    "    print(f\"   â€¢ {row['datetime']}: {row[supertrend_dir_col]}\")\n",
    "\n",
    "print(\"=\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b81e8f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ TEST STRATÃ‰GIE SIMPLIFIÃ‰E\n",
      "==============================\n",
      "ðŸ“Š RÃ‰SULTATS STRATÃ‰GIE SIMPLIFIÃ‰E:\n",
      "   â€¢ Croisements EMA bullish: 73\n",
      "   â€¢ RSI OK (30-70): 4,194 (83.9%)\n",
      "   â€¢ ðŸŽ¯ Signaux d'ACHAT: 69\n",
      "   â€¢ ðŸŽ¯ Signaux de VENTE: 72\n",
      "\n",
      "âœ… SUCCÃˆS ! La stratÃ©gie simplifiÃ©e gÃ©nÃ¨re des signaux.\n",
      "ðŸ’¡ Le problÃ¨me vient bien du SuperTrend qui contient uniquement des NaN\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ STRATÃ‰GIE SIMPLIFIÃ‰E SANS SUPERTREND (TEST)\n",
    "print(\"ðŸš€ TEST STRATÃ‰GIE SIMPLIFIÃ‰E\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Modifier temporairement la stratÃ©gie pour ignorer SuperTrend\n",
    "def compute_simple_strategy_signals(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Version simplifiÃ©e sans SuperTrend pour test\"\"\"\n",
    "    \n",
    "    ema_fast_col = config.get_indicator_columns()[\"ema_fast\"]\n",
    "    ema_slow_col = config.get_indicator_columns()[\"ema_slow\"]\n",
    "    rsi_col = config.get_indicator_columns()[\"rsi_14\"]\n",
    "    \n",
    "    # Calcul des signaux simplifiÃ©s (sans SuperTrend)\n",
    "    signals_df = df.with_columns([\n",
    "        # === CONDITIONS EMA ===\n",
    "        (pl.col(ema_fast_col) > pl.col(ema_slow_col)).alias(\"ema_fast_above_slow\"),\n",
    "        (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)).alias(\"ema_was_below_or_equal\"),\n",
    "        \n",
    "        # Crossover EMA avec validation de continuitÃ©\n",
    "        ((pl.col(ema_fast_col) > pl.col(ema_slow_col)) & \n",
    "         (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)) &\n",
    "         (pl.int_range(pl.len()) > 0)).alias(\"ema_bullish_cross\"),\n",
    "        \n",
    "        # === CONDITIONS RSI (assouplies) ===\n",
    "        ((pl.col(rsi_col) >= 30) & (pl.col(rsi_col) <= 70)).alias(\"rsi_ok\"),\n",
    "        \n",
    "        # === SIGNAL SIMPLIFIÃ‰ ===\n",
    "        # Achat : Croisement EMA bullish + RSI OK\n",
    "        ((pl.col(ema_fast_col) > pl.col(ema_slow_col)) & \n",
    "         (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)) &\n",
    "         (pl.int_range(pl.len()) > 0) &\n",
    "         (pl.col(rsi_col) >= 30) & (pl.col(rsi_col) <= 70)).alias(\"buy_signal\"),\n",
    "        \n",
    "        # Vente : Croisement EMA bearish\n",
    "        ((pl.col(ema_fast_col) <= pl.col(ema_slow_col)) & \n",
    "         (pl.col(ema_fast_col).shift(1) > pl.col(ema_slow_col).shift(1)) &\n",
    "         (pl.int_range(pl.len()) > 0)).alias(\"sell_signal\")\n",
    "    ])\n",
    "    \n",
    "    return signals_df\n",
    "\n",
    "# Test sur Ã©chantillon\n",
    "test_simple = compute_simple_strategy_signals(sample_df)\n",
    "\n",
    "buy_count_simple = test_simple.select(pl.col(\"buy_signal\").sum()).item()\n",
    "sell_count_simple = test_simple.select(pl.col(\"sell_signal\").sum()).item()\n",
    "ema_cross_count = test_simple.select(pl.col(\"ema_bullish_cross\").sum()).item()\n",
    "rsi_ok_count = test_simple.select(pl.col(\"rsi_ok\").sum()).item()\n",
    "\n",
    "print(f\"ðŸ“Š RÃ‰SULTATS STRATÃ‰GIE SIMPLIFIÃ‰E:\")\n",
    "print(f\"   â€¢ Croisements EMA bullish: {ema_cross_count:,}\")\n",
    "print(f\"   â€¢ RSI OK (30-70): {rsi_ok_count:,} ({rsi_ok_count/len(sample_df)*100:.1f}%)\")  \n",
    "print(f\"   â€¢ ðŸŽ¯ Signaux d'ACHAT: {buy_count_simple:,}\")\n",
    "print(f\"   â€¢ ðŸŽ¯ Signaux de VENTE: {sell_count_simple:,}\")\n",
    "\n",
    "if buy_count_simple > 0:\n",
    "    print(f\"\\nâœ… SUCCÃˆS ! La stratÃ©gie simplifiÃ©e gÃ©nÃ¨re des signaux.\")\n",
    "    print(f\"ðŸ’¡ Le problÃ¨me vient bien du SuperTrend qui contient uniquement des NaN\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ MÃªme la stratÃ©gie simplifiÃ©e ne gÃ©nÃ¨re pas de signaux.\")\n",
    "    print(f\"ðŸ’¡ VÃ©rifier les donnÃ©es EMA et RSI\")\n",
    "\n",
    "print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "255aca40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ DÃ‰MARRAGE DU BACKTESTING CHUNKED AVEC VALIDATION\n",
      "============================================================\n",
      "ðŸ“Š Total Ã  traiter: 5,844 lignes\n",
      "ðŸ”„ Chunks estimÃ©s: 1\n",
      "ðŸ›¡ï¸ Buffer de contexte: 100 lignes\n",
      "â±ï¸ DÃ©but: 00:40:37\n",
      "\n",
      "[  1/1] Chunk 0-5,844 |  5844 lignes | ðŸ“ˆ 10 achats | ðŸ“‰ 66 ventes | âš¡  13145 l/s | âœ… no-ctx | ðŸ”„   1 trades\n",
      "\n",
      "============================================================\n",
      "âœ… BACKTESTING CHUNKED TERMINÃ‰ AVEC VALIDATION\n",
      "============================================================\n",
      "ðŸ“Š Lignes traitÃ©es: 5,844\n",
      "ðŸ“ˆ Total signaux achat: 10\n",
      "ðŸ“‰ Total signaux vente: 66\n",
      "ðŸ”„ Total trades complÃ©tÃ©s: 1\n",
      "â±ï¸ Temps total: 0.4s\n",
      "âš¡ Performance: 13,140 lignes/sec\n",
      "\n",
      "ðŸ›¡ï¸ RAPPORT DE VALIDATION:\n",
      "   â€¢ Chunks traitÃ©s: 1\n",
      "   â€¢ ProblÃ¨mes de continuitÃ©: 0\n",
      "   â€¢ Warnings total: 0\n",
      "   âœ… CohÃ©rence parfaite - tous les signaux sont fiables\n",
      "|  5844 lignes | ðŸ“ˆ 10 achats | ðŸ“‰ 66 ventes | âš¡  13145 l/s | âœ… no-ctx | ðŸ”„   1 trades\n",
      "\n",
      "============================================================\n",
      "âœ… BACKTESTING CHUNKED TERMINÃ‰ AVEC VALIDATION\n",
      "============================================================\n",
      "ðŸ“Š Lignes traitÃ©es: 5,844\n",
      "ðŸ“ˆ Total signaux achat: 10\n",
      "ðŸ“‰ Total signaux vente: 66\n",
      "ðŸ”„ Total trades complÃ©tÃ©s: 1\n",
      "â±ï¸ Temps total: 0.4s\n",
      "âš¡ Performance: 13,140 lignes/sec\n",
      "\n",
      "ðŸ›¡ï¸ RAPPORT DE VALIDATION:\n",
      "   â€¢ Chunks traitÃ©s: 1\n",
      "   â€¢ ProblÃ¨mes de continuitÃ©: 0\n",
      "   â€¢ Warnings total: 0\n",
      "   âœ… CohÃ©rence parfaite - tous les signaux sont fiables\n"
     ]
    }
   ],
   "source": [
    "def run_chunked_backtest() -> List[Dict]:\n",
    "    \"\"\"ExÃ©cute le backtesting par chunks sur toutes les donnÃ©es avec validation de cohÃ©rence\"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ DÃ‰MARRAGE DU BACKTESTING CHUNKED AVEC VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # RÃ©cupÃ©rer le nombre total de lignes\n",
    "    if not data_summary:\n",
    "        print(\"âŒ Pas d'informations sur les donnÃ©es\")\n",
    "        return []\n",
    "    \n",
    "    total_rows = data_summary['total_rows']\n",
    "    estimated_chunks = (total_rows // config.chunk_size) + 1\n",
    "    \n",
    "    print(f\"ðŸ“Š Total Ã  traiter: {total_rows:,} lignes\")\n",
    "    print(f\"ðŸ”„ Chunks estimÃ©s: {estimated_chunks:,}\")\n",
    "    print(f\"ðŸ›¡ï¸ Buffer de contexte: {backtester.state['context_buffer_size']} lignes\")\n",
    "    print(f\"â±ï¸ DÃ©but: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print()\n",
    "    \n",
    "    all_results = []\n",
    "    start_time = datetime.now()\n",
    "    total_warnings = 0\n",
    "    continuity_issues = 0\n",
    "    \n",
    "    # Traitement chunk par chunk\n",
    "    for offset in range(0, total_rows, config.chunk_size):\n",
    "        chunk_num = (offset // config.chunk_size) + 1\n",
    "        current_chunk_size = min(config.chunk_size, total_rows - offset)\n",
    "        \n",
    "        print(f\"[{chunk_num:>3}/{estimated_chunks}] \", end=\"\")\n",
    "        print(f\"Chunk {offset:,}-{offset + current_chunk_size:,} \", end=\"\")\n",
    "        \n",
    "        try:\n",
    "            # Charger le chunk avec contexte\n",
    "            chunk_start = datetime.now()\n",
    "            chunk_df = backtester.load_chunk_with_context(offset, current_chunk_size)\n",
    "            \n",
    "            if len(chunk_df) == 0:\n",
    "                print(\"âš ï¸ Chunk vide - arrÃªt\")\n",
    "                break\n",
    "            \n",
    "            # Traiter le chunk\n",
    "            is_first = (offset == 0)\n",
    "            chunk_result = backtester.process_chunk(chunk_df, is_first)\n",
    "            \n",
    "            # VÃ©rifier les erreurs\n",
    "            if 'error' in chunk_result:\n",
    "                print(f\"âŒ Erreur: {chunk_result['error']}\")\n",
    "                break\n",
    "            \n",
    "            # Calculer les mÃ©triques du chunk\n",
    "            chunk_time = (datetime.now() - chunk_start).total_seconds()\n",
    "            rows_per_sec = chunk_result['rows_processed'] / max(chunk_time, 0.001)\n",
    "            \n",
    "            # Affichage des mÃ©triques avec validation\n",
    "            continuity_status = \"âœ…\" if chunk_result.get('continuity_validated', False) else \"âš ï¸\"\n",
    "            context_info = f\"ctx:{chunk_result.get('context_size', 0)}\" if chunk_result.get('context_size', 0) > 0 else \"no-ctx\"\n",
    "            \n",
    "            print(f\"| {chunk_result['rows_processed']:>5} lignes \", end=\"\")\n",
    "            print(f\"| ðŸ“ˆ {chunk_result['buy_signals']:>2} achats \", end=\"\")\n",
    "            print(f\"| ðŸ“‰ {chunk_result['sell_signals']:>2} ventes \", end=\"\")\n",
    "            print(f\"| âš¡ {rows_per_sec:>6.0f} l/s \", end=\"\")\n",
    "            print(f\"| {continuity_status} {context_info} \", end=\"\")\n",
    "            print(f\"| ðŸ”„ {backtester.state['total_trades']:>3} trades\")\n",
    "            \n",
    "            # Gestion des warnings\n",
    "            if 'warnings' in chunk_result and chunk_result['warnings']:\n",
    "                for warning in chunk_result['warnings']:\n",
    "                    print(f\"    âš ï¸ {warning}\")\n",
    "                    total_warnings += 1\n",
    "            \n",
    "            if not chunk_result.get('continuity_validated', False):\n",
    "                continuity_issues += 1\n",
    "            \n",
    "            all_results.append(chunk_result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Statistiques finales avec validation\n",
    "    total_time = (datetime.now() - start_time).total_seconds()\n",
    "    total_processed = sum(r['rows_processed'] for r in all_results)\n",
    "    total_buy_signals = sum(r['buy_signals'] for r in all_results)\n",
    "    total_sell_signals = sum(r['sell_signals'] for r in all_results)\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"âœ… BACKTESTING CHUNKED TERMINÃ‰ AVEC VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ðŸ“Š Lignes traitÃ©es: {total_processed:,}\")\n",
    "    print(f\"ðŸ“ˆ Total signaux achat: {total_buy_signals:,}\")\n",
    "    print(f\"ðŸ“‰ Total signaux vente: {total_sell_signals:,}\")\n",
    "    print(f\"ðŸ”„ Total trades complÃ©tÃ©s: {backtester.state['total_trades']:,}\")\n",
    "    print(f\"â±ï¸ Temps total: {total_time:.1f}s\")\n",
    "    print(f\"âš¡ Performance: {total_processed/max(total_time, 0.001):,.0f} lignes/sec\")\n",
    "    \n",
    "    # Rapport de validation\n",
    "    print(f\"\\nðŸ›¡ï¸ RAPPORT DE VALIDATION:\")\n",
    "    print(f\"   â€¢ Chunks traitÃ©s: {len(all_results):,}\")\n",
    "    print(f\"   â€¢ ProblÃ¨mes de continuitÃ©: {continuity_issues:,}\")\n",
    "    print(f\"   â€¢ Warnings total: {total_warnings:,}\")\n",
    "    \n",
    "    if continuity_issues == 0 and total_warnings == 0:\n",
    "        print(\"   âœ… CohÃ©rence parfaite - tous les signaux sont fiables\")\n",
    "    elif continuity_issues > 0:\n",
    "        print(f\"   âš ï¸ {continuity_issues} chunks avec problÃ¨mes de continuitÃ©\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# ExÃ©cution du backtesting avec validation\n",
    "backtest_results = run_chunked_backtest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f2fc8",
   "metadata": {},
   "source": [
    "## 5. ðŸ’¾ Sauvegarde des RÃ©sultats dans Table Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89a96af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ SAUVEGARDE DES RÃ‰SULTATS\n",
      "==============================\n",
      "ðŸ“Š DonnÃ©es combinÃ©es: 5,844 lignes\n",
      "ðŸ“… PÃ©riode: 2023-01-01 00:00:00 â†’ 2025-08-31 20:00:00\n",
      "âœ… Sauvegarde locale: /tmp/backtest_BTCUSDT_20251004_004046.parquet\n",
      "âš ï¸ MinIO non disponible: Invalid Error: Unexpected response while initializing S3 multipart upload\n",
      "ðŸ“ Utilisation sauvegarde locale: /tmp/backtest_BTCUSDT_20251004_004046.parquet\n",
      "âœ… RÃ©sultats sauvegardÃ©s: /tmp/backtest_BTCUSDT_20251004_004046.parquet\n",
      "ðŸ“ Taille: ~1.6 MB\n",
      "âŒ Erreur lors de la sauvegarde: Invalid Error: Unexpected response while initializing S3 multipart upload\n"
     ]
    }
   ],
   "source": [
    "def save_results_to_test_table(results: List[Dict]) -> bool:\n",
    "    \"\"\"Sauvegarde les rÃ©sultats dans la table test pour analyse VectorBT\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"âŒ Pas de rÃ©sultats Ã  sauvegarder\")\n",
    "        return False\n",
    "    \n",
    "    print(\"ðŸ’¾ SAUVEGARDE DES RÃ‰SULTATS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    try:\n",
    "        # Combiner tous les DataFrames de rÃ©sultats\n",
    "        all_data = []\n",
    "        for result in results:\n",
    "            if 'data' in result and result['data'] is not None:\n",
    "                # Ajouter les mÃ©tadonnÃ©es du chunk\n",
    "                chunk_data = result['data'].with_columns([\n",
    "                    pl.lit(result['chunk_id']).alias('chunk_id'),\n",
    "                    pl.lit(datetime.now().isoformat()).alias('backtest_timestamp'),\n",
    "                    pl.lit(config.symbol).alias('symbol'),\n",
    "                    pl.lit(f\"{config.strategy_name if hasattr(config, 'strategy_name') else 'smart_momentum'}\").alias('strategy_name')\n",
    "                ])\n",
    "                all_data.append(chunk_data)\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"âŒ Pas de donnÃ©es Ã  combiner\")\n",
    "            return False\n",
    "        \n",
    "        # Combiner toutes les donnÃ©es\n",
    "        final_df = pl.concat(all_data)\n",
    "        \n",
    "        print(f\"ðŸ“Š DonnÃ©es combinÃ©es: {len(final_df):,} lignes\")\n",
    "        print(f\"ðŸ“… PÃ©riode: {final_df['datetime'].min()} â†’ {final_df['datetime'].max()}\")\n",
    "        \n",
    "        # GÃ©nÃ©rer le chemin de sortie avec timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = f\"{config.test_table_path}backtest_{config.symbol}_{timestamp}.parquet\"\n",
    "        \n",
    "        # Sauvegarder localement en attendant la correction MinIO\n",
    "        local_output_path = f\"/tmp/backtest_{config.symbol}_{timestamp}.parquet\"\n",
    "        \n",
    "        # Sauvegarder en local d'abord\n",
    "        final_df.write_parquet(local_output_path, compression='snappy')\n",
    "        print(f\"âœ… Sauvegarde locale: {local_output_path}\")\n",
    "        \n",
    "        # Essayer MinIO en option\n",
    "        try:\n",
    "            temp_table = \"temp_backtest_results\"\n",
    "            data_loader.con.register(temp_table, final_df.to_arrow())\n",
    "            \n",
    "            export_query = f\"\"\"\n",
    "                COPY (SELECT * FROM {temp_table})\n",
    "                TO '{output_path}'\n",
    "                (FORMAT PARQUET, COMPRESSION 'snappy')\n",
    "            \"\"\"\n",
    "            data_loader.con.execute(export_query)\n",
    "            print(f\"âœ… Sauvegarde MinIO: {output_path}\")\n",
    "        except Exception as minio_error:\n",
    "            print(f\"âš ï¸ MinIO non disponible: {minio_error}\")\n",
    "            print(f\"ðŸ“ Utilisation sauvegarde locale: {local_output_path}\")\n",
    "            output_path = local_output_path\n",
    "        \n",
    "        print(f\"âœ… RÃ©sultats sauvegardÃ©s: {output_path}\")\n",
    "        print(f\"ðŸ“ Taille: ~{final_df.estimated_size('mb'):.1f} MB\")\n",
    "        \n",
    "        # Sauvegarder Ã©galement les mÃ©tadonnÃ©es\n",
    "        metadata = {\n",
    "            'config': {\n",
    "                'symbol': config.symbol,\n",
    "                'chunk_size': config.chunk_size,\n",
    "                'start_date': config.start_date,\n",
    "                'end_date': config.end_date,\n",
    "                'initial_cash': config.initial_cash,\n",
    "                'fees': config.fees\n",
    "            },\n",
    "            'results': {\n",
    "                'total_rows': len(final_df),\n",
    "                'total_chunks': len(results),\n",
    "                'total_buy_signals': sum(r['buy_signals'] for r in results),\n",
    "                'total_sell_signals': sum(r['sell_signals'] for r in results),\n",
    "                'total_trades': backtester.state['total_trades']\n",
    "            },\n",
    "            'paths': {\n",
    "                'data_path': output_path,\n",
    "                'source_path': config.feature_store_path\n",
    "            },\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        metadata_path = f\"{config.test_table_path}metadata_{config.symbol}_{timestamp}.json\"\n",
    "        \n",
    "        # Sauvegarder les mÃ©tadonnÃ©es (mÃ©thode simplifiÃ©e)\n",
    "        metadata_df = pl.DataFrame([metadata])\n",
    "        data_loader.con.register(\"temp_metadata\", metadata_df.to_arrow())\n",
    "        data_loader.con.execute(f\"\"\"\n",
    "            COPY (SELECT * FROM temp_metadata)\n",
    "            TO '{metadata_path.replace('.json', '.parquet')}'\n",
    "            (FORMAT PARQUET)\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"ðŸ“‹ MÃ©tadonnÃ©es sauvegardÃ©es: {metadata_path.replace('.json', '.parquet')}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur lors de la sauvegarde: {e}\")\n",
    "        return False\n",
    "\n",
    "# Sauvegarder les rÃ©sultats\n",
    "save_success = save_results_to_test_table(backtest_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d8beb",
   "metadata": {},
   "source": [
    "## 6. ðŸ“ˆ Validation et Analyse avec VectorBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cea9a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ ANALYSE VECTORBT\n",
      "====================\n",
      "ðŸ“Š DonnÃ©es pour VectorBT: 5,844 lignes\n",
      "ðŸ“… PÃ©riode: 2023-01-01 00:00:00 â†’ 2025-08-31 20:00:00\n",
      "\n",
      "ðŸ“Š RÃ‰SULTATS VECTORBT:\n",
      "ðŸ’° Capital initial: $10,000.00\n",
      "ðŸ’° Capital final: $13,705.34\n",
      "ðŸ“ˆ Rendement total: 37.05%\n",
      "ðŸ”„ Nombre de trades: 10\n",
      "ðŸ’¹ Trade moyen: 370.53\n",
      "ðŸŽ¯ Taux de rÃ©ussite: 40.0%\n",
      "ðŸ“‰ Drawdown max: 0.00%\n",
      "ðŸ“‰ Drawdown max: 0.00%\n",
      "\n",
      "ðŸ“Š STATISTIQUES AVANCÃ‰ES:\n",
      "Start                         2023-01-01 00:00:00\n",
      "End                           2025-08-31 20:00:00\n",
      "Period                          974 days 00:00:00\n",
      "Start Value                               10000.0\n",
      "End Value                            13705.338196\n",
      "Total Return [%]                        37.053382\n",
      "Benchmark Return [%]                   554.727443\n",
      "Max Gross Exposure [%]                      100.0\n",
      "Total Fees Paid                        236.514826\n",
      "Max Drawdown [%]                        11.481144\n",
      "Max Drawdown Duration           319 days 16:00:00\n",
      "Total Trades                                   10\n",
      "Total Closed Trades                            10\n",
      "Total Open Trades                               0\n",
      "Open Trade PnL                                0.0\n",
      "Win Rate [%]                                 40.0\n",
      "Best Trade [%]                          18.690917\n",
      "Worst Trade [%]                         -3.870799\n",
      "Avg Winning Trade [%]                   12.991374\n",
      "Avg Losing Trade [%]                    -2.765411\n",
      "Avg Winning Trade Duration       14 days 20:00:00\n",
      "Avg Losing Trade Duration         2 days 18:00:00\n",
      "Profit Factor                            2.807848\n",
      "Expectancy                              370.53382\n",
      "Sharpe Ratio                             1.020706\n",
      "Calmar Ratio                             1.092036\n",
      "Omega Ratio                              1.273007\n",
      "Sortino Ratio                            1.452173\n",
      "dtype: object\n",
      "\n",
      "ðŸ“Š STATISTIQUES AVANCÃ‰ES:\n",
      "Start                         2023-01-01 00:00:00\n",
      "End                           2025-08-31 20:00:00\n",
      "Period                          974 days 00:00:00\n",
      "Start Value                               10000.0\n",
      "End Value                            13705.338196\n",
      "Total Return [%]                        37.053382\n",
      "Benchmark Return [%]                   554.727443\n",
      "Max Gross Exposure [%]                      100.0\n",
      "Total Fees Paid                        236.514826\n",
      "Max Drawdown [%]                        11.481144\n",
      "Max Drawdown Duration           319 days 16:00:00\n",
      "Total Trades                                   10\n",
      "Total Closed Trades                            10\n",
      "Total Open Trades                               0\n",
      "Open Trade PnL                                0.0\n",
      "Win Rate [%]                                 40.0\n",
      "Best Trade [%]                          18.690917\n",
      "Worst Trade [%]                         -3.870799\n",
      "Avg Winning Trade [%]                   12.991374\n",
      "Avg Losing Trade [%]                    -2.765411\n",
      "Avg Winning Trade Duration       14 days 20:00:00\n",
      "Avg Losing Trade Duration         2 days 18:00:00\n",
      "Profit Factor                            2.807848\n",
      "Expectancy                              370.53382\n",
      "Sharpe Ratio                             1.020706\n",
      "Calmar Ratio                             1.092036\n",
      "Omega Ratio                              1.273007\n",
      "Sortino Ratio                            1.452173\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def analyze_with_vectorbt() -> Optional[vbt.Portfolio]:\n",
    "    \"\"\"Analyse des rÃ©sultats avec VectorBT\"\"\"\n",
    "    \n",
    "    if not backtest_results:\n",
    "        print(\"âŒ Pas de rÃ©sultats Ã  analyser\")\n",
    "        return None\n",
    "    \n",
    "    print(\"ðŸ“ˆ ANALYSE VECTORBT\")\n",
    "    print(\"=\" * 20)\n",
    "    \n",
    "    try:\n",
    "        # Combiner toutes les donnÃ©es pour VectorBT\n",
    "        all_data = []\n",
    "        for result in backtest_results:\n",
    "            if 'data' in result and result['data'] is not None:\n",
    "                all_data.append(result['data'])\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"âŒ Pas de donnÃ©es Ã  analyser\")\n",
    "            return None\n",
    "        \n",
    "        # Combiner et convertir en pandas pour VectorBT\n",
    "        combined_df = pl.concat(all_data)\n",
    "        df_pd = combined_df.to_pandas().set_index('datetime')\n",
    "        \n",
    "        print(f\"ðŸ“Š DonnÃ©es pour VectorBT: {len(df_pd):,} lignes\")\n",
    "        print(f\"ðŸ“… PÃ©riode: {df_pd.index.min()} â†’ {df_pd.index.max()}\")\n",
    "        \n",
    "        # CrÃ©er le portfolio VectorBT\n",
    "        portfolio = vbt.Portfolio.from_signals(\n",
    "            close=df_pd['close'],\n",
    "            entries=df_pd['buy_signal'],\n",
    "            exits=df_pd['sell_signal'],\n",
    "            init_cash=config.initial_cash,\n",
    "            fees=config.fees,\n",
    "            freq='4H'  # Ajuster selon vos donnÃ©es\n",
    "        )\n",
    "        \n",
    "        # Statistiques de base\n",
    "        print(f\"\\nðŸ“Š RÃ‰SULTATS VECTORBT:\")\n",
    "        print(f\"ðŸ’° Capital initial: ${config.initial_cash:,.2f}\")\n",
    "        print(f\"ðŸ’° Capital final: ${portfolio.final_value():,.2f}\")\n",
    "        print(f\"ðŸ“ˆ Rendement total: {(portfolio.final_value() / config.initial_cash - 1) * 100:.2f}%\")\n",
    "        print(f\"ðŸ”„ Nombre de trades: {portfolio.trades.count()}\")\n",
    "        \n",
    "        if portfolio.trades.count() > 0:\n",
    "            print(f\"ðŸ’¹ Trade moyen: {portfolio.trades.pnl.mean():.2f}\")\n",
    "            print(f\"ðŸŽ¯ Taux de rÃ©ussite: {portfolio.trades.win_rate() * 100:.1f}%\")\n",
    "            try:\n",
    "                print(f\"ðŸ“‰ Drawdown max: {portfolio.drawdown().max() * 100:.2f}%\")\n",
    "            except:\n",
    "                print(f\"ðŸ“‰ Drawdown max: N/A\")\n",
    "        \n",
    "        # Statistiques avancÃ©es\n",
    "        stats = portfolio.stats()\n",
    "        print(f\"\\nðŸ“Š STATISTIQUES AVANCÃ‰ES:\")\n",
    "        print(stats)\n",
    "        \n",
    "        return portfolio\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur lors de l'analyse VectorBT: {e}\")\n",
    "        return None\n",
    "\n",
    "# Analyse avec VectorBT\n",
    "portfolio = analyze_with_vectorbt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32d2c243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Impossible de gÃ©nÃ©rer les graphiques\n"
     ]
    }
   ],
   "source": [
    "# Visualisation des rÃ©sultats\n",
    "if portfolio is not None:\n",
    "    print(\"ðŸŽ¨ GÃ©nÃ©ration des graphiques...\")\n",
    "    \n",
    "    # Graphique principal du portfolio\n",
    "    fig = portfolio.plot()\n",
    "    fig.show()\n",
    "    \n",
    "    # Graphique des trades\n",
    "    if portfolio.trades.count() > 0:\n",
    "        trades_fig = portfolio.trades.plot()\n",
    "        trades_fig.show()\n",
    "    \n",
    "    print(\"âœ… Graphiques gÃ©nÃ©rÃ©s avec succÃ¨s\")\n",
    "else:\n",
    "    print(\"âŒ Impossible de gÃ©nÃ©rer les graphiques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260c15e",
   "metadata": {},
   "source": [
    "## 7. ðŸ“‹ RÃ©sumÃ© et Prochaines Ã‰tapes\n",
    "\n",
    "### âœ… Ce qui a Ã©tÃ© accompli\n",
    "- Backtesting chunked avec continuitÃ© des indicateurs\n",
    "- Traitement de gros volumes avec mÃ©moire constante\n",
    "- Sauvegarde des rÃ©sultats dans tables test\n",
    "- Analyse et validation avec VectorBT\n",
    "\n",
    "### ðŸš€ Prochaines Ã©tapes suggÃ©rÃ©es\n",
    "1. **Optimisation des paramÃ¨tres** : Utiliser les rÃ©sultats pour ajuster la stratÃ©gie\n",
    "2. **Backtesting multi-timeframes** : Tester sur diffÃ©rentes pÃ©riodes\n",
    "3. **StratÃ©gies avancÃ©es** : IntÃ©grer de nouvelles conditions\n",
    "4. **Production** : DÃ©ployer la stratÃ©gie validÃ©e\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a44fc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ RÃ‰SUMÃ‰ DU BACKTESTING CHUNKED\n",
      "\n",
      "ðŸ“Š DonnÃ©es traitÃ©es: 5,844 lignes en 1 chunks\n",
      "ðŸ“ˆ Signaux gÃ©nÃ©rÃ©s: 76\n",
      "ðŸ”„ Trades complÃ©tÃ©s: 1\n",
      "ðŸ’° Performance: 37.05%\n",
      "ðŸ“Š Sharpe Ratio: 1.02\n",
      "ðŸ’¾ RÃ©sultats sauvegardÃ©s: âŒ\n",
      "ðŸ“ˆ Analyse VectorBT: âœ…\n",
      "\n",
      "âœ… Backtesting terminÃ© avec succÃ¨s !\n",
      "ðŸš€ PrÃªt pour l'optimisation et la production\n"
     ]
    }
   ],
   "source": [
    "# RÃ©sumÃ© final\n",
    "print(\"\" * 60)\n",
    "print(\"ðŸŽ¯ RÃ‰SUMÃ‰ DU BACKTESTING CHUNKED\")\n",
    "print(\"\" * 60)\n",
    "\n",
    "if backtest_results:\n",
    "    total_processed = sum(r['rows_processed'] for r in backtest_results)\n",
    "    total_chunks = len(backtest_results)\n",
    "    total_signals = sum(r['buy_signals'] + r['sell_signals'] for r in backtest_results)\n",
    "    \n",
    "    print(f\"ðŸ“Š DonnÃ©es traitÃ©es: {total_processed:,} lignes en {total_chunks} chunks\")\n",
    "    print(f\"ðŸ“ˆ Signaux gÃ©nÃ©rÃ©s: {total_signals:,}\")\n",
    "    print(f\"ðŸ”„ Trades complÃ©tÃ©s: {backtester.state['total_trades']:,}\")\n",
    "    \n",
    "    if portfolio is not None:\n",
    "        print(f\"ðŸ’° Performance: {(portfolio.final_value() / config.initial_cash - 1) * 100:.2f}%\")\n",
    "        print(f\"ðŸ“Š Sharpe Ratio: {portfolio.sharpe_ratio():.2f}\" if hasattr(portfolio, 'sharpe_ratio') else \"\")\n",
    "    \n",
    "    print(f\"ðŸ’¾ RÃ©sultats sauvegardÃ©s: {'âœ…' if save_success else 'âŒ'}\")\n",
    "    print(f\"ðŸ“ˆ Analyse VectorBT: {'âœ…' if portfolio is not None else 'âŒ'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Aucun rÃ©sultat gÃ©nÃ©rÃ©\")\n",
    "\n",
    "print(\"\\nâœ… Backtesting terminÃ© avec succÃ¨s !\")\n",
    "print(\"ðŸš€ PrÃªt pour l'optimisation et la production\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hermes-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
