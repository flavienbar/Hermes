{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "226599df",
   "metadata": {},
   "source": [
    "# 🚀 Hermes - Backtesting Chunked de Stratégies\n",
    "\n",
    "## Architecture\n",
    "Ce notebook suit l'architecture Medallion de Hermes :\n",
    "- **Source** : Hub Features Gold (indicateurs pré-calculés)\n",
    "- **Traitement** : Chunks avec continuité pour gros volumes\n",
    "- **Analyse** : VectorBT pour validation des stratégies\n",
    "- **Sortie** : Table test pour résultats intermédiaires\n",
    "\n",
    "## Workflow\n",
    "1. **Configuration** : Imports et paramètres\n",
    "2. **Connexion Sources** : Hub Features Gold (indicateurs pré-calculés)\n",
    "3. **Stratégie Chunked** : Génération signaux par chunks\n",
    "4. **Validation VectorBT** : Analyse des performances\n",
    "\n",
    "**Important** : Tous les indicateurs doivent être pré-calculés dans le Hub Features Gold\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650fd35",
   "metadata": {},
   "source": [
    "## 1. 📦 Configuration et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16ef6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports chargés\n",
      "🐍 Python: 3.10.18\n",
      "📊 Polars: 0.20.31\n",
      "🧮 VectorBT: 0.25.5\n",
      "🦆 DuckDB: 0.9.2\n"
     ]
    }
   ],
   "source": [
    "# Imports essentiels\n",
    "import os\n",
    "import json\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import vectorbt as vbt\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"✅ Imports chargés\")\n",
    "print(f\"🐍 Python: {os.sys.version.split()[0]}\")\n",
    "print(f\"📊 Polars: {pl.__version__}\")\n",
    "print(f\"🧮 VectorBT: {vbt.__version__}\")\n",
    "print(f\"🦆 DuckDB: {duckdb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ea41c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Configuration initialisée\n",
      "📊 Symbole: BTCUSDT\n",
      "🔄 Chunk size: 50,000 lignes\n",
      "🛡️ Buffer contexte: 100 lignes\n",
      "📅 Période: 2023-01-01 → fin\n",
      "💰 Capital initial: $10,000.00\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class BacktestConfig:\n",
    "    \"\"\"Configuration pour le backtesting chunked\"\"\"\n",
    "    \n",
    "    # Source de données\n",
    "    symbol: str = \"BTCUSDT\"\n",
    "    feature_store_path: str = \"s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/**/*.parquet\"\n",
    "    \n",
    "    # Paramètres de chunking\n",
    "    chunk_size: int = 50_000  # Lignes par chunk\n",
    "    overlap_window: int = 100  # Lignes de contexte entre chunks\n",
    "    \n",
    "    # Fenêtre temporelle (optionnel - None = tout l'historique)\n",
    "    start_date: Optional[str] = \"2023-01-01\"  # Format: \"YYYY-MM-DD\" ou None\n",
    "    end_date: Optional[str] = None\n",
    "    \n",
    "    # Paramètres de stratégie par défaut\n",
    "    rsi_oversold: int = 30\n",
    "    rsi_neutral_low: int = 45\n",
    "    rsi_neutral_high: int = 55\n",
    "    ema_fast: int = 12\n",
    "    ema_slow: int = 26\n",
    "    supertrend_period: int = 10\n",
    "    supertrend_multiplier: float = 3.0\n",
    "\n",
    "    # Buffer contexte - calculé automatiquement\n",
    "    min_context_buffer: int = 50  # Minimum de sécurité\n",
    "\n",
    "    def get_required_context_size(self) -> int:\n",
    "        \"\"\"Calcule la taille de contexte requise selon les indicateurs\"\"\"\n",
    "        # Prendre le plus grand indicateur + marge de sécurité\n",
    "        max_indicator_period = max([\n",
    "            self.ema_fast,\n",
    "            self.ema_slow, \n",
    "            self.supertrend_period,\n",
    "            14,  # RSI par défaut\n",
    "            20,  # Bollinger Bands par défaut\n",
    "            26   # MACD par défaut\n",
    "        ])\n",
    "\n",
    "        # Ajouter une marge de sécurité (50% du plus grand indicateur)\n",
    "        safety_margin = int(max_indicator_period * 0.5)\n",
    "        required_size = max_indicator_period + safety_margin\n",
    "        \n",
    "        # S'assurer d'avoir au moins le minimum\n",
    "        return max(required_size, self.min_context_buffer, self.overlap_window)\n",
    "    \n",
    "    # Backtesting\n",
    "    initial_cash: float = 10000.0\n",
    "    fees: float = 0.001  # 0.1%\n",
    "    \n",
    "    # MinIO\n",
    "    minio_endpoint: str = \"127.0.0.1:9000\"\n",
    "    minio_access_key: str = \"minioadm\"\n",
    "    minio_secret_key: str = \"minioadm\"\n",
    "    \n",
    "    # Sortie\n",
    "    test_table_path: str = \"s3://test/backtest_results/\"\n",
    "    \n",
    "    def get_indicator_columns(self) -> Dict[str, str]:\n",
    "        \"\"\"Mapping des colonnes d'indicateurs\"\"\"\n",
    "        return {\n",
    "            \"ema_fast\": f\"ema_{self.ema_fast}\",\n",
    "            \"ema_slow\": f\"ema_{self.ema_slow}\",\n",
    "            \"rsi_14\": \"rsi_14\",\n",
    "            \"supertrend\": f\"supertrend_{self.supertrend_period}_{self.supertrend_multiplier}\",\n",
    "            \"supertrend_dir\": f\"supertrend_dir_{self.supertrend_period}_{self.supertrend_multiplier}\",\n",
    "            \"bb_upper\": \"bb_upper_20_2\",\n",
    "            \"bb_middle\": \"bb_middle_20_2\", \n",
    "            \"bb_lower\": \"bb_lower_20_2\",\n",
    "            \"macd\": \"macd_12_26_9\",\n",
    "            \"macd_signal\": \"macd_signal_12_26_9\",\n",
    "            \"atr_14\": \"atr_14\"\n",
    "        }\n",
    "\n",
    "# Configuration par défaut\n",
    "config = BacktestConfig()\n",
    "\n",
    "print(\"⚙️ Configuration initialisée\")\n",
    "print(f\"📊 Symbole: {config.symbol}\")\n",
    "print(f\"🔄 Chunk size: {config.chunk_size:,} lignes\")\n",
    "print(f\"🛡️ Buffer contexte: {config.get_required_context_size()} lignes\")\n",
    "print(f\"📅 Période: {config.start_date} → {config.end_date or 'fin'}\")\n",
    "print(f\"💰 Capital initial: ${config.initial_cash:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2026e64",
   "metadata": {},
   "source": [
    "## 2. 🔌 Connexion aux Sources de Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc991dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 TEST DE CONNEXION ET VALIDATION DU CHEMIN\n",
      "=============================================\n",
      "🔌 Configuration connexion DuckDB → MinIO...\n",
      "✅ Connexion DuckDB configurée\n",
      "\n",
      "🧪 Test 1: s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/**/*.parquet\n",
      "   ✅ 5 fichier(s) trouvé(s)\n",
      "     📁 s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=10/data_0.parquet\n",
      "     📁 s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=11/data_0.parquet\n",
      "     📁 s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=12/data_0.parquet\n",
      "     ... et 2 autres\n",
      "   📊 Test lecture: 17,604 lignes, 1 symbole(s)\n",
      "   🎯 Chemin optimal trouvé !\n",
      "\n",
      "🎯 Chemin final utilisé: s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/**/*.parquet\n",
      "\n",
      "=============================================\n",
      "📊 Analyse des partitions disponibles...\n",
      "📁 97 partitions trouvées\n",
      "📁 Fichiers trouvés:\n",
      "   • s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=10/data_0.parquet\n",
      "   • s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=11/data_0.parquet\n",
      "   • s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=12/data_0.parquet\n",
      "   • s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=8/data_0.parquet\n",
      "   • s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/year=2017/month=9/data_0.parquet\n",
      "   • ... et 92 autres\n",
      "📈 Analyse du contenu des données...\n",
      "📊 Résumé des données:\n",
      "   • Total lignes: 5,844\n",
      "   • Période: 2023-01-01 00:00:00 → 2025-08-31 20:00:00\n",
      "   • Jours uniques: 974\n",
      "   • Chunks estimés: 1\n",
      "📊 Résumé des données:\n",
      "   • Total lignes: 5,844\n",
      "   • Période: 2023-01-01 00:00:00 → 2025-08-31 20:00:00\n",
      "   • Jours uniques: 974\n",
      "   • Chunks estimés: 1\n"
     ]
    }
   ],
   "source": [
    "class HermesDataLoader:\n",
    "    \"\"\"Gestionnaire de connexion aux données Hermes\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BacktestConfig):\n",
    "        self.config = config\n",
    "        self.con = None\n",
    "        self.indicator_cols = config.get_indicator_columns()\n",
    "        \n",
    "        # Colonnes de base OHLCV\n",
    "        self.price_cols = ['datetime', 'open', 'high', 'low', 'close', 'volume']\n",
    "        \n",
    "    def setup_connection(self):\n",
    "        \"\"\"Configure la connexion DuckDB vers MinIO\"\"\"\n",
    "        print(\"🔌 Configuration connexion DuckDB → MinIO...\")\n",
    "        \n",
    "        self.con = duckdb.connect()\n",
    "        \n",
    "        # Configuration S3/MinIO\n",
    "        self.con.execute(f\"\"\"\n",
    "            SET s3_access_key_id='{self.config.minio_access_key}';\n",
    "            SET s3_secret_access_key='{self.config.minio_secret_key}';\n",
    "            SET s3_endpoint='{self.config.minio_endpoint}';\n",
    "            SET s3_url_style='path';\n",
    "            SET s3_use_ssl='false';\n",
    "        \"\"\")\n",
    "        \n",
    "        # Optimisations mémoire\n",
    "        self.con.execute(\"\"\"\n",
    "            SET threads TO 6;\n",
    "            SET memory_limit = '4GB';\n",
    "            SET enable_progress_bar = true;\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"✅ Connexion DuckDB configurée\")\n",
    "    \n",
    "    def get_partition_info(self) -> List[Dict]:\n",
    "        \"\"\"Récupère les informations des partitions disponibles\"\"\"\n",
    "        if not self.con:\n",
    "            self.setup_connection()\n",
    "        \n",
    "        print(\"📊 Analyse des partitions disponibles...\")\n",
    "        \n",
    "        try:\n",
    "            # Récupérer la liste des fichiers avec métadonnées\n",
    "            result = self.con.execute(f\"\"\"\n",
    "                SELECT file as filename\n",
    "                FROM glob('{self.config.feature_store_path}')\n",
    "                ORDER BY file\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            partitions = []\n",
    "            for row in result:\n",
    "                partitions.append({\n",
    "                    'path': row[0],\n",
    "                    'size_mb': 0,  # Taille non disponible avec glob simple\n",
    "                    'last_modified': 'unknown'\n",
    "                })\n",
    "            \n",
    "            print(f\"📁 {len(partitions)} partitions trouvées\")\n",
    "            if partitions:\n",
    "                print(f\"📁 Fichiers trouvés:\")\n",
    "                for i, p in enumerate(partitions[:5]):  # Afficher les 5 premiers\n",
    "                    print(f\"   • {p['path']}\")\n",
    "                if len(partitions) > 5:\n",
    "                    print(f\"   • ... et {len(partitions)-5} autres\")\n",
    "            \n",
    "            return partitions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur lors de l'analyse des partitions: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_data_summary(self) -> Dict:\n",
    "        \"\"\"Récupère un résumé des données disponibles\"\"\"\n",
    "        if not self.con:\n",
    "            self.setup_connection()\n",
    "        \n",
    "        print(\"📈 Analyse du contenu des données...\")\n",
    "        \n",
    "        # Requête avec filtre temporel si spécifié\n",
    "        where_clause = f\"WHERE symbol = '{self.config.symbol}'\"\n",
    "        if self.config.start_date:\n",
    "            where_clause += f\" AND datetime >= '{self.config.start_date}'\"\n",
    "        if self.config.end_date:\n",
    "            where_clause += f\" AND datetime <= '{self.config.end_date}'\"\n",
    "        \n",
    "        try:\n",
    "            result = self.con.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_rows,\n",
    "                    MIN(datetime) as start_date,\n",
    "                    MAX(datetime) as end_date,\n",
    "                    COUNT(DISTINCT date_trunc('day', datetime)) as unique_days\n",
    "                FROM read_parquet('{self.config.feature_store_path}')\n",
    "                {where_clause}\n",
    "            \"\"\").fetchone()\n",
    "            \n",
    "            summary = {\n",
    "                'total_rows': result[0],\n",
    "                'start_date': result[1],\n",
    "                'end_date': result[2],\n",
    "                'unique_days': result[3]\n",
    "            }\n",
    "            \n",
    "            print(f\"📊 Résumé des données:\")\n",
    "            print(f\"   • Total lignes: {summary['total_rows']:,}\")\n",
    "            print(f\"   • Période: {summary['start_date']} → {summary['end_date']}\")\n",
    "            print(f\"   • Jours uniques: {summary['unique_days']:,}\")\n",
    "            \n",
    "            # Estimation des chunks\n",
    "            estimated_chunks = (summary['total_rows'] // self.config.chunk_size) + 1\n",
    "            print(f\"   • Chunks estimés: {estimated_chunks:,}\")\n",
    "            \n",
    "            return summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur lors de l'analyse: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Initialisation du loader\n",
    "data_loader = HermesDataLoader(config)\n",
    "\n",
    "# 🔍 Test de connexion et validation du chemin\n",
    "print(\"🔍 TEST DE CONNEXION ET VALIDATION DU CHEMIN\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Test direct du chemin avec glob\n",
    "try:\n",
    "    if not data_loader.con:\n",
    "        data_loader.setup_connection()\n",
    "    \n",
    "    # Tester différents patterns de chemin\n",
    "    test_paths = [\n",
    "        \"s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/**/*.parquet\",  # Pattern actuel\n",
    "        \"s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/*.parquet\",     # Pattern direct\n",
    "        \"s3://gold/gold_features_spot_monthly_klines_BTCUSDT_4h/*\",             # Tous les fichiers\n",
    "    ]\n",
    "    \n",
    "    for i, test_path in enumerate(test_paths, 1):\n",
    "        print(f\"\\n🧪 Test {i}: {test_path}\")\n",
    "        try:\n",
    "            # Test avec glob pour lister les fichiers\n",
    "            files_result = data_loader.con.execute(f\"\"\"\n",
    "                SELECT file as filename\n",
    "                FROM glob('{test_path}')\n",
    "                LIMIT 5\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            if files_result:\n",
    "                print(f\"   ✅ {len(files_result)} fichier(s) trouvé(s)\")\n",
    "                for file_info in files_result[:3]:  # Afficher les 3 premiers\n",
    "                    print(f\"     📁 {file_info[0]}\")\n",
    "                if len(files_result) > 3:\n",
    "                    print(f\"     ... et {len(files_result)-3} autres\")\n",
    "                \n",
    "                # Si on trouve des fichiers, tester la lecture\n",
    "                try:\n",
    "                    test_read = data_loader.con.execute(f\"\"\"\n",
    "                        SELECT COUNT(*) as row_count, COUNT(DISTINCT symbol) as symbols\n",
    "                        FROM read_parquet('{test_path}')\n",
    "                        LIMIT 1\n",
    "                    \"\"\").fetchone()\n",
    "                    \n",
    "                    if test_read:\n",
    "                        print(f\"   📊 Test lecture: {test_read[0]:,} lignes, {test_read[1]} symbole(s)\")\n",
    "                        # Mettre à jour la config avec le chemin qui fonctionne\n",
    "                        config.feature_store_path = test_path\n",
    "                        print(f\"   🎯 Chemin optimal trouvé !\")\n",
    "                        break\n",
    "                        \n",
    "                except Exception as read_error:\n",
    "                    print(f\"   ❌ Erreur lecture: {read_error}\")\n",
    "            else:\n",
    "                print(f\"   ❌ Aucun fichier trouvé\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Erreur test: {e}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Chemin final utilisé: {config.feature_store_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur lors du test de connexion: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 45)\n",
    "\n",
    "# Maintenant charger les infos avec le bon chemin\n",
    "partitions = data_loader.get_partition_info()\n",
    "data_summary = data_loader.get_data_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df3c10",
   "metadata": {},
   "source": [
    "## 3. 📊 Processeur de Backtesting Chunked avec Garanties de Cohérence\n",
    "\n",
    "### Système de traitement par chunks avec continuité des signaux et positions\n",
    "\n",
    "**🛡️ Garanties de Cohérence Implémentées** :\n",
    "\n",
    "1. **Buffer de Contexte Étendu** : 50+ lignes de contexte entre chunks\n",
    "2. **Validation des Signaux `shift()`** : Vérification que les valeurs précédentes existent\n",
    "3. **Détection de Chevauchements** : Gestion automatique des doublons temporels\n",
    "4. **Validation Continue** : Contrôles à chaque étape du traitement\n",
    "5. **Diagnostic Préalable** : Vérification de la cohérence des données source\n",
    "\n",
    "**Note** : Ce processeur utilise uniquement les indicateurs pré-calculés dans le Hub Features Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1746505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Backtester chunked initialisé avec garanties de cohérence\n",
      "⚙️ Configuration: 50,000 lignes/chunk avec 100 lignes de contexte\n",
      "💰 Capital initial: $10,000.00\n",
      "📊 Tous les indicateurs doivent être pré-calculés dans le Hub Features Gold\n",
      "✅ Système de validation de continuité des signaux activé\n"
     ]
    }
   ],
   "source": [
    "class HermesChunkedBacktester:\n",
    "    \"\"\"Backtesting chunked avec continuité pour stratégies Hermes\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BacktestConfig, data_loader: HermesDataLoader):\n",
    "        self.config = config\n",
    "        self.data_loader = data_loader\n",
    "        self.indicator_cols = config.get_indicator_columns()\n",
    "        \n",
    "        # État persistant entre chunks\n",
    "        self.state = {\n",
    "            'context_rows': None,        # Lignes de contexte pour continuité\n",
    "            'last_position': None,       # 'long', 'short' ou None\n",
    "            'cumulative_cash': config.initial_cash,\n",
    "            'cumulative_value': config.initial_cash,\n",
    "            'total_trades': 0,\n",
    "            'chunk_results': [],         # Résultats par chunk\n",
    "            'chunk_counter': 0,\n",
    "            'context_buffer_size': config.get_required_context_size()  #  Calcul automatique du buffer plus grand pour les signaux\n",
    "        }\n",
    "    \n",
    "    def compute_strategy_signals(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Calcule les signaux de la stratégie Smart Momentum avec validation de cohérence\"\"\"\n",
    "        \n",
    "        # Récupérer les colonnes d'indicateurs\n",
    "        ema_fast_col = self.indicator_cols[\"ema_fast\"]\n",
    "        ema_slow_col = self.indicator_cols[\"ema_slow\"]\n",
    "        rsi_col = self.indicator_cols[\"rsi_14\"]\n",
    "        supertrend_dir_col = self.indicator_cols[\"supertrend_dir\"]\n",
    "        \n",
    "        # Vérifier que les colonnes existent\n",
    "        missing_cols = []\n",
    "        for col_name, col_actual in self.indicator_cols.items():\n",
    "            if col_actual not in df.columns:\n",
    "                missing_cols.append(f\"{col_name} -> {col_actual}\")\n",
    "        \n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"❌ Colonnes d'indicateurs manquantes: {missing_cols}\")\n",
    "        \n",
    "        # Calcul des signaux avec validation de continuité\n",
    "        signals_df = df.with_columns([\n",
    "            # === VALIDATION DE CONTINUITÉ ===\n",
    "            # Marquer les lignes où shift(1) sera valide\n",
    "            (pl.int_range(pl.len()) > 0).alias(\"has_previous_value\"),\n",
    "            \n",
    "            # === CONDITIONS EMA ===\n",
    "            # Condition actuelle : EMA rapide > EMA lente\n",
    "            (pl.col(ema_fast_col) > pl.col(ema_slow_col)).alias(\"ema_fast_above_slow\"),\n",
    "            \n",
    "            # Condition précédente : EMA rapide <= EMA lente (avec gestion des nulls)\n",
    "            (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)).alias(\"ema_was_below_or_equal\"),\n",
    "            \n",
    "            # Crossover EMA seulement si on a une valeur précédente valide\n",
    "            ((pl.col(ema_fast_col) > pl.col(ema_slow_col)) & \n",
    "             (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)) &\n",
    "             (pl.int_range(pl.len()) > 0)).alias(\"ema_bullish_cross\"),\n",
    "            \n",
    "            # === CONDITIONS RSI ===\n",
    "            ((pl.col(rsi_col) >= self.config.rsi_neutral_low) & \n",
    "             (pl.col(rsi_col) <= self.config.rsi_neutral_high)).alias(\"rsi_neutral\"),\n",
    "            \n",
    "            # === CONDITIONS SUPERTREND ===\n",
    "            (pl.col(supertrend_dir_col) == 1).alias(\"supertrend_bullish\"),\n",
    "            \n",
    "            # SuperTrend exit avec validation de continuité\n",
    "            ((pl.col(supertrend_dir_col).shift(1) == 1) & \n",
    "             (pl.col(supertrend_dir_col) == -1) &\n",
    "             (pl.int_range(pl.len()) > 0)).alias(\"supertrend_exit\")\n",
    "        ])\n",
    "        \n",
    "        # Signaux finaux avec validation\n",
    "        final_signals = signals_df.with_columns([\n",
    "            # Signal d'entrée (achat) - uniquement si toutes conditions remplies\n",
    "            (pl.col(\"ema_bullish_cross\") & \n",
    "             pl.col(\"rsi_neutral\") & \n",
    "             pl.col(\"supertrend_bullish\")).alias(\"buy_signal\"),\n",
    "            \n",
    "            # Signal de sortie (vente) - uniquement avec continuité validée\n",
    "            (pl.col(\"supertrend_exit\")).alias(\"sell_signal\")\n",
    "        ])\n",
    "        \n",
    "        return final_signals\n",
    "    \n",
    "    def load_chunk_with_context(self, offset: int, limit: int) -> pl.DataFrame:\n",
    "        \"\"\"Charge un chunk avec le contexte nécessaire pour garantir la cohérence\"\"\"\n",
    "        \n",
    "        # Construire la requête avec filtre temporel\n",
    "        where_clause = f\"WHERE symbol = '{self.config.symbol}'\"\n",
    "        if self.config.start_date:\n",
    "            where_clause += f\" AND datetime >= '{self.config.start_date}'\"\n",
    "        if self.config.end_date:\n",
    "            where_clause += f\" AND datetime <= '{self.config.end_date}'\"\n",
    "        \n",
    "        # AMÉLIORATION : Charger plus de contexte pour les premiers chunks\n",
    "        actual_offset = offset\n",
    "        actual_limit = limit\n",
    "        \n",
    "        # Si ce n'est pas le premier chunk, commencer plus tôt pour avoir du contexte\n",
    "        if offset > 0 and self.state['context_rows'] is None:\n",
    "            # Charger du contexte supplémentaire depuis la base\n",
    "            context_needed = self.state['context_buffer_size']\n",
    "            actual_offset = max(0, offset - context_needed)\n",
    "            actual_limit = limit + (offset - actual_offset)\n",
    "        \n",
    "        # Requête DuckDB pour le chunk avec contexte\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM read_parquet('{self.config.feature_store_path}')\n",
    "            {where_clause}\n",
    "            ORDER BY datetime\n",
    "            LIMIT {actual_limit}\n",
    "            OFFSET {actual_offset}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Exécuter la requête\n",
    "        result = self.data_loader.con.execute(query).arrow()\n",
    "        chunk_df = pl.from_arrow(result)\n",
    "        \n",
    "        # Ajouter le contexte des lignes précédentes si disponible\n",
    "        if self.state['context_rows'] is not None and offset > 0:\n",
    "            # Vérifier la continuité temporelle\n",
    "            if len(self.state['context_rows']) > 0 and len(chunk_df) > 0:\n",
    "                last_context_time = self.state['context_rows']['datetime'].max()\n",
    "                first_chunk_time = chunk_df['datetime'].min()\n",
    "                \n",
    "                if last_context_time >= first_chunk_time:\n",
    "                    print(f\"⚠️ Chevauchement temporel détecté - ajustement automatique\")\n",
    "                    # Filtrer les doublons\n",
    "                    chunk_df = chunk_df.filter(pl.col('datetime') > last_context_time)\n",
    "            \n",
    "            # Concaténer avec le contexte\n",
    "            if len(chunk_df) > 0:\n",
    "                chunk_df = pl.concat([self.state['context_rows'], chunk_df])\n",
    "        \n",
    "        return chunk_df\n",
    "    \n",
    "    def process_chunk(self, chunk_df: pl.DataFrame, is_first_chunk: bool) -> Dict:\n",
    "        \"\"\"Traite un chunk avec calcul des signaux et backtesting\"\"\"\n",
    "        \n",
    "        if len(chunk_df) == 0:\n",
    "            return {\n",
    "                'chunk_id': self.state['chunk_counter'],\n",
    "                'rows_processed': 0,\n",
    "                'buy_signals': 0,\n",
    "                'sell_signals': 0,\n",
    "                'start_time': None,\n",
    "                'end_time': None,\n",
    "                'data': None,\n",
    "                'warnings': ['Chunk vide']\n",
    "            }\n",
    "        \n",
    "        # 1. Calculer les signaux de stratégie avec validation de cohérence\n",
    "        try:\n",
    "            signals_df = self.compute_strategy_signals(chunk_df)\n",
    "        except ValueError as e:\n",
    "            print(f\"❌ Erreur dans le calcul des signaux: {e}\")\n",
    "            return {\n",
    "                'chunk_id': self.state['chunk_counter'],\n",
    "                'rows_processed': 0,\n",
    "                'buy_signals': 0,\n",
    "                'sell_signals': 0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "        \n",
    "        # 2. Ajuster les signaux selon l'état précédent\n",
    "        if not is_first_chunk and self.state['last_position'] is not None:\n",
    "            # Si on était en position, ne pas générer d'entrée immédiate\n",
    "            if self.state['last_position'] == 'long':\n",
    "                signals_df = signals_df.with_columns(\n",
    "                    pl.when(pl.int_range(pl.len()) == 0)\n",
    "                    .then(False)\n",
    "                    .otherwise(pl.col(\"buy_signal\"))\n",
    "                    .alias(\"buy_signal\")\n",
    "                )\n",
    "        \n",
    "        # 3. Extraire les résultats sans le contexte\n",
    "        context_size = len(self.state['context_rows']) if self.state['context_rows'] is not None and not is_first_chunk else 0\n",
    "        \n",
    "        if context_size > 0:\n",
    "            result_df = signals_df.slice(context_size)\n",
    "        else:\n",
    "            result_df = signals_df\n",
    "        \n",
    "        # 4. Validation des signaux calculés\n",
    "        warnings = []\n",
    "        if len(result_df) > 0:\n",
    "            # Vérifier qu'on n'a pas de signaux sur la première ligne d'un chunk (sauf premier chunk)\n",
    "            if not is_first_chunk and context_size == 0:\n",
    "                first_row_signals = result_df.head(1)\n",
    "                if (first_row_signals.select(pl.col(\"buy_signal\").sum()).item() > 0 or \n",
    "                    first_row_signals.select(pl.col(\"sell_signal\").sum()).item() > 0):\n",
    "                    warnings.append(\"Signaux détectés sur première ligne sans contexte\")\n",
    "        \n",
    "        # 5. Compter les signaux\n",
    "        buy_signals = result_df.select(pl.col(\"buy_signal\").sum()).item() if len(result_df) > 0 else 0\n",
    "        sell_signals = result_df.select(pl.col(\"sell_signal\").sum()).item() if len(result_df) > 0 else 0\n",
    "        \n",
    "        # 6. Mettre à jour l'état pour le chunk suivant avec plus de contexte\n",
    "        if len(signals_df) > 0:\n",
    "            self.state['context_rows'] = signals_df.tail(self.state['context_buffer_size'])\n",
    "        \n",
    "        # Simuler la position (logique simplifiée)\n",
    "        if buy_signals > 0 and self.state['last_position'] != 'long':\n",
    "            self.state['last_position'] = 'long'\n",
    "        elif sell_signals > 0 and self.state['last_position'] == 'long':\n",
    "            self.state['last_position'] = None\n",
    "            self.state['total_trades'] += 1\n",
    "        \n",
    "        # 7. Retourner les résultats du chunk avec métadonnées de validation\n",
    "        chunk_result = {\n",
    "            'chunk_id': self.state['chunk_counter'],\n",
    "            'rows_processed': len(result_df),\n",
    "            'buy_signals': buy_signals,\n",
    "            'sell_signals': sell_signals,\n",
    "            'start_time': result_df.select(pl.col(\"datetime\").min()).item() if len(result_df) > 0 else None,\n",
    "            'end_time': result_df.select(pl.col(\"datetime\").max()).item() if len(result_df) > 0 else None,\n",
    "            'data': result_df,  # Conserver les données pour sauvegarde\n",
    "            'context_size': context_size,\n",
    "            'warnings': warnings,\n",
    "            'continuity_validated': context_size > 0 or is_first_chunk\n",
    "        }\n",
    "        \n",
    "        self.state['chunk_counter'] += 1\n",
    "        \n",
    "        return chunk_result\n",
    "\n",
    "# Initialisation du backtester\n",
    "backtester = HermesChunkedBacktester(config, data_loader)\n",
    "\n",
    "print(\"🚀 Backtester chunked initialisé avec garanties de cohérence\")\n",
    "print(f\"⚙️ Configuration: {config.chunk_size:,} lignes/chunk avec {backtester.state['context_buffer_size']} lignes de contexte\")\n",
    "print(f\"💰 Capital initial: ${config.initial_cash:,.2f}\")\n",
    "print(\"📊 Tous les indicateurs doivent être pré-calculés dans le Hub Features Gold\")\n",
    "print(\"✅ Système de validation de continuité des signaux activé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce5e05c",
   "metadata": {},
   "source": [
    "## 4. 🔄 Exécution du Backtesting Chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19f8c0",
   "metadata": {},
   "source": [
    "## 4.1 🔍 Diagnostic de Cohérence des Données\n",
    "\n",
    "Avant d'exécuter le backtesting, vérifions la cohérence des données et la continuité temporelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3ad4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DIAGNOSTIC DE COHERENCE DES DONNÉES\n",
      "========================================\n",
      "📅 Vérification de la continuité temporelle...\n",
      "   • Lignes analysées: 9,999\n",
      "   • Intervalles uniques: 4\n",
      "   • Intervalle min: 4.00 heures\n",
      "   • Intervalle max: 32.00 heures\n",
      "   • Intervalle moyen: 4.01 heures\n",
      "\n",
      "📊 Vérification des indicateurs requis...\n",
      "   • Colonnes disponibles: 35\n",
      "   • Indicateurs requis: 11\n",
      "   • Indicateurs manquants: 0\n",
      "   ✅ Tous les indicateurs requis sont présents\n",
      "\n",
      "🧪 Test de calcul de signaux sur échantillon...\n",
      "   ✅ Test réussi - 1 signaux d'achat, 11 signaux de vente sur échantillon\n",
      "\n",
      "📋 RÉSUMÉ DU DIAGNOSTIC:\n",
      "   • Continuité temporelle: ❌\n",
      "   • Indicateurs complets: ✅\n",
      "\n",
      "💡 RECOMMANDATIONS:\n",
      "   ⚠️ Intervalles temporels irréguliers détectés - vérifier la qualité des données\n",
      "   ✅ Test réussi - 1 signaux d'achat, 11 signaux de vente sur échantillon\n",
      "\n",
      "📋 RÉSUMÉ DU DIAGNOSTIC:\n",
      "   • Continuité temporelle: ❌\n",
      "   • Indicateurs complets: ✅\n",
      "\n",
      "💡 RECOMMANDATIONS:\n",
      "   ⚠️ Intervalles temporels irréguliers détectés - vérifier la qualité des données\n"
     ]
    }
   ],
   "source": [
    "def run_coherence_diagnostic() -> Dict:\n",
    "    \"\"\"Diagnostic de cohérence des données avant backtesting\"\"\"\n",
    "    \n",
    "    print(\"🔍 DIAGNOSTIC DE COHERENCE DES DONNÉES\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if not data_loader.con:\n",
    "        data_loader.setup_connection()\n",
    "    \n",
    "    diagnostic_results = {\n",
    "        'temporal_continuity': True,\n",
    "        'indicator_completeness': True,\n",
    "        'data_gaps': [],\n",
    "        'missing_indicators': [],\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 1. Vérifier la continuité temporelle\n",
    "        print(\"📅 Vérification de la continuité temporelle...\")\n",
    "        \n",
    "        temporal_query = f\"\"\"\n",
    "            WITH time_diffs AS (\n",
    "                SELECT \n",
    "                    datetime,\n",
    "                    LAG(datetime) OVER (ORDER BY datetime) as prev_datetime,\n",
    "                    datetime - LAG(datetime) OVER (ORDER BY datetime) as time_diff_interval,\n",
    "                    EXTRACT(EPOCH FROM (datetime - LAG(datetime) OVER (ORDER BY datetime))) / 3600.0 as time_diff_hours\n",
    "                FROM read_parquet('{config.feature_store_path}')\n",
    "                WHERE symbol = '{config.symbol}'\n",
    "                ORDER BY datetime\n",
    "                LIMIT 10000  -- Échantillon pour diagnostic\n",
    "            )\n",
    "            SELECT \n",
    "                COUNT(*) as total_rows,\n",
    "                COUNT(DISTINCT time_diff_hours) as unique_intervals,\n",
    "                MIN(time_diff_hours) as min_interval_hours,\n",
    "                MAX(time_diff_hours) as max_interval_hours,\n",
    "                AVG(time_diff_hours) as avg_interval_hours\n",
    "            FROM time_diffs \n",
    "            WHERE time_diff_hours IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        result = data_loader.con.execute(temporal_query).fetchone()\n",
    "        total_rows, unique_intervals, min_interval_hours, max_interval_hours, avg_interval_hours = result\n",
    "        \n",
    "        print(f\"   • Lignes analysées: {total_rows:,}\")\n",
    "        print(f\"   • Intervalles uniques: {unique_intervals}\")\n",
    "        print(f\"   • Intervalle min: {min_interval_hours:.2f} heures\")\n",
    "        print(f\"   • Intervalle max: {max_interval_hours:.2f} heures\")\n",
    "        print(f\"   • Intervalle moyen: {avg_interval_hours:.2f} heures\")\n",
    "        \n",
    "        if unique_intervals > 2:  # Tolérance pour quelques variations\n",
    "            diagnostic_results['temporal_continuity'] = False\n",
    "            diagnostic_results['recommendations'].append(\n",
    "                \"⚠️ Intervalles temporels irréguliers détectés - vérifier la qualité des données\"\n",
    "            )\n",
    "        \n",
    "        # 2. Vérifier la présence des indicateurs requis\n",
    "        print(\"\\n📊 Vérification des indicateurs requis...\")\n",
    "        \n",
    "        schema_query = f\"\"\"\n",
    "            DESCRIBE SELECT * FROM read_parquet('{config.feature_store_path}') LIMIT 1\n",
    "        \"\"\"\n",
    "        \n",
    "        available_columns = [row[0] for row in data_loader.con.execute(schema_query).fetchall()]\n",
    "        required_indicators = list(config.get_indicator_columns().values())\n",
    "        \n",
    "        missing_indicators = [ind for ind in required_indicators if ind not in available_columns]\n",
    "        \n",
    "        print(f\"   • Colonnes disponibles: {len(available_columns)}\")\n",
    "        print(f\"   • Indicateurs requis: {len(required_indicators)}\")\n",
    "        print(f\"   • Indicateurs manquants: {len(missing_indicators)}\")\n",
    "        \n",
    "        if missing_indicators:\n",
    "            diagnostic_results['indicator_completeness'] = False\n",
    "            diagnostic_results['missing_indicators'] = missing_indicators\n",
    "            print(f\"   ❌ Indicateurs manquants: {missing_indicators}\")\n",
    "            diagnostic_results['recommendations'].append(\n",
    "                f\"❌ Recalculer ou ajouter les indicateurs manquants: {missing_indicators}\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"   ✅ Tous les indicateurs requis sont présents\")\n",
    "        \n",
    "        # 3. Tester un échantillon de calcul de signaux\n",
    "        print(\"\\n🧪 Test de calcul de signaux sur échantillon...\")\n",
    "        \n",
    "        sample_query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM read_parquet('{config.feature_store_path}')\n",
    "            WHERE symbol = '{config.symbol}'\n",
    "            ORDER BY datetime\n",
    "            LIMIT 1000\n",
    "        \"\"\"\n",
    "        \n",
    "        sample_result = data_loader.con.execute(sample_query).arrow()\n",
    "        sample_df = pl.from_arrow(sample_result)\n",
    "        \n",
    "        if len(sample_df) > 0:\n",
    "            try:\n",
    "                # Test du calcul de signaux\n",
    "                test_signals = backtester.compute_strategy_signals(sample_df)\n",
    "                \n",
    "                buy_count = test_signals.select(pl.col(\"buy_signal\").sum()).item()\n",
    "                sell_count = test_signals.select(pl.col(\"sell_signal\").sum()).item()\n",
    "                \n",
    "                print(f\"   ✅ Test réussi - {buy_count} signaux d'achat, {sell_count} signaux de vente sur échantillon\")\n",
    "                \n",
    "                # Vérifier les signaux sur la première ligne (problème de shift)\n",
    "                first_row_signals = test_signals.head(1)\n",
    "                first_buy = first_row_signals.select(pl.col(\"buy_signal\")).item()\n",
    "                first_sell = first_row_signals.select(pl.col(\"sell_signal\")).item()\n",
    "                \n",
    "                if first_buy or first_sell:\n",
    "                    diagnostic_results['recommendations'].append(\n",
    "                        \"⚠️ Signaux détectés sur première ligne - vérifier la logique de shift()\"\n",
    "                    )\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Erreur dans le calcul de signaux: {e}\")\n",
    "                diagnostic_results['recommendations'].append(f\"❌ Erreur calcul signaux: {e}\")\n",
    "        \n",
    "        # 4. Résumé du diagnostic\n",
    "        print(f\"\\n📋 RÉSUMÉ DU DIAGNOSTIC:\")\n",
    "        print(f\"   • Continuité temporelle: {'✅' if diagnostic_results['temporal_continuity'] else '❌'}\")\n",
    "        print(f\"   • Indicateurs complets: {'✅' if diagnostic_results['indicator_completeness'] else '❌'}\")\n",
    "        \n",
    "        if diagnostic_results['recommendations']:\n",
    "            print(f\"\\n💡 RECOMMANDATIONS:\")\n",
    "            for rec in diagnostic_results['recommendations']:\n",
    "                print(f\"   {rec}\")\n",
    "        else:\n",
    "            print(f\"\\n✅ TOUTES LES VÉRIFICATIONS PASSÉES - PRÊT POUR LE BACKTESTING\")\n",
    "        \n",
    "        return diagnostic_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors du diagnostic: {e}\")\n",
    "        diagnostic_results['recommendations'].append(f\"❌ Erreur diagnostic: {e}\")\n",
    "        return diagnostic_results\n",
    "\n",
    "# Exécution du diagnostic\n",
    "diagnostic = run_coherence_diagnostic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4988618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DIAGNOSTIC AVANCÉ DES SIGNAUX\n",
      "===================================\n",
      "📊 Échantillon analysé: 5,000 lignes\n",
      "\n",
      "📈 COLONNES D'INDICATEURS:\n",
      "   • EMA Fast (ema_12): ✅\n",
      "   • EMA Slow (ema_26): ✅\n",
      "   • RSI (rsi_14): ✅\n",
      "   • SuperTrend Dir (supertrend_dir_10_3.0): ✅\n",
      "\n",
      "📊 ANALYSE DES CONDITIONS (sur 5,000 lignes):\n",
      "   • EMA Fast > Slow: 2,575 (51.5%)\n",
      "   • EMA Bullish Cross: 73 (1.5%)\n",
      "   • RSI Neutral (45-55): 1,392 (27.8%)\n",
      "   • SuperTrend Bullish: 2,531 (50.6%)\n",
      "   • 🎯 SIGNAUX D'ACHAT FINAUX: 9\n",
      "\n",
      "📊 STATISTIQUES RSI:\n",
      "   • Min: 7.7\n",
      "   • Q25: 41.4\n",
      "   • Moyenne: nan\n",
      "   • Q75: 60.3\n",
      "   • Max: 95.0\n",
      "   • Plage neutre configurée: 45-55\n",
      "\n",
      "💡 SUGGESTIONS D'AMÉLIORATION:\n",
      "\n",
      "===================================\n",
      "📊 Échantillon analysé: 5,000 lignes\n",
      "\n",
      "📈 COLONNES D'INDICATEURS:\n",
      "   • EMA Fast (ema_12): ✅\n",
      "   • EMA Slow (ema_26): ✅\n",
      "   • RSI (rsi_14): ✅\n",
      "   • SuperTrend Dir (supertrend_dir_10_3.0): ✅\n",
      "\n",
      "📊 ANALYSE DES CONDITIONS (sur 5,000 lignes):\n",
      "   • EMA Fast > Slow: 2,575 (51.5%)\n",
      "   • EMA Bullish Cross: 73 (1.5%)\n",
      "   • RSI Neutral (45-55): 1,392 (27.8%)\n",
      "   • SuperTrend Bullish: 2,531 (50.6%)\n",
      "   • 🎯 SIGNAUX D'ACHAT FINAUX: 9\n",
      "\n",
      "📊 STATISTIQUES RSI:\n",
      "   • Min: 7.7\n",
      "   • Q25: 41.4\n",
      "   • Moyenne: nan\n",
      "   • Q75: 60.3\n",
      "   • Max: 95.0\n",
      "   • Plage neutre configurée: 45-55\n",
      "\n",
      "💡 SUGGESTIONS D'AMÉLIORATION:\n",
      "\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# 🔍 DIAGNOSTIC AVANCÉ : Pourquoi aucun signal n'est généré ?\n",
    "print(\"🔍 DIAGNOSTIC AVANCÉ DES SIGNAUX\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Charger un échantillon plus large pour diagnostic\n",
    "sample_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_parquet('{config.feature_store_path}')\n",
    "    WHERE symbol = '{config.symbol}'\n",
    "    ORDER BY datetime\n",
    "    LIMIT 5000\n",
    "\"\"\"\n",
    "\n",
    "sample_result = data_loader.con.execute(sample_query).arrow()\n",
    "sample_df = pl.from_arrow(sample_result)\n",
    "\n",
    "print(f\"📊 Échantillon analysé: {len(sample_df):,} lignes\")\n",
    "\n",
    "if len(sample_df) > 0:\n",
    "    # Calculer les signaux avec diagnostics détaillés\n",
    "    ema_fast_col = config.get_indicator_columns()[\"ema_fast\"]\n",
    "    ema_slow_col = config.get_indicator_columns()[\"ema_slow\"]\n",
    "    rsi_col = config.get_indicator_columns()[\"rsi_14\"]\n",
    "    supertrend_dir_col = config.get_indicator_columns()[\"supertrend_dir\"]\n",
    "    \n",
    "    # Vérifier les colonnes d'indicateurs\n",
    "    print(f\"\\n📈 COLONNES D'INDICATEURS:\")\n",
    "    print(f\"   • EMA Fast ({ema_fast_col}): {'✅' if ema_fast_col in sample_df.columns else '❌ MANQUANT'}\")\n",
    "    print(f\"   • EMA Slow ({ema_slow_col}): {'✅' if ema_slow_col in sample_df.columns else '❌ MANQUANT'}\")\n",
    "    print(f\"   • RSI ({rsi_col}): {'✅' if rsi_col in sample_df.columns else '❌ MANQUANT'}\")\n",
    "    print(f\"   • SuperTrend Dir ({supertrend_dir_col}): {'✅' if supertrend_dir_col in sample_df.columns else '❌ MANQUANT'}\")\n",
    "    \n",
    "    # Analyser les conditions individuelles\n",
    "    if all(col in sample_df.columns for col in [ema_fast_col, ema_slow_col, rsi_col, supertrend_dir_col]):\n",
    "        # Calculer les conditions individuelles\n",
    "        analysis_df = sample_df.with_columns([\n",
    "            # Conditions EMA\n",
    "            (pl.col(ema_fast_col) > pl.col(ema_slow_col)).alias(\"ema_fast_above_slow\"),\n",
    "            (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)).alias(\"ema_was_below_or_equal\"),\n",
    "            \n",
    "            # Crossover EMA\n",
    "            ((pl.col(ema_fast_col) > pl.col(ema_slow_col)) & \n",
    "             (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)) &\n",
    "             (pl.int_range(pl.len()) > 0)).alias(\"ema_bullish_cross\"),\n",
    "            \n",
    "            # Conditions RSI\n",
    "            ((pl.col(rsi_col) >= config.rsi_neutral_low) & \n",
    "             (pl.col(rsi_col) <= config.rsi_neutral_high)).alias(\"rsi_neutral\"),\n",
    "            \n",
    "            # Conditions SuperTrend\n",
    "            (pl.col(supertrend_dir_col) == 1).alias(\"supertrend_bullish\"),\n",
    "            \n",
    "            # Signal final\n",
    "            ((pl.col(ema_fast_col) > pl.col(ema_slow_col)) & \n",
    "             (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)) &\n",
    "             (pl.int_range(pl.len()) > 0) &\n",
    "             (pl.col(rsi_col) >= config.rsi_neutral_low) & \n",
    "             (pl.col(rsi_col) <= config.rsi_neutral_high) &\n",
    "             (pl.col(supertrend_dir_col) == 1)).alias(\"buy_signal\")\n",
    "        ])\n",
    "        \n",
    "        # Statistiques des conditions\n",
    "        print(f\"\\n📊 ANALYSE DES CONDITIONS (sur {len(analysis_df):,} lignes):\")\n",
    "        \n",
    "        ema_fast_above = analysis_df.select(pl.col(\"ema_fast_above_slow\").sum()).item()\n",
    "        ema_crossover = analysis_df.select(pl.col(\"ema_bullish_cross\").sum()).item()\n",
    "        rsi_neutral_count = analysis_df.select(pl.col(\"rsi_neutral\").sum()).item()\n",
    "        supertrend_bull = analysis_df.select(pl.col(\"supertrend_bullish\").sum()).item()\n",
    "        buy_signals = analysis_df.select(pl.col(\"buy_signal\").sum()).item()\n",
    "        \n",
    "        print(f\"   • EMA Fast > Slow: {ema_fast_above:,} ({ema_fast_above/len(analysis_df)*100:.1f}%)\")\n",
    "        print(f\"   • EMA Bullish Cross: {ema_crossover:,} ({ema_crossover/len(analysis_df)*100:.1f}%)\")\n",
    "        print(f\"   • RSI Neutral ({config.rsi_neutral_low}-{config.rsi_neutral_high}): {rsi_neutral_count:,} ({rsi_neutral_count/len(analysis_df)*100:.1f}%)\")\n",
    "        print(f\"   • SuperTrend Bullish: {supertrend_bull:,} ({supertrend_bull/len(analysis_df)*100:.1f}%)\")\n",
    "        print(f\"   • 🎯 SIGNAUX D'ACHAT FINAUX: {buy_signals:,}\")\n",
    "        \n",
    "        # Analyser les valeurs RSI pour comprendre le problème\n",
    "        rsi_stats = sample_df.select([\n",
    "            pl.col(rsi_col).min().alias(\"rsi_min\"),\n",
    "            pl.col(rsi_col).max().alias(\"rsi_max\"),\n",
    "            pl.col(rsi_col).mean().alias(\"rsi_mean\"),\n",
    "            pl.col(rsi_col).quantile(0.25).alias(\"rsi_q25\"),\n",
    "            pl.col(rsi_col).quantile(0.75).alias(\"rsi_q75\")\n",
    "        ]).to_dicts()[0]\n",
    "        \n",
    "        print(f\"\\n📊 STATISTIQUES RSI:\")\n",
    "        print(f\"   • Min: {rsi_stats['rsi_min']:.1f}\")\n",
    "        print(f\"   • Q25: {rsi_stats['rsi_q25']:.1f}\")\n",
    "        print(f\"   • Moyenne: {rsi_stats['rsi_mean']:.1f}\")\n",
    "        print(f\"   • Q75: {rsi_stats['rsi_q75']:.1f}\")\n",
    "        print(f\"   • Max: {rsi_stats['rsi_max']:.1f}\")\n",
    "        print(f\"   • Plage neutre configurée: {config.rsi_neutral_low}-{config.rsi_neutral_high}\")\n",
    "        \n",
    "        if rsi_stats['rsi_mean'] < config.rsi_neutral_low or rsi_stats['rsi_mean'] > config.rsi_neutral_high:\n",
    "            print(f\"   ⚠️ La moyenne RSI ({rsi_stats['rsi_mean']:.1f}) est en dehors de la plage neutre!\")\n",
    "            \n",
    "        # Suggestions d'amélioration\n",
    "        print(f\"\\n💡 SUGGESTIONS D'AMÉLIORATION:\")\n",
    "        if ema_crossover == 0:\n",
    "            print(\"   • Aucun croisement EMA détecté - vérifier les périodes EMA\")\n",
    "        if rsi_neutral_count < len(analysis_df) * 0.1:\n",
    "            print(f\"   • Plage RSI trop restrictive - essayer {config.rsi_neutral_low-10}-{config.rsi_neutral_high+10}\")\n",
    "        if supertrend_bull < len(analysis_df) * 0.3:\n",
    "            print(\"   • SuperTrend rarement bullish - ajuster les paramètres\")\n",
    "            \n",
    "    else:\n",
    "        print(\"❌ Colonnes d'indicateurs manquantes - impossible d'analyser les conditions\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f794330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DIAGNOSTIC SUPERTREND\n",
      "=========================\n",
      "📊 Valeurs uniques de supertrend_dir_10_3.0:\n",
      "   • Valeur -1.0: 2,459 occurrences (49.2%)\n",
      "   • Valeur 1.0: 2,531 occurrences (50.6%)\n",
      "   • Valeur nan: 10 occurrences (0.2%)\n",
      "   • Valeurs nulles: 0\n",
      "\n",
      "📈 Dernières valeurs SuperTrend:\n",
      "   • 2019-11-29 12:00:00: 1.0\n",
      "   • 2019-11-29 16:00:00: 1.0\n",
      "   • 2019-11-29 20:00:00: 1.0\n",
      "   • 2019-11-30 00:00:00: 1.0\n",
      "   • 2019-11-30 04:00:00: 1.0\n",
      "   • 2019-11-30 08:00:00: 1.0\n",
      "   • 2019-11-30 12:00:00: 1.0\n",
      "   • 2019-11-30 16:00:00: 1.0\n",
      "   • 2019-11-30 20:00:00: 1.0\n",
      "   • 2019-12-01 00:00:00: 1.0\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "# 🔍 DIAGNOSTIC SUPERTREND SPÉCIFIQUE\n",
    "print(\"🔍 DIAGNOSTIC SUPERTREND\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "supertrend_dir_col = config.get_indicator_columns()[\"supertrend_dir\"]\n",
    "\n",
    "# Analyser les valeurs SuperTrend de façon simplifiée\n",
    "unique_values = sample_df.select(pl.col(supertrend_dir_col).unique()).to_series().to_list()\n",
    "print(f\"📊 Valeurs uniques de {supertrend_dir_col}:\")\n",
    "for value in unique_values:\n",
    "    if value is not None:\n",
    "        count = sample_df.filter(pl.col(supertrend_dir_col) == value).height\n",
    "        percentage = (count / len(sample_df)) * 100\n",
    "        print(f\"   • Valeur {value}: {count:,} occurrences ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        null_count = sample_df.filter(pl.col(supertrend_dir_col).is_null()).height\n",
    "        print(f\"   • Valeur NULL: {null_count:,} occurrences ({null_count/len(sample_df)*100:.1f}%)\")\n",
    "\n",
    "# Vérifier s'il y a des NaN/null\n",
    "null_count = sample_df.select(pl.col(supertrend_dir_col).is_null().sum()).item()\n",
    "print(f\"   • Valeurs nulles: {null_count:,}\")\n",
    "\n",
    "# Vérifier les dernières valeurs pour tendance récente\n",
    "recent_supertrend = sample_df.tail(100).select([\n",
    "    pl.col('datetime'),\n",
    "    pl.col(supertrend_dir_col)\n",
    "]).tail(10)\n",
    "\n",
    "print(f\"\\n📈 Dernières valeurs SuperTrend:\")\n",
    "for row in recent_supertrend.to_dicts():\n",
    "    print(f\"   • {row['datetime']}: {row[supertrend_dir_col]}\")\n",
    "\n",
    "print(\"=\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b81e8f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 TEST STRATÉGIE SIMPLIFIÉE\n",
      "==============================\n",
      "📊 RÉSULTATS STRATÉGIE SIMPLIFIÉE:\n",
      "   • Croisements EMA bullish: 73\n",
      "   • RSI OK (30-70): 4,194 (83.9%)\n",
      "   • 🎯 Signaux d'ACHAT: 69\n",
      "   • 🎯 Signaux de VENTE: 72\n",
      "\n",
      "✅ SUCCÈS ! La stratégie simplifiée génère des signaux.\n",
      "💡 Le problème vient bien du SuperTrend qui contient uniquement des NaN\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# 🚀 STRATÉGIE SIMPLIFIÉE SANS SUPERTREND (TEST)\n",
    "print(\"🚀 TEST STRATÉGIE SIMPLIFIÉE\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Modifier temporairement la stratégie pour ignorer SuperTrend\n",
    "def compute_simple_strategy_signals(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Version simplifiée sans SuperTrend pour test\"\"\"\n",
    "    \n",
    "    ema_fast_col = config.get_indicator_columns()[\"ema_fast\"]\n",
    "    ema_slow_col = config.get_indicator_columns()[\"ema_slow\"]\n",
    "    rsi_col = config.get_indicator_columns()[\"rsi_14\"]\n",
    "    \n",
    "    # Calcul des signaux simplifiés (sans SuperTrend)\n",
    "    signals_df = df.with_columns([\n",
    "        # === CONDITIONS EMA ===\n",
    "        (pl.col(ema_fast_col) > pl.col(ema_slow_col)).alias(\"ema_fast_above_slow\"),\n",
    "        (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)).alias(\"ema_was_below_or_equal\"),\n",
    "        \n",
    "        # Crossover EMA avec validation de continuité\n",
    "        ((pl.col(ema_fast_col) > pl.col(ema_slow_col)) & \n",
    "         (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)) &\n",
    "         (pl.int_range(pl.len()) > 0)).alias(\"ema_bullish_cross\"),\n",
    "        \n",
    "        # === CONDITIONS RSI (assouplies) ===\n",
    "        ((pl.col(rsi_col) >= 30) & (pl.col(rsi_col) <= 70)).alias(\"rsi_ok\"),\n",
    "        \n",
    "        # === SIGNAL SIMPLIFIÉ ===\n",
    "        # Achat : Croisement EMA bullish + RSI OK\n",
    "        ((pl.col(ema_fast_col) > pl.col(ema_slow_col)) & \n",
    "         (pl.col(ema_fast_col).shift(1) <= pl.col(ema_slow_col).shift(1)) &\n",
    "         (pl.int_range(pl.len()) > 0) &\n",
    "         (pl.col(rsi_col) >= 30) & (pl.col(rsi_col) <= 70)).alias(\"buy_signal\"),\n",
    "        \n",
    "        # Vente : Croisement EMA bearish\n",
    "        ((pl.col(ema_fast_col) <= pl.col(ema_slow_col)) & \n",
    "         (pl.col(ema_fast_col).shift(1) > pl.col(ema_slow_col).shift(1)) &\n",
    "         (pl.int_range(pl.len()) > 0)).alias(\"sell_signal\")\n",
    "    ])\n",
    "    \n",
    "    return signals_df\n",
    "\n",
    "# Test sur échantillon\n",
    "test_simple = compute_simple_strategy_signals(sample_df)\n",
    "\n",
    "buy_count_simple = test_simple.select(pl.col(\"buy_signal\").sum()).item()\n",
    "sell_count_simple = test_simple.select(pl.col(\"sell_signal\").sum()).item()\n",
    "ema_cross_count = test_simple.select(pl.col(\"ema_bullish_cross\").sum()).item()\n",
    "rsi_ok_count = test_simple.select(pl.col(\"rsi_ok\").sum()).item()\n",
    "\n",
    "print(f\"📊 RÉSULTATS STRATÉGIE SIMPLIFIÉE:\")\n",
    "print(f\"   • Croisements EMA bullish: {ema_cross_count:,}\")\n",
    "print(f\"   • RSI OK (30-70): {rsi_ok_count:,} ({rsi_ok_count/len(sample_df)*100:.1f}%)\")  \n",
    "print(f\"   • 🎯 Signaux d'ACHAT: {buy_count_simple:,}\")\n",
    "print(f\"   • 🎯 Signaux de VENTE: {sell_count_simple:,}\")\n",
    "\n",
    "if buy_count_simple > 0:\n",
    "    print(f\"\\n✅ SUCCÈS ! La stratégie simplifiée génère des signaux.\")\n",
    "    print(f\"💡 Le problème vient bien du SuperTrend qui contient uniquement des NaN\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Même la stratégie simplifiée ne génère pas de signaux.\")\n",
    "    print(f\"💡 Vérifier les données EMA et RSI\")\n",
    "\n",
    "print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "255aca40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 DÉMARRAGE DU BACKTESTING CHUNKED AVEC VALIDATION\n",
      "============================================================\n",
      "📊 Total à traiter: 5,844 lignes\n",
      "🔄 Chunks estimés: 1\n",
      "🛡️ Buffer de contexte: 100 lignes\n",
      "⏱️ Début: 00:40:37\n",
      "\n",
      "[  1/1] Chunk 0-5,844 |  5844 lignes | 📈 10 achats | 📉 66 ventes | ⚡  13145 l/s | ✅ no-ctx | 🔄   1 trades\n",
      "\n",
      "============================================================\n",
      "✅ BACKTESTING CHUNKED TERMINÉ AVEC VALIDATION\n",
      "============================================================\n",
      "📊 Lignes traitées: 5,844\n",
      "📈 Total signaux achat: 10\n",
      "📉 Total signaux vente: 66\n",
      "🔄 Total trades complétés: 1\n",
      "⏱️ Temps total: 0.4s\n",
      "⚡ Performance: 13,140 lignes/sec\n",
      "\n",
      "🛡️ RAPPORT DE VALIDATION:\n",
      "   • Chunks traités: 1\n",
      "   • Problèmes de continuité: 0\n",
      "   • Warnings total: 0\n",
      "   ✅ Cohérence parfaite - tous les signaux sont fiables\n",
      "|  5844 lignes | 📈 10 achats | 📉 66 ventes | ⚡  13145 l/s | ✅ no-ctx | 🔄   1 trades\n",
      "\n",
      "============================================================\n",
      "✅ BACKTESTING CHUNKED TERMINÉ AVEC VALIDATION\n",
      "============================================================\n",
      "📊 Lignes traitées: 5,844\n",
      "📈 Total signaux achat: 10\n",
      "📉 Total signaux vente: 66\n",
      "🔄 Total trades complétés: 1\n",
      "⏱️ Temps total: 0.4s\n",
      "⚡ Performance: 13,140 lignes/sec\n",
      "\n",
      "🛡️ RAPPORT DE VALIDATION:\n",
      "   • Chunks traités: 1\n",
      "   • Problèmes de continuité: 0\n",
      "   • Warnings total: 0\n",
      "   ✅ Cohérence parfaite - tous les signaux sont fiables\n"
     ]
    }
   ],
   "source": [
    "def run_chunked_backtest() -> List[Dict]:\n",
    "    \"\"\"Exécute le backtesting par chunks sur toutes les données avec validation de cohérence\"\"\"\n",
    "    \n",
    "    print(\"🚀 DÉMARRAGE DU BACKTESTING CHUNKED AVEC VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Récupérer le nombre total de lignes\n",
    "    if not data_summary:\n",
    "        print(\"❌ Pas d'informations sur les données\")\n",
    "        return []\n",
    "    \n",
    "    total_rows = data_summary['total_rows']\n",
    "    estimated_chunks = (total_rows // config.chunk_size) + 1\n",
    "    \n",
    "    print(f\"📊 Total à traiter: {total_rows:,} lignes\")\n",
    "    print(f\"🔄 Chunks estimés: {estimated_chunks:,}\")\n",
    "    print(f\"🛡️ Buffer de contexte: {backtester.state['context_buffer_size']} lignes\")\n",
    "    print(f\"⏱️ Début: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print()\n",
    "    \n",
    "    all_results = []\n",
    "    start_time = datetime.now()\n",
    "    total_warnings = 0\n",
    "    continuity_issues = 0\n",
    "    \n",
    "    # Traitement chunk par chunk\n",
    "    for offset in range(0, total_rows, config.chunk_size):\n",
    "        chunk_num = (offset // config.chunk_size) + 1\n",
    "        current_chunk_size = min(config.chunk_size, total_rows - offset)\n",
    "        \n",
    "        print(f\"[{chunk_num:>3}/{estimated_chunks}] \", end=\"\")\n",
    "        print(f\"Chunk {offset:,}-{offset + current_chunk_size:,} \", end=\"\")\n",
    "        \n",
    "        try:\n",
    "            # Charger le chunk avec contexte\n",
    "            chunk_start = datetime.now()\n",
    "            chunk_df = backtester.load_chunk_with_context(offset, current_chunk_size)\n",
    "            \n",
    "            if len(chunk_df) == 0:\n",
    "                print(\"⚠️ Chunk vide - arrêt\")\n",
    "                break\n",
    "            \n",
    "            # Traiter le chunk\n",
    "            is_first = (offset == 0)\n",
    "            chunk_result = backtester.process_chunk(chunk_df, is_first)\n",
    "            \n",
    "            # Vérifier les erreurs\n",
    "            if 'error' in chunk_result:\n",
    "                print(f\"❌ Erreur: {chunk_result['error']}\")\n",
    "                break\n",
    "            \n",
    "            # Calculer les métriques du chunk\n",
    "            chunk_time = (datetime.now() - chunk_start).total_seconds()\n",
    "            rows_per_sec = chunk_result['rows_processed'] / max(chunk_time, 0.001)\n",
    "            \n",
    "            # Affichage des métriques avec validation\n",
    "            continuity_status = \"✅\" if chunk_result.get('continuity_validated', False) else \"⚠️\"\n",
    "            context_info = f\"ctx:{chunk_result.get('context_size', 0)}\" if chunk_result.get('context_size', 0) > 0 else \"no-ctx\"\n",
    "            \n",
    "            print(f\"| {chunk_result['rows_processed']:>5} lignes \", end=\"\")\n",
    "            print(f\"| 📈 {chunk_result['buy_signals']:>2} achats \", end=\"\")\n",
    "            print(f\"| 📉 {chunk_result['sell_signals']:>2} ventes \", end=\"\")\n",
    "            print(f\"| ⚡ {rows_per_sec:>6.0f} l/s \", end=\"\")\n",
    "            print(f\"| {continuity_status} {context_info} \", end=\"\")\n",
    "            print(f\"| 🔄 {backtester.state['total_trades']:>3} trades\")\n",
    "            \n",
    "            # Gestion des warnings\n",
    "            if 'warnings' in chunk_result and chunk_result['warnings']:\n",
    "                for warning in chunk_result['warnings']:\n",
    "                    print(f\"    ⚠️ {warning}\")\n",
    "                    total_warnings += 1\n",
    "            \n",
    "            if not chunk_result.get('continuity_validated', False):\n",
    "                continuity_issues += 1\n",
    "            \n",
    "            all_results.append(chunk_result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Statistiques finales avec validation\n",
    "    total_time = (datetime.now() - start_time).total_seconds()\n",
    "    total_processed = sum(r['rows_processed'] for r in all_results)\n",
    "    total_buy_signals = sum(r['buy_signals'] for r in all_results)\n",
    "    total_sell_signals = sum(r['sell_signals'] for r in all_results)\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"✅ BACKTESTING CHUNKED TERMINÉ AVEC VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"📊 Lignes traitées: {total_processed:,}\")\n",
    "    print(f\"📈 Total signaux achat: {total_buy_signals:,}\")\n",
    "    print(f\"📉 Total signaux vente: {total_sell_signals:,}\")\n",
    "    print(f\"🔄 Total trades complétés: {backtester.state['total_trades']:,}\")\n",
    "    print(f\"⏱️ Temps total: {total_time:.1f}s\")\n",
    "    print(f\"⚡ Performance: {total_processed/max(total_time, 0.001):,.0f} lignes/sec\")\n",
    "    \n",
    "    # Rapport de validation\n",
    "    print(f\"\\n🛡️ RAPPORT DE VALIDATION:\")\n",
    "    print(f\"   • Chunks traités: {len(all_results):,}\")\n",
    "    print(f\"   • Problèmes de continuité: {continuity_issues:,}\")\n",
    "    print(f\"   • Warnings total: {total_warnings:,}\")\n",
    "    \n",
    "    if continuity_issues == 0 and total_warnings == 0:\n",
    "        print(\"   ✅ Cohérence parfaite - tous les signaux sont fiables\")\n",
    "    elif continuity_issues > 0:\n",
    "        print(f\"   ⚠️ {continuity_issues} chunks avec problèmes de continuité\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Exécution du backtesting avec validation\n",
    "backtest_results = run_chunked_backtest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f2fc8",
   "metadata": {},
   "source": [
    "## 5. 💾 Sauvegarde des Résultats dans Table Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89a96af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SAUVEGARDE DES RÉSULTATS\n",
      "==============================\n",
      "📊 Données combinées: 5,844 lignes\n",
      "📅 Période: 2023-01-01 00:00:00 → 2025-08-31 20:00:00\n",
      "✅ Sauvegarde locale: /tmp/backtest_BTCUSDT_20251004_004046.parquet\n",
      "⚠️ MinIO non disponible: Invalid Error: Unexpected response while initializing S3 multipart upload\n",
      "📁 Utilisation sauvegarde locale: /tmp/backtest_BTCUSDT_20251004_004046.parquet\n",
      "✅ Résultats sauvegardés: /tmp/backtest_BTCUSDT_20251004_004046.parquet\n",
      "📁 Taille: ~1.6 MB\n",
      "❌ Erreur lors de la sauvegarde: Invalid Error: Unexpected response while initializing S3 multipart upload\n"
     ]
    }
   ],
   "source": [
    "def save_results_to_test_table(results: List[Dict]) -> bool:\n",
    "    \"\"\"Sauvegarde les résultats dans la table test pour analyse VectorBT\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"❌ Pas de résultats à sauvegarder\")\n",
    "        return False\n",
    "    \n",
    "    print(\"💾 SAUVEGARDE DES RÉSULTATS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    try:\n",
    "        # Combiner tous les DataFrames de résultats\n",
    "        all_data = []\n",
    "        for result in results:\n",
    "            if 'data' in result and result['data'] is not None:\n",
    "                # Ajouter les métadonnées du chunk\n",
    "                chunk_data = result['data'].with_columns([\n",
    "                    pl.lit(result['chunk_id']).alias('chunk_id'),\n",
    "                    pl.lit(datetime.now().isoformat()).alias('backtest_timestamp'),\n",
    "                    pl.lit(config.symbol).alias('symbol'),\n",
    "                    pl.lit(f\"{config.strategy_name if hasattr(config, 'strategy_name') else 'smart_momentum'}\").alias('strategy_name')\n",
    "                ])\n",
    "                all_data.append(chunk_data)\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"❌ Pas de données à combiner\")\n",
    "            return False\n",
    "        \n",
    "        # Combiner toutes les données\n",
    "        final_df = pl.concat(all_data)\n",
    "        \n",
    "        print(f\"📊 Données combinées: {len(final_df):,} lignes\")\n",
    "        print(f\"📅 Période: {final_df['datetime'].min()} → {final_df['datetime'].max()}\")\n",
    "        \n",
    "        # Générer le chemin de sortie avec timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = f\"{config.test_table_path}backtest_{config.symbol}_{timestamp}.parquet\"\n",
    "        \n",
    "        # Sauvegarder localement en attendant la correction MinIO\n",
    "        local_output_path = f\"/tmp/backtest_{config.symbol}_{timestamp}.parquet\"\n",
    "        \n",
    "        # Sauvegarder en local d'abord\n",
    "        final_df.write_parquet(local_output_path, compression='snappy')\n",
    "        print(f\"✅ Sauvegarde locale: {local_output_path}\")\n",
    "        \n",
    "        # Essayer MinIO en option\n",
    "        try:\n",
    "            temp_table = \"temp_backtest_results\"\n",
    "            data_loader.con.register(temp_table, final_df.to_arrow())\n",
    "            \n",
    "            export_query = f\"\"\"\n",
    "                COPY (SELECT * FROM {temp_table})\n",
    "                TO '{output_path}'\n",
    "                (FORMAT PARQUET, COMPRESSION 'snappy')\n",
    "            \"\"\"\n",
    "            data_loader.con.execute(export_query)\n",
    "            print(f\"✅ Sauvegarde MinIO: {output_path}\")\n",
    "        except Exception as minio_error:\n",
    "            print(f\"⚠️ MinIO non disponible: {minio_error}\")\n",
    "            print(f\"📁 Utilisation sauvegarde locale: {local_output_path}\")\n",
    "            output_path = local_output_path\n",
    "        \n",
    "        print(f\"✅ Résultats sauvegardés: {output_path}\")\n",
    "        print(f\"📁 Taille: ~{final_df.estimated_size('mb'):.1f} MB\")\n",
    "        \n",
    "        # Sauvegarder également les métadonnées\n",
    "        metadata = {\n",
    "            'config': {\n",
    "                'symbol': config.symbol,\n",
    "                'chunk_size': config.chunk_size,\n",
    "                'start_date': config.start_date,\n",
    "                'end_date': config.end_date,\n",
    "                'initial_cash': config.initial_cash,\n",
    "                'fees': config.fees\n",
    "            },\n",
    "            'results': {\n",
    "                'total_rows': len(final_df),\n",
    "                'total_chunks': len(results),\n",
    "                'total_buy_signals': sum(r['buy_signals'] for r in results),\n",
    "                'total_sell_signals': sum(r['sell_signals'] for r in results),\n",
    "                'total_trades': backtester.state['total_trades']\n",
    "            },\n",
    "            'paths': {\n",
    "                'data_path': output_path,\n",
    "                'source_path': config.feature_store_path\n",
    "            },\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        metadata_path = f\"{config.test_table_path}metadata_{config.symbol}_{timestamp}.json\"\n",
    "        \n",
    "        # Sauvegarder les métadonnées (méthode simplifiée)\n",
    "        metadata_df = pl.DataFrame([metadata])\n",
    "        data_loader.con.register(\"temp_metadata\", metadata_df.to_arrow())\n",
    "        data_loader.con.execute(f\"\"\"\n",
    "            COPY (SELECT * FROM temp_metadata)\n",
    "            TO '{metadata_path.replace('.json', '.parquet')}'\n",
    "            (FORMAT PARQUET)\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"📋 Métadonnées sauvegardées: {metadata_path.replace('.json', '.parquet')}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de la sauvegarde: {e}\")\n",
    "        return False\n",
    "\n",
    "# Sauvegarder les résultats\n",
    "save_success = save_results_to_test_table(backtest_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d8beb",
   "metadata": {},
   "source": [
    "## 6. 📈 Validation et Analyse avec VectorBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cea9a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 ANALYSE VECTORBT\n",
      "====================\n",
      "📊 Données pour VectorBT: 5,844 lignes\n",
      "📅 Période: 2023-01-01 00:00:00 → 2025-08-31 20:00:00\n",
      "\n",
      "📊 RÉSULTATS VECTORBT:\n",
      "💰 Capital initial: $10,000.00\n",
      "💰 Capital final: $13,705.34\n",
      "📈 Rendement total: 37.05%\n",
      "🔄 Nombre de trades: 10\n",
      "💹 Trade moyen: 370.53\n",
      "🎯 Taux de réussite: 40.0%\n",
      "📉 Drawdown max: 0.00%\n",
      "📉 Drawdown max: 0.00%\n",
      "\n",
      "📊 STATISTIQUES AVANCÉES:\n",
      "Start                         2023-01-01 00:00:00\n",
      "End                           2025-08-31 20:00:00\n",
      "Period                          974 days 00:00:00\n",
      "Start Value                               10000.0\n",
      "End Value                            13705.338196\n",
      "Total Return [%]                        37.053382\n",
      "Benchmark Return [%]                   554.727443\n",
      "Max Gross Exposure [%]                      100.0\n",
      "Total Fees Paid                        236.514826\n",
      "Max Drawdown [%]                        11.481144\n",
      "Max Drawdown Duration           319 days 16:00:00\n",
      "Total Trades                                   10\n",
      "Total Closed Trades                            10\n",
      "Total Open Trades                               0\n",
      "Open Trade PnL                                0.0\n",
      "Win Rate [%]                                 40.0\n",
      "Best Trade [%]                          18.690917\n",
      "Worst Trade [%]                         -3.870799\n",
      "Avg Winning Trade [%]                   12.991374\n",
      "Avg Losing Trade [%]                    -2.765411\n",
      "Avg Winning Trade Duration       14 days 20:00:00\n",
      "Avg Losing Trade Duration         2 days 18:00:00\n",
      "Profit Factor                            2.807848\n",
      "Expectancy                              370.53382\n",
      "Sharpe Ratio                             1.020706\n",
      "Calmar Ratio                             1.092036\n",
      "Omega Ratio                              1.273007\n",
      "Sortino Ratio                            1.452173\n",
      "dtype: object\n",
      "\n",
      "📊 STATISTIQUES AVANCÉES:\n",
      "Start                         2023-01-01 00:00:00\n",
      "End                           2025-08-31 20:00:00\n",
      "Period                          974 days 00:00:00\n",
      "Start Value                               10000.0\n",
      "End Value                            13705.338196\n",
      "Total Return [%]                        37.053382\n",
      "Benchmark Return [%]                   554.727443\n",
      "Max Gross Exposure [%]                      100.0\n",
      "Total Fees Paid                        236.514826\n",
      "Max Drawdown [%]                        11.481144\n",
      "Max Drawdown Duration           319 days 16:00:00\n",
      "Total Trades                                   10\n",
      "Total Closed Trades                            10\n",
      "Total Open Trades                               0\n",
      "Open Trade PnL                                0.0\n",
      "Win Rate [%]                                 40.0\n",
      "Best Trade [%]                          18.690917\n",
      "Worst Trade [%]                         -3.870799\n",
      "Avg Winning Trade [%]                   12.991374\n",
      "Avg Losing Trade [%]                    -2.765411\n",
      "Avg Winning Trade Duration       14 days 20:00:00\n",
      "Avg Losing Trade Duration         2 days 18:00:00\n",
      "Profit Factor                            2.807848\n",
      "Expectancy                              370.53382\n",
      "Sharpe Ratio                             1.020706\n",
      "Calmar Ratio                             1.092036\n",
      "Omega Ratio                              1.273007\n",
      "Sortino Ratio                            1.452173\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def analyze_with_vectorbt() -> Optional[vbt.Portfolio]:\n",
    "    \"\"\"Analyse des résultats avec VectorBT\"\"\"\n",
    "    \n",
    "    if not backtest_results:\n",
    "        print(\"❌ Pas de résultats à analyser\")\n",
    "        return None\n",
    "    \n",
    "    print(\"📈 ANALYSE VECTORBT\")\n",
    "    print(\"=\" * 20)\n",
    "    \n",
    "    try:\n",
    "        # Combiner toutes les données pour VectorBT\n",
    "        all_data = []\n",
    "        for result in backtest_results:\n",
    "            if 'data' in result and result['data'] is not None:\n",
    "                all_data.append(result['data'])\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"❌ Pas de données à analyser\")\n",
    "            return None\n",
    "        \n",
    "        # Combiner et convertir en pandas pour VectorBT\n",
    "        combined_df = pl.concat(all_data)\n",
    "        df_pd = combined_df.to_pandas().set_index('datetime')\n",
    "        \n",
    "        print(f\"📊 Données pour VectorBT: {len(df_pd):,} lignes\")\n",
    "        print(f\"📅 Période: {df_pd.index.min()} → {df_pd.index.max()}\")\n",
    "        \n",
    "        # Créer le portfolio VectorBT\n",
    "        portfolio = vbt.Portfolio.from_signals(\n",
    "            close=df_pd['close'],\n",
    "            entries=df_pd['buy_signal'],\n",
    "            exits=df_pd['sell_signal'],\n",
    "            init_cash=config.initial_cash,\n",
    "            fees=config.fees,\n",
    "            freq='4H'  # Ajuster selon vos données\n",
    "        )\n",
    "        \n",
    "        # Statistiques de base\n",
    "        print(f\"\\n📊 RÉSULTATS VECTORBT:\")\n",
    "        print(f\"💰 Capital initial: ${config.initial_cash:,.2f}\")\n",
    "        print(f\"💰 Capital final: ${portfolio.final_value():,.2f}\")\n",
    "        print(f\"📈 Rendement total: {(portfolio.final_value() / config.initial_cash - 1) * 100:.2f}%\")\n",
    "        print(f\"🔄 Nombre de trades: {portfolio.trades.count()}\")\n",
    "        \n",
    "        if portfolio.trades.count() > 0:\n",
    "            print(f\"💹 Trade moyen: {portfolio.trades.pnl.mean():.2f}\")\n",
    "            print(f\"🎯 Taux de réussite: {portfolio.trades.win_rate() * 100:.1f}%\")\n",
    "            try:\n",
    "                print(f\"📉 Drawdown max: {portfolio.drawdown().max() * 100:.2f}%\")\n",
    "            except:\n",
    "                print(f\"📉 Drawdown max: N/A\")\n",
    "        \n",
    "        # Statistiques avancées\n",
    "        stats = portfolio.stats()\n",
    "        print(f\"\\n📊 STATISTIQUES AVANCÉES:\")\n",
    "        print(stats)\n",
    "        \n",
    "        return portfolio\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de l'analyse VectorBT: {e}\")\n",
    "        return None\n",
    "\n",
    "# Analyse avec VectorBT\n",
    "portfolio = analyze_with_vectorbt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32d2c243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Impossible de générer les graphiques\n"
     ]
    }
   ],
   "source": [
    "# Visualisation des résultats\n",
    "if portfolio is not None:\n",
    "    print(\"🎨 Génération des graphiques...\")\n",
    "    \n",
    "    # Graphique principal du portfolio\n",
    "    fig = portfolio.plot()\n",
    "    fig.show()\n",
    "    \n",
    "    # Graphique des trades\n",
    "    if portfolio.trades.count() > 0:\n",
    "        trades_fig = portfolio.trades.plot()\n",
    "        trades_fig.show()\n",
    "    \n",
    "    print(\"✅ Graphiques générés avec succès\")\n",
    "else:\n",
    "    print(\"❌ Impossible de générer les graphiques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260c15e",
   "metadata": {},
   "source": [
    "## 7. 📋 Résumé et Prochaines Étapes\n",
    "\n",
    "### ✅ Ce qui a été accompli\n",
    "- Backtesting chunked avec continuité des indicateurs\n",
    "- Traitement de gros volumes avec mémoire constante\n",
    "- Sauvegarde des résultats dans tables test\n",
    "- Analyse et validation avec VectorBT\n",
    "\n",
    "### 🚀 Prochaines étapes suggérées\n",
    "1. **Optimisation des paramètres** : Utiliser les résultats pour ajuster la stratégie\n",
    "2. **Backtesting multi-timeframes** : Tester sur différentes périodes\n",
    "3. **Stratégies avancées** : Intégrer de nouvelles conditions\n",
    "4. **Production** : Déployer la stratégie validée\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a44fc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 RÉSUMÉ DU BACKTESTING CHUNKED\n",
      "\n",
      "📊 Données traitées: 5,844 lignes en 1 chunks\n",
      "📈 Signaux générés: 76\n",
      "🔄 Trades complétés: 1\n",
      "💰 Performance: 37.05%\n",
      "📊 Sharpe Ratio: 1.02\n",
      "💾 Résultats sauvegardés: ❌\n",
      "📈 Analyse VectorBT: ✅\n",
      "\n",
      "✅ Backtesting terminé avec succès !\n",
      "🚀 Prêt pour l'optimisation et la production\n"
     ]
    }
   ],
   "source": [
    "# Résumé final\n",
    "print(\"\" * 60)\n",
    "print(\"🎯 RÉSUMÉ DU BACKTESTING CHUNKED\")\n",
    "print(\"\" * 60)\n",
    "\n",
    "if backtest_results:\n",
    "    total_processed = sum(r['rows_processed'] for r in backtest_results)\n",
    "    total_chunks = len(backtest_results)\n",
    "    total_signals = sum(r['buy_signals'] + r['sell_signals'] for r in backtest_results)\n",
    "    \n",
    "    print(f\"📊 Données traitées: {total_processed:,} lignes en {total_chunks} chunks\")\n",
    "    print(f\"📈 Signaux générés: {total_signals:,}\")\n",
    "    print(f\"🔄 Trades complétés: {backtester.state['total_trades']:,}\")\n",
    "    \n",
    "    if portfolio is not None:\n",
    "        print(f\"💰 Performance: {(portfolio.final_value() / config.initial_cash - 1) * 100:.2f}%\")\n",
    "        print(f\"📊 Sharpe Ratio: {portfolio.sharpe_ratio():.2f}\" if hasattr(portfolio, 'sharpe_ratio') else \"\")\n",
    "    \n",
    "    print(f\"💾 Résultats sauvegardés: {'✅' if save_success else '❌'}\")\n",
    "    print(f\"📈 Analyse VectorBT: {'✅' if portfolio is not None else '❌'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Aucun résultat généré\")\n",
    "\n",
    "print(\"\\n✅ Backtesting terminé avec succès !\")\n",
    "print(\"🚀 Prêt pour l'optimisation et la production\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hermes-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
