# Hermes Data Lab

Ce document dÃ©crit l'architecture du Data Lab Hermes, une plateforme conÃ§ue pour l'ingestion, la transformation et l'analyse de donnÃ©es financiÃ¨res en vue de dÃ©velopper et backtester des stratÃ©gies de trading.

ðŸ›ï¸ Architecture GÃ©nÃ©rale : ModÃ¨le Medallion

L'architecture repose sur le modÃ¨le Medallion en trois couches (Bronze, Argent, Or). Ce modÃ¨le garantit une sÃ©paration claire des responsabilitÃ©s, une traÃ§abilitÃ© des donnÃ©es et une montÃ©e en qualitÃ© progressive, de la donnÃ©e brute Ã  la donnÃ©e prÃªte Ã  l'emploi.
Plaintext

          +-------------------------+
          |     Source : NFS        |
          |  Fichiers bruts (.zip)  |
          | (DonnÃ©es d'API, etc.)   |
          +-----------+-------------+
                      |
                      v
          +-------------------------+
          |      ðŸ¥‰ Couche Bronze     |
          | - Tables brutes, immuables|
          | - PartitionnÃ©es par date  |
          | - Format : Parquet        |
          | - Stockage : MinIO        |
          +-----------+-------------+
                      |
                      v
          +-------------------------+
          |      ðŸ¥ˆ Couche Argent     |
          | - DonnÃ©es nettoyÃ©es, validÃ©es|
          | - Types, dates et schÃ©mas standardisÃ©s |
          | - ContrÃ´les qualitÃ©       |
          | - Stockage : MinIO        |
          +-----------+-------------+
                      |
                      v
          +-------------------------+
          |       ðŸ¥‡ Couche Or        |
          | - Datamarts spÃ©cialisÃ©s   |
          | - DonnÃ©es enrichies, agrÃ©gÃ©es|
          | - Indicateurs de trading  |
          | - Stockage : MinIO        |
          +-------------------------+

ðŸ› ï¸ Ã‰cosystÃ¨me Technologique

Notre architecture s'appuie sur des outils modernes et performants, choisis pour leur efficacitÃ© dans le traitement de donnÃ©es volumineuses.
Outil	RÃ´le dans l'architecture
MinIO (S3)	Data Lake central pour le stockage des couches Bronze, Argent et Or.
DuckDB	Moteur d'interrogation SQL rapide pour lire/Ã©crire sur MinIO, effectuer des jointures et des agrÃ©gations.
Polars	Moteur de transformation ultra-rapide pour le nettoyage, le calcul d'indicateurs et les manipulations complexes de DataFrames.
Jupyter Notebook	Interface d'analyse et d'exploration pour le dÃ©veloppement des stratÃ©gies, la visualisation et le backtesting.

ðŸŒŠ Flux de DonnÃ©es (Data Flow)

Le parcours de la donnÃ©e suit un processus clair Ã  travers les trois couches.

Ã‰tape 1 : Ingestion (Source â†’ Bronze)

L'objectif de cette Ã©tape est de crÃ©er une copie brute, fiable et partitionnÃ©e des donnÃ©es sources.

    Les fichiers .zip contenant les donnÃ©es brutes sont rÃ©cupÃ©rÃ©s depuis le montage NFS.

    Un script lit chaque archive, ajoute des colonnes de date (year, month, day) pour le partitionnement.

    Les donnÃ©es sont Ã©crites sur MinIO au format Parquet, partitionnÃ©es par date. Polars et DuckDB sont utilisÃ©s pour leur efficacitÃ© dans ce processus.

Ã‰tape 2 : Structuration et Nettoyage (Bronze â†’ Argent)

Ici, nous transformons les donnÃ©es brutes en une source de vÃ©ritÃ© propre et fiable.

    Les tables de la couche Bronze sont lues depuis MinIO.

    Des transformations structurelles sont appliquÃ©es avec Polars :

        Conversion des timestamps en dates lisibles et fuseaux horaires cohÃ©rents.

        Correction et unification des types de donnÃ©es (ex: string â†’ float).

        Application de contrÃ´les qualitÃ© pour dÃ©tecter les anomalies (valeurs manquantes, outliers).

    Les tables nettoyÃ©es et structurÃ©es sont Ã©crites dans la couche Argent sur MinIO.

Ã‰tape 3 : Enrichissement MÃ©tier (Argent â†’ Or)

Cette couche crÃ©e des datamarts spÃ©cialisÃ©s, prÃªts Ã  Ãªtre consommÃ©s par les algorithmes de trading.

    Les donnÃ©es propres de la couche Argent sont lues.

    Des rÃ¨gles mÃ©tier et des calculs complexes sont appliquÃ©s :

        Calcul d'indicateurs techniques (Moyennes Mobiles, RSI, etc.) via Polars.

        Jointures avec d'autres sources de donnÃ©es (ex: donnÃ©es macro-Ã©conomiques) via DuckDB.

        CrÃ©ation de signaux de trading basÃ©s sur les stratÃ©gies dÃ©finies.

    Les tables finales, enrichies et souvent agrÃ©gÃ©es, sont stockÃ©es dans la couche Or.

Ã‰tape 4 : Consommation et Analyse (Or â†’ Jupyter)

La couche Or est le point d'entrÃ©e pour toute analyse.

    Les Jupyter Notebooks se connectent directement aux tables de la couche Or via DuckDB pour explorer les donnÃ©es, visualiser les rÃ©sultats, et itÃ©rer rapidement sur le backtesting des stratÃ©gies.

âœ¨ Bonnes Pratiques et Recommandations

Pour garantir la robustesse et la maintenabilitÃ© de la plateforme, les principes suivants sont appliquÃ©s :

    Format de Fichiers : Le format Parquet est utilisÃ© systÃ©matiquement aprÃ¨s l'ingestion initiale pour bÃ©nÃ©ficier de la compression, du stockage en colonnes et de l'optimisation des lectures (predicate pushdown).

    QualitÃ© des DonnÃ©es : Des mÃ©triques de qualitÃ© sont calculÃ©es et suivies lors du passage Ã  la couche Argent pour garantir la fiabilitÃ© des analyses en aval.

    Versioning : Chaque couche (donnÃ©es) et chaque transformation (code) doit Ãªtre versionnÃ©e. Cela permet de garantir la reproductibilitÃ© des expÃ©riences et des backtests.

    SÃ©paration des Environnements : Les donnÃ©es utilisÃ©es pour le backtesting (couche Or) doivent Ãªtre distinctes de celles utilisÃ©es en production pour Ã©viter tout biais ou contamination des modÃ¨les.

    TraÃ§abilitÃ© : Chaque table de la couche Or doit pouvoir Ãªtre tracÃ©e jusqu'Ã  sa source brute dans les fichiers NFS, en documentant toutes les transformations intermÃ©diaires.


















# Hermes
Data Lab : exploration 
---

          +-------------------------+
          |         NFS             |
          |  Fichiers bruts .zip    |
          | (API ou tÃ©lÃ©chargÃ©s)    |
          +-----------+-------------+
                      |
                      v
          +-------------------------+
          |        Bronze           |
          | - Tables brutes         |
          | - PartitionnÃ©es (date)  |
          | - Stockage : MinIO  (format Parquet)    |
          | - Lecture/Ã©criture :   |
          |   Polars / DuckDB       |
          +-----------+-------------+
                      |
                      v
          +-------------------------+
          |        Argent  
          Tables Bronze         |
          | - Nettoyage / structuration |
          | - Dates lisibles, alignement |
          |   temporel, types cohÃ©rents,contrÃ´les   qualitÃ©   |
          | - Stockage : MinIO           |
          | - Traitement : Polars / DuckDB|
          +-----------+-------------+
                      |
                      v
          +-------------------------+
          |         Or              |
          | - Tables Argent enrichies,     |
          |   prÃªtes pour trading   |
          | - Application stratÃ©gie |
          | - Stockage : MinIO      |
          | - Analyse : Polars / DuckDB / Jupyter |
          +-------------------------+


| Couche        | MinIO                   | DuckDB                           | Polars                               | Jupyter Notebook                            |
| ------------- | ----------------------- | -------------------------------- | ------------------------------------ | ------------------------------------------- |
| Bronze        | Stockage partitionnÃ©    | Lecture rapide, SQL sur fichiers | Chargement / transformations rapides | Exploration des donnÃ©es                     |
| Argent        | Stockage structurÃ©      | SQL + Joins, agrÃ©gations         | Transformations complexes, nettoyage | Notebooks pour test / ETL                   |
| Or (datamart) | Stockage tables finales | Interrogation pour backtesting   | Calcul indicateurs, signaux          | Analyse / visualisation / stratÃ©gie trading |


Ingestion NFS â†’ Bronze :

Tu prends les .zip de lâ€™API, tu les dÃ©compresses et tu stockes les fichiers bruts sur MinIO.

DuckDB et Polars permettent de charger les fichiers Parquet/CSV directement depuis MinIO pour traitement.

Bronze â†’ Argent :

Nettoyage minimal et transformation structurelle : date lisible, alignement temporel, types corrects.

Polars est trÃ¨s rapide pour les transformations colonne par colonne.

Argent â†’ Or (Datamart) :

Application des rÃ¨gles de trading et calcul des indicateurs.

DuckDB permet des jointures SQL complexes si nÃ©cessaire.

Polars sert pour le calcul vectorisÃ© trÃ¨s rapide (ex : indicateurs techniques).

Or â†’ Consommation / Analyse :

Jupyter pour visualisation et exploration.

Tu peux itÃ©rer sur stratÃ©gies de trading directement sur les tables Or.


Chaque couche peut avoir un versioning pour tracer les changements.
Bronze = â€œrawâ€, Argent = â€œcurated / structuredâ€, Or = â€œconsommable / ready-to-useâ€


NFS : fichiers bruts .zip en provenance de lâ€™API

Positif :

Stocker les fichiers bruts tels quels est une bonne pratique pour reproduire lâ€™ingestion initiale et conserver lâ€™historique exact des donnÃ©es.

Permet de refaire des transformations si nÃ©cessaire.

Ã€ amÃ©liorer :

VÃ©rifie que tu as un mÃ©canisme de tracking de version/date pour ces fichiers, afin de pouvoir retracer exactement quelle version a Ã©tÃ© ingÃ©rÃ©e.

Penser Ã  lâ€™archivage et nettoyage automatique si le volume devient trÃ¨s important.

2ï¸âƒ£ Bronze : tables brutes partitionnÃ©es sur MinIO

Positif :

Partitionner dÃ¨s le dÃ©part (par date par exemple) est excellent pour le scaling et lâ€™optimisation des lectures dans Spark ou PySpark.

Stocker sur MinIO (S3-like) est trÃ¨s adaptÃ© pour un datalake.

Ã€ amÃ©liorer :

Assure-toi que les formats sont optimisÃ©s pour le traitement : Parquet ou Delta Lake (plutÃ´t que CSV/JSON), pour bÃ©nÃ©ficier de compression, schema evolution, predicate pushdown.

Bronze doit rester â€œrawâ€ : Ã©vite de faire des transformations ici (mÃªme minimes), sauf nettoyage obligatoire pour ingestion.

3ï¸âƒ£ Argent : tables de Bronze avec transformations structurelles

Positif :

Transformation pour rendre la date lisible et alignement temporel sont classiques pour la couche â€œsilverâ€.

Ici, on commence Ã  corriger/structurer les donnÃ©es sans perte dâ€™information.

Ã€ amÃ©liorer :

Documente bien toutes les transformations, surtout si elles influencent les calculs de stratÃ©gie.

Penser Ã  inclure des indicateurs de qualitÃ© / anomalies dÃ©tectÃ©es, trÃ¨s important en trading.

4ï¸âƒ£ Or : tables dâ€™Argent avec application de stratÃ©gie trading

Positif :

Bonne pratique dâ€™avoir une couche finale â€œready-to-useâ€ pour les algos de trading.

IdÃ©alement, ces tables sont aggrÃ©gÃ©es, filtrÃ©es, enrichies, et prÃªtes pour lâ€™analytics ou la production.

Ã€ amÃ©liorer :

SÃ©pare Ã©ventuellement les donnÃ©es utilisÃ©es pour backtesting vs celles pour production pour Ã©viter les contaminations.

Penser Ã  des mÃ©ta-donnÃ©es de version de stratÃ©gie et des logs de backtest pour audit et traÃ§abilitÃ©.

âœ… Conclusion gÃ©nÃ©rale

Ton architecture suit globalement les bonnes pratiques du datalake en 3 couches (bronze/silver/gold) :

NFS = raw files

Bronze = ingestion brute partitionnÃ©e

Argent = structuration / nettoyage

Or = application mÃ©tier / trading

ðŸ’¡ Quelques conseils pour lâ€™amÃ©liorer :

Utiliser Parquet/Delta Lake pour Bronze/Argent/Or pour performance et fiabilitÃ©.

Ajouter un systÃ¨me de versioning/metadata Ã  chaque couche.

Tracker la qualitÃ© des donnÃ©es Ã  la couche Argent.

SÃ©parer backtesting et production pour Or.


raph TD
    subgraph Zone Bronze
        A[API Exchange] --> B(Minio: /bronze/btcusdt_raw.parquet);
    end

    subgraph Zone Silver
        B --> C{Nettoyage/Typage};
        C --> D(Minio: /silver/btcusdt_clean.parquet);
    end

    subgraph Zone Gold
        D --> E[Hub d'Indicateurs];
        E --> F1[Mart StratÃ©gie 1];
        E --> F2[Mart StratÃ©gie 2];
    end

    subgraph Couche de DÃ©cision
        F1 & F2 --> G{Moteur de DÃ©cision};
        G --> H[decision_log];
        H --> I[order_log];
    end

    I --> J[API Exchange];